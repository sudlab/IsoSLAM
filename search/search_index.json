{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to IsoSLAM","text":"<ul> <li>Introduction</li> <li>Installation</li> <li>Usage</li> <li>Workflow</li> <li>Contributing</li> <li>Extending</li> <li>API</li> </ul>"},{"location":"extending/","title":"Extending IsoSLAM","text":"<p>The modular nature of IsoSLAM and its use of <code>ruffus</code> and <code>cgat</code> mean that it is relatively straight-forward to add additional steps or processing.</p>"},{"location":"extending/#overview","title":"Overview","text":"<ol> <li>Add a new module to <code>isoslam/&lt;module_name&gt;.py</code> and write functions, include numpydoc    strings so the functions/classes are documented.</li> <li>Add all options to <code>isoslam/default_config.yaml</code>.</li> <li>Add a <code>sub-parser</code> to <code>isoslam/processing.py</code> with command line options for all arguments to your function.</li> <li>Add a <code>process_&lt;module_name&gt;</code> function to <code>isoslam/processing.py</code>.</li> <li>Add an entry to the documentation to build the API documentation automatically.</li> </ol> <p>By way of example the implementation of the <code>isoslam summary</code> sub-command is explained.</p>"},{"location":"extending/#adding-a-module","title":"Adding a module","text":"<p>This is probably the most flexible part, you can add the module as you see fit. You can use Object Orientated approach and write a class or classes or functional programming and a series.</p> <p>However you will need a single function that takes the input and various options.</p>"},{"location":"extending/#example","title":"Example","text":"<p>The <code>summary</code> module appends multiple files produced from running IsoSLAM on a series of inputs and appends the data. These are then summarised by a set of variables to give the number of counts.</p>"},{"location":"extending/#isoslamsummarypy","title":"<code>isoslam/summary.py</code>","text":"<pre><code>\"\"\"Functions for summarising output.\"\"\"\n\nimport pandas as pd\n\nfrom isoslam import io\n\n\ndef append_files(pattern: str = \"**/*.tsv\", separator: str = \"\\t\") -&gt; pd.DataFrame:\n    \"\"\"\n    Append a set of files into a Pandas DataFrames.\n\n    Parameters\n    ----------\n    pattern : str\n        File name pattern to search for.\n    separator : str\n        Separator/delimiter used in files.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrames of each file found.\n    \"\"\"\n    _data = io.load_files(pattern, separator)\n    all_data = [data.assign(filename=key) for key, data in _data.items()]\n    return pd.concat(all_data)\n\n\ndef summary_counts(\n    file_pattern: str = \"**/*.tsv\",\n    separator: str = \"\\t\",\n    groupby: list[str] | None = None,\n    dropna: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Count the number of assigned read pairs.\n\n    Groups the data by\n\n    Parameters\n    ----------\n    file_pattern : str\n        File name pattern to search for.\n    separator : str\n        Separator/delimiter used in files.\n    groupby : list[str]\n        List of variables to group the counts by.\n    dropna : book\n        Whether to drop rows with ``NA`` values.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrames of each file found.\n    \"\"\"\n    if groupby is None:\n        groupby = [\n            \"Transcript_id\",\n            \"Chr\",\n            \"Strand\",\n            \"Start\",\n            \"End\",\n            \"Assignment\",\n            \"Conversions\",\n            \"filename\",\n        ]\n    _data = append_files(file_pattern, separator)\n    _data[\"one_or_more_conversion\"] = _data[\"Conversions\"] &gt;= 1\n    groupby.append(\"one_or_more_conversion\")\n    return _data.value_counts(subset=groupby, dropna=dropna).reset_index()\n</code></pre> <p>This included writing a function to search for files with a given <code>pattern</code> and load them using the specified <code>separator</code>. As this is an Input/Output operation the functions were added to the <code>isoslam/io.py</code> module.</p>"},{"location":"extending/#iopy","title":"<code>io.py</code>","text":"<pre><code>def _find_files(pattern: str = \"**/*.tsv\") -&gt; Generator:  # type: ignore[type-arg]\n    \"\"\"\n    Find files that match the given pattern.\n\n    Parameters\n    ----------\n    pattern : str\n        Pattern (regular expression) of files to search for.\n\n    Returns\n    -------\n    Generator[_P, None, None]\n        A generator of files found that match the given pattern.\n    \"\"\"\n    pwd = Path.cwd()\n    return pwd.rglob(pattern)\n\n\ndef load_files(pattern: str = \"**/*.tsv\", sep: str = \"\\t\") -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Read a set of files into a list of Pandas DataFrames.\n\n    Parameters\n    ----------\n    pattern : str\n        File name pattern to search for.\n    sep : str\n        Separator/delimiter used in files.\n\n    Returns\n    -------\n    list[pd.DataFrame]\n        A list of Pandas DataFrames of each file found.\n    \"\"\"\n    return {x.stem: pd.read_csv(x, sep=sep) for x in _find_files(pattern)}\n</code></pre>"},{"location":"extending/#add-options-to-isoslamdefault_configyaml","title":"Add options to <code>isoslam/default_config.yaml</code>","text":"<p>We want to be consistent across the configuration file, which resides in <code>isoslam/default_config.yaml</code> and is used when generating configurations using <code>isoslam create-config</code>. To do so the function parameters, in this example <code>summary_counts()</code>, should be used as entries in the <code>isoslam/default_config.yaml</code>.</p>"},{"location":"extending/#example_1","title":"Example","text":"<p>The top level of a modules configuration should match the module name, here <code>summary_counts</code>. Each entry is a key/value pair that corresponds to the arguments of the function, and so we have <code>file_pattern</code>, <code>separator</code>, <code>groupby</code> and <code>output</code> with their various options.</p> <pre><code>summary_counts:\n  file_pattern: \"**/*.tsv\"\n  separator: \"\\t\"\n  groupby:\n    - Transcript_id\n    - Chr\n    - Strand\n    - Start\n    - End\n    - Assignment\n    - Conversions\n    - filename\n  output:\n    outfile: summary_counts.tsv\n    sep: \"\\t\"\n    index: false\n</code></pre>"},{"location":"extending/#add-a-sub-parser-to-isoslamprocessingpy","title":"Add a sub-parser to <code>isoslam/processing.py</code>","text":"<p>The function <code>create_parser()</code> is responsible for creating the <code>isoslam</code> arguments and sub-parsers and their associated arguments.</p> <p>Define a sub-parser and <code>add_argument()</code> for each option that is available. Keep names consistent with the arguments of the main function you have written above and in turn the configuration values in <code>isoslam/default_config.yaml</code>.</p>"},{"location":"extending/#example_2","title":"Example","text":"<pre><code># Summarise counts sub-parser\nsummary_counts_parser = subparsers.add_parser(\n    \"summary-counts\",\n    description=\"Summarise the counts.\",\n    help=\"Summarise the counts.\",\n)\nsummary_counts_parser.add_argument(\n    \"--file-pattern\",\n    dest=\"file_pattern\",\n    type=str,\n    required=False,\n    default=\"*_summarized.tsv\",\n    help=\"Regular expression for summarized files to process.\",\n)\nsummary_counts_parser.add_argument(\n    \"--outfile\",\n    dest=\"outfile\",\n    type=Path,\n    required=False,\n    default=\"summary_counts.tsv\",\n    help=\"Output filename to save results to, will be nested under 'output_dir'.\",\n)\nsummary_counts_parser.add_argument(\n    \"--separator\",\n    dest=\"sep\",\n    type=str,\n    required=False,\n    default=\"\\t\",\n    help=\"Field separator to use in output file, default is '\\t' but other values (e.g. ',' are allowed).\",\n)\nsummary_counts_parser.set_defaults(func=summarise_counts)\n</code></pre> <p>The last line here <code>.set_defaults(func=summarise_counts)</code> is the function that will be called when running the subcommand and corresponds to the function</p>"},{"location":"extending/#documentation","title":"Documentation","text":"<p>To have the documentation automatically built from the docstrings you have written for your functions you need to add a <code>docs/api/&lt;module_name&gt;.md</code> that corresponds to each of the modules you have introduced and add a title, short description and <code>isoslam.&lt;module_name&gt;</code>. If you have introduced more than one module then you will have to add a corresponding file for each module/sub-module you have introduced. If these are nested please mirror the nesting structure in the documentation.</p>"},{"location":"extending/#example_3","title":"Example","text":"<pre><code># Summary\n\n::: isoslam.summary\nhandler: python\noptions:\ndocstring_style:\nnumpy\nrendering:\nshow_signature_annotations: true\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Ideally you should install IsoSLAM under a Python Virtual Environment. Details of how to work with and use these is beyond the scope of this documentation but some advice can be found in the contributing section.</p>"},{"location":"installation/#isoslam","title":"IsoSLAM","text":""},{"location":"installation/#github","title":"GitHub","text":"<p>There are two methods of installing IsoSLAM from its GitHub repository.</p>"},{"location":"installation/#cloning","title":"Cloning","text":"<p>You can clone the repository and install from the clone.</p> <pre><code>git clone git@github.com:sudlab/IsoSLAM.git\ncd IsoSLAM\npip install -e .\n</code></pre> <p>By using the <code>-e</code> (editable) flag it means you can switch branches.</p>"},{"location":"installation/#pip-from-github","title":"<code>pip</code> from GitHub","text":"<p>The package installer for Python pip can be used to install packages directly from their version control homepage.</p> <pre><code>pip install git+ssh://git@github.com/IsoSLAM\n</code></pre> <p>If you want to install a specific branch or commit you can do so.</p> <pre><code>pip install git+ssh://git@github.com/IsoSLAM@&lt;branch-name&gt;\npip install git+ssh://git@github.com/IsoSLAM@&lt;commit-hash&gt;\n</code></pre>"},{"location":"installation/#pypi","title":"PyPI","text":"<p>We intend to publish IsoSLAM to the Python Package Index (PyPI). When available you will be able to install with</p> <pre><code>pip install IsoSLAM\n</code></pre> <p>NB IsoSLAM is NOT currently available on PyPI.</p>"},{"location":"installation/#bioconda","title":"Bioconda","text":"<p>NB IsoSLAM is NOT currently available on Bioconda.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>There are a number of external dependencies required for running IsoSLAM as part of a ruffus pipeline, which is typically essential to prepare the files for processing.</p> <ul> <li><code>samtools</code> / <code>bcftools</code> are both required and can be downloaded from htslib</li> <li>VarScan (documentation)</li> <li>subread (documentation)</li> </ul>"},{"location":"installation/#indirect-dependencies","title":"Indirect Dependencies","text":"<p>The pipeline for running the various steps in processing data rely on the cgat tools. These have two external dependencies themselves and if you wish to use such a pipeline will have to install these.</p> <ul> <li>bedtools</li> <li>wigToBigWig - the minimal tools may suffice.</li> </ul>"},{"location":"installation/#gnulinux","title":"GNU/Linux","text":""},{"location":"installation/#arch-linux","title":"Arch Linux","text":"<p>If you use Arch Linux the packages are available in the Arch Linux User Repository (AUR)</p> <pre><code>mkdir ~/aur &amp;&amp; cd ~/aur\ngit clone https://aur.archlinux.org/htslib.git\ngit clone https://aur.archlinux.org/samtools.git\ngit clone https://aur.archlinux.org/bcftools.git\ngit clone https://aur.archlinux.org/subread.git\ngit clone https://aur.archlinux.org/bedtools.git\ncd htslib\nmakepkg -sri\ncd ../bcftools\nmakepkg -sri\ncd ../samtools\nmakepkg -sri\ncd ../subread\nmakepkg -sri\ncd ../bedtools\nmakepkg -sri\n</code></pre> <p>varscan is written in Java, you need to download the latest release</p> <p>You can make a wrapper to run this, assuming you have saved the file to <code>~/.local/jar/VarScan.v2.4.6.jar</code> (adjust for the version you have downloaded), you can create the following short script and make it executable, placing it in your <code>$PATH</code> (the example below uses <code>~/.local/bin/</code>)</p> <pre><code>#!/bin/bash\n\njava -jar ~/.local\"\n</code></pre> <p>Make the file executable and you can then run <code>varscan</code></p> <pre><code>chmod 755 ~/.local/bin/varscan\nvarscan\nVarScan v2.4.6\n\n***NON-COMMERCIAL VERSION***\n\nUSAGE: java -jar VarScan.jar [COMMAND] [OPTIONS]\n\nCOMMANDS:\n pileup2snp  Identify SNPs from a pileup file\n pileup2indel  Identify indels a pileup file\n pileup2cns  Call consensus and variants from a pileup file\n mpileup2snp  Identify SNPs from an mpileup file\n mpileup2indel  Identify indels an mpileup file\n mpileup2cns  Call consensus and variants from an mpileup file\n\n somatic   Call germline/somatic variants from tumor-normal pileups\n mpileup2somatic  Call germline/somatic variants in multi-tumor-normal mpileup (beta feature in v2.4.5)\n copynumber  Determine relative tumor copy number from tumor-normal pileups\n readcounts  Obtain read counts for a list of variants from a pileup file\n\n filter   Filter SNPs by coverage, frequency, p-value, etc.\n somaticFilter  Filter somatic variants for clusters/indels\n fpfilter  Apply the false-positive filter\n\n processSomatic  Isolate Germline/LOH/Somatic calls from output\n copyCaller  GC-adjust and process copy number changes from VarScan copynumber output\n compare   Compare two lists of positions/variants\n limit   Restrict pileup/snps/indels to ROI positions\n</code></pre>"},{"location":"installation/#gentoo","title":"Gentoo","text":"<p><code>samtools</code> and <code>bcftools</code> are available in Portage, to install.</p> <pre><code>emerge --sync &amp;&amp; emerge -av samtools bcftools bedtools ucsc-genome-browser\n</code></pre> <p>to write - how to install</p>"},{"location":"installation/#ubuntudebian","title":"Ubuntu/Debian","text":"<p>All three packages are available for Debian based repositories.</p> <pre><code>sudo apt-get update\nsudo apt-get install samtools bcftools bedtools\n</code></pre>"},{"location":"installation/#ucsc-genome-browser","title":"UCSC Genome Browser","text":"<p>This needs installing from source across all distributions. The minimal should suffice.</p> <pre><code>git clone git@github.com:ucscGenomeBrowser/kent-core.git\ncd kent-core\nsudo make\n</code></pre>"},{"location":"installation/#source-install","title":"Source Install","text":"<p>The releases pages includes instructions on how to build the package from source, but note that you will then have to manually update the packages when new releases are made.</p>"},{"location":"installation/#windows","title":"Windows","text":"<p>To write at some point.</p>"},{"location":"installation/#osx","title":"OSX","text":"<p>To write at some point.</p>"},{"location":"installation/#conda","title":"Conda","text":"<p>If you don't have the ability to install these programmes at the system level an alternative is to use a Conda environment.</p> <pre><code>conda create -n isoslam python==3.12\nconda activate isoslam\nconda install mamba\nmamba install -c conda-forge -c bioconda cgat-apps\nmamba install -c conda-forge -c bioconda samtools bcftools\nmamba install -c conda-forge -c bioconda subread\nmamba install -c conda-forge -c bioconda varscan\n</code></pre>"},{"location":"introduction/","title":"Introduction","text":"<p>IsoSLAM is a Python package for processing the output of...</p>"},{"location":"links/","title":"Links","text":"<p>Links to software, documentation and related topics.</p> <ul> <li>Bedtools</li> <li>CGAT</li> <li>htslib</li> <li>Ruffus</li> <li>varscan (GitHub : varscan)</li> </ul>"},{"location":"links/#related-software","title":"Related Software","text":""},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#stand-alone-usage","title":"Stand alone usage","text":"<p>Once installed the <code>isolsam</code> command is should be available and you can view the options and sub-commands with</p> <pre><code>\u2771 isoslam --help\n\nusage: isoslam [-h] [-v] [-c CONFIG_FILE] [-b BASE_DIR] [-o OUTPUT_DIR] [-l LOG_LEVEL] {process,create-config,summary-counts} ...\n\nRun various programs related to IsoSLAM. Add the name of the program you wish to run.\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         Report the installed version of IsoSLAM.\n  -c, --config-file CONFIG_FILE\n                        Path to a YAML configuration file.\n  -b, --base-dir BASE_DIR\n                        Base directory to run isoslam on.\n  -o, --output-dir OUTPUT_DIR\n                        Output directory to write results to.\n  -l, --log-level LOG_LEVEL\n                        Logging level to use, default is 'info' for verbose output use 'debug'.\n\nprogram:\n  Available programs listed below:\n\n  {process,create-config,summary-counts}\n    process             Process all files and run all summary plotting and statistics.\n    create-config       Create a configuration file using the defaults.\n    summary-counts      Summarise the counts.\n</code></pre> <p>Global options that set the configuration file to use, the base and output directory and log-level can be specified. Users then have a number of programs from IsoSLAM.</p> <ul> <li><code>process</code></li> <li><code>create-config</code></li> <li><code>summary-counts</code></li> </ul>"},{"location":"usage/#processing","title":"Processing","text":"<p>Help is available on using the <code>isoslam process</code> command and can be viewed with the <code>--help</code> option.</p> <pre><code>\u2771 isoslam process --help\nusage: isoslam process [-h] [-b BAM_FILE] [-g GTF_FILE] [-d BED_FILE] [-v VCF_FILE] [-u UPPER_PAIRS_LIMIT] [-f FIRST_MATCHED_LIMIT] [--delim DELIM]\n                       [--output-file OUTPUT_FILE]\n\nProcess all files and run all summary plotting and statistics.\n\noptions:\n  -h, --help            show this help message and exit\n  -b, --bam-file BAM_FILE\n                        Path to '.bam' file that has undergone read assignment with 'featureCount'.\n  -g, --gtf-file GTF_FILE\n                        Path to '.gtf' transcript assembly file.\n  -d, --bed-file BED_FILE\n                        Path to '.bed' utron file. Must be bed6 format.\n  -v, --vcf-file VCF_FILE\n                        Path to '.vcf.gz' file.\n  -u, --upper-pairs-limit UPPER_PAIRS_LIMIT\n                        Upper limit of pairs to be processed.\n  -f, --first-matched-limit FIRST_MATCHED_LIMIT\n                        Limit of matches.\n  --delim DELIM         Delimiter to use in output.\n  --output-file OUTPUT_FILE\n                        File to write results to.\n</code></pre> <p>It is important to remember that the <code>.bam</code> file requires pre-processing and instructions on how to do that can be found below.</p> <p>Three additional input files are required.</p> <ul> <li><code>.gtf</code> : DESCRIPTION</li> <li><code>.bed</code> : DESCRIPTION</li> <li><code>.vcf</code> : DESCRIPTION</li> </ul> <p>By default output is written to the <code>./output</code> directory which will be created if it does not exist and the default output file is <code>results.parquet</code> which is in Parquet format. The output directory is configurable, as is the output file name. If you would rather your output is saved as ASCII delimited file it is easy to do so by specifying the <code>--delim ,</code> (for comma-delimited) or <code>--delim \\t</code> (for tab-delimited) and using the appropriate file extension in the <code>--output-file</code>. For example the following outputs to <code>new_results</code> directory in a file <code>output_20250213.csv</code></p> <pre><code>\u2771 isoslam --output-dir new_results process \\\n         --bam-file d0_no4sU_EKRN230046545-1A_HFWGNDSX7_L4.star.bam \\\n         --gtf-file test_wash1.gtf \\\n         --bed-file test_coding_introns.bed \\\n         --vcf-file d0.vcf.gz \\\n         --delim \",\" \\\n         --output-file output_20250213.csv\n</code></pre>"},{"location":"usage/#configuration-file","title":"Configuration File","text":"<p>It is possible to place all configuration options in a YAML configuration file. A sample configuration file can be generated using the <code>isoslam create-config</code> command and there are options to specify the output filename, the default is <code>config.yaml</code> in the current directory (see <code>isoslam create-config --help</code> for all options).</p> <pre><code>\u2771 isoslam create-config --help\nusage: isoslam create-config [-h] [-f FILENAME] [-o OUTPUT_DIR]\n\nCreate a configuration file using the defaults.\n\noptions:\n  -h, --help            show this help message and exit\n  -f, --filename FILENAME\n                        Name of YAML file to save configuration to (default 'config.yaml').\n  -o, --output-dir OUTPUT_DIR\n                        Path to where the YAML file should be saved (default './' the current directory).\n</code></pre> <p>Once created you can edit the <code>config.yaml</code> to specify all fields and parameters and then invoke processing using this file.</p> <pre><code>\u2771 isoslam --config-file config.yaml process\n</code></pre> <p>Any command line options given such as the <code>--upper-pairs-limit</code> or <code>--first-matched-limit</code> will override those details in the custom <code>config.yaml</code>. This means it is particularly useful for usage in the full pipeline as <code>.gtf</code>, <code>.bed</code> and <code>.vcf</code> files are often common and it is the <code>.bam</code> input file that changes between runs. Thus you can specify the common files in your configuration file and then use the <code>--bam-file &lt;file_path&gt;</code> to override the field in the configuration file.</p>"},{"location":"usage/#ruffuscgat-pipeline","title":"Ruffus/CGAT pipeline","text":"<p>Typically <code>isoslam</code> is part of a workflow pipeline that processes a large number of files. As such ruffus is used to keep track of tasks.</p> <p>EXPAND THIS SECTION WITH WORKED EXAMPLES BASED ON EXISTING SCRIPTS.</p>"},{"location":"workflow/","title":"Workflow","text":"<p>This page gives an overview of the workflow undertaken by IsoSLAM, it is a WORK IN PROGRESS as the code base is underoing refactoring.</p> <ul> <li>This is very much a work in progress and is not yet complete. Contributions are welcome.</li> </ul> flowchart TB     subgraph Input         BAM[(BAM Files)]         VCF[(\"VCF Files\")]         GTF[(\"GTF Files\")]         Config[(\"Config Files\")]     end      subgraph Core[\"Core Processing\"]         IO[\"Input/Output Handler\"]:::core         Process[\"Processing Engine\"]:::core         Pipeline[\"SLAM Pipeline\"]:::core         Summary[\"Summary Generator\"]:::core     end      subgraph Support[\"Support Components\"]         Logger[\"Logging System\"]:::support         Utils[\"Utility Functions\"]:::support         DefaultConfig[\"Configuration Manager\"]:::support     end      subgraph Testing[\"Testing &amp; Documentation\"]         TestInfra[\"Testing Infrastructure\"]:::test         TestRes[\"Test Resources\"]:::test         APIDoc[\"API Documentation\"]:::doc     end      subgraph Integration[\"External Integration\"]         RScript[\"R Script Integration\"]:::integration         CICD[\"CI/CD Pipeline\"]:::integration         PipeConfig[\"Pipeline Configuration\"]:::integration     end      %% Main Data Flow     BAM --&gt; IO     VCF --&gt; IO     GTF --&gt; IO     Config --&gt; DefaultConfig      IO --&gt; Process     Process --&gt; Pipeline     Pipeline --&gt; Summary      %% Support Flow     DefaultConfig -.-&gt; IO     DefaultConfig -.-&gt; Process     DefaultConfig -.-&gt; Pipeline      Logger -.-&gt; IO     Logger -.-&gt; Process     Logger -.-&gt; Pipeline     Logger -.-&gt; Summary      Utils --&gt; IO     Utils --&gt; Process     Utils --&gt; Pipeline      %% Integration Flow     Pipeline --&gt; RScript     PipeConfig -.-&gt; Pipeline     CICD -.-&gt; TestInfra      %% Testing Flow     TestRes --&gt; TestInfra     TestInfra -.-&gt; Core      %% Click Events     click IO \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/io.py\"     click Process \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/processing.py\"     click Pipeline \"https://github.com/sudlab/IsoSLAM/tree/main/isoslam/pipeline_slam_3UIs/\"     click Summary \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/summary.py\"     click Logger \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/logging.py\"     click Utils \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/utils.py\"     click DefaultConfig \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/default_config.yaml\"     click TestInfra \"https://github.com/sudlab/IsoSLAM/tree/main/tests/\"     click APIDoc \"https://github.com/sudlab/IsoSLAM/tree/main/docs/api/\"     click CICD \"https://github.com/sudlab/IsoSLAM/tree/main/.github/workflows/\"     click PipeConfig \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/pipeline_slam_3UIs/pipeline.yml\"     click RScript \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/pipeline_slam_3UIs/summarize_counts.R\"     click TestRes \"https://github.com/sudlab/IsoSLAM/tree/main/tests/resources/\"      %% Styling     classDef core fill:#90EE90,stroke:#333,stroke-width:2px     classDef support fill:#FFE4B5,stroke:#333,stroke-width:2px     classDef test fill:#DDA0DD,stroke:#333,stroke-width:2px     classDef doc fill:#87CEEB,stroke:#333,stroke-width:2px     classDef integration fill:#F08080,stroke:#333,stroke-width:2px      %% Legend     subgraph Legend         L1[\"Core Components\"]:::core         L2[\"Support Components\"]:::support         L3[\"Testing Components\"]:::test         L4[\"Documentation\"]:::doc         L5[\"Integration\"]:::integration     end  <p>The above diagram is written in Mermaid and generated using GitDiagram. You can view the source code in the IsoSLAM repository and develop/modify it using the Mermaid Live Editor and make pull-requests to update this documentation.</p>"},{"location":"workflow/#isoslam","title":"IsoSLAM","text":"<p>A number of pre-processing steps are undertaken prior to IsoSLAM work being done. The following is work in progress as the code is refactored.</p> <ol> <li>Iterate over <code>.bam</code> file and pair segments. If two or more <code>AlignedSegments</code> with the same <code>query_name</code> are found    then <code>n &gt; 1</code> segments are dropped.</li> <li>Pairs of segments (individual <code>AlignedSegments</code>) are then assessed and if they are <code>Assigned</code> the <code>start</code>, <code>end</code>,    <code>length</code>, <code>status</code> (i.e. <code>Assigned</code>), <code>transcript_id</code>, <code>block_start</code> and <code>block_end</code> are extracted.</li> </ol>"},{"location":"workflow/#descriptive-workflow","title":"Descriptive Workflow","text":""},{"location":"workflow/#read-alignments-are-loaded","title":"Read alignments are loaded","text":""},{"location":"workflow/#the-gene-transcript-these-are-within-are-identified","title":"The gene transcript these are within are identified","text":""},{"location":"workflow/#introns-within-these-genes-are-identified","title":"Introns within these genes are identified","text":""},{"location":"workflow/#retain-reads-that-are-overlap-with-introns-or-splice-ends","title":"Retain reads that are overlap with introns or splice ends","text":"<pre><code>                                                 Genome -----------------------&gt;\n\nRead Alignment Blocks                     |&gt;&gt;&gt;&gt;|                     |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|\nTranscript 1:                      |===========|---------------------|=========|-------------------|==========|\nTranscript 2:              |====|------------------------------------|=========|-------------------|==========|\n\nIntrons[0]                                      ---------------------\nIntrons[1]                                                                      -------------------\nIntrons[2]                       ------------------------------------\nIntrons[3]                                                                      -------------------\n\nExon : |========|   Read alignment block:  |&gt;&gt;&gt;&gt;&gt;|\n</code></pre>"},{"location":"api/","title":"API","text":"<ul> <li><code>io</code></li> <li><code>isoslam</code></li> <li><code>logging</code></li> <li><code>plotting</code></li> <li><code>processing</code></li> <li><code>summary</code></li> <li><code>utils</code></li> </ul>"},{"location":"api/io/","title":"IO Modules","text":"<p>Module for reading and writing files.</p>"},{"location":"api/io/#isoslam.io._find_files","title":"<code>_find_files(pattern='**/*.tsv')</code>","text":"<p>Find files that match the given pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>Pattern (regular expression) of files to search for.</p> <code>'**/*.tsv'</code> <p>Returns:</p> Type Description <code>Generator[_P, None, None]</code> <p>A generator of files found that match the given pattern.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _find_files(pattern: str = \"**/*.tsv\") -&gt; Generator:  # type: ignore[type-arg]\n    \"\"\"\n    Find files that match the given pattern.\n\n    Parameters\n    ----------\n    pattern : str\n        Pattern (regular expression) of files to search for.\n\n    Returns\n    -------\n    Generator[_P, None, None]\n        A generator of files found that match the given pattern.\n    \"\"\"\n    pwd = Path.cwd()\n    return pwd.rglob(pattern)\n</code></pre>"},{"location":"api/io/#isoslam.io._get_date_time","title":"<code>_get_date_time()</code>","text":"<p>Get a date and time for adding to generated files or logging.</p> <p>Returns:</p> Type Description <code>str</code> <p>A string of the current date and time, formatted appropriately.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _get_date_time() -&gt; str:\n    \"\"\"\n    Get a date and time for adding to generated files or logging.\n\n    Returns\n    -------\n    str\n        A string of the current date and time, formatted appropriately.\n    \"\"\"\n    return datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n</code></pre>"},{"location":"api/io/#isoslam.io._get_loader","title":"<code>_get_loader(file_ext='bam')</code>","text":"<p>Creator component which determines which file loader to use.</p> <p>Parameters:</p> Name Type Description Default <code>file_ext</code> <code>str</code> <p>File extension of file to be loaded.</p> <code>'bam'</code> <p>Returns:</p> Type Description <code>function</code> <p>Returns the function appropriate for the required file type to be loaded.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Unsupported file extension results in ValueError.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _get_loader(file_ext: str = \"bam\") -&gt; Callable:  # type: ignore[type-arg]\n    \"\"\"\n    Creator component which determines which file loader to use.\n\n    Parameters\n    ----------\n    file_ext : str\n        File extension of file to be loaded.\n\n    Returns\n    -------\n    function\n        Returns the function appropriate for the required file type to be loaded.\n\n    Raises\n    ------\n    ValueError\n        Unsupported file extension results in ValueError.\n    \"\"\"\n    if file_ext == \".bam\":\n        return _load_bam\n    if file_ext in (\".bed\", \".bed.gz\"):\n        return _load_bed\n    if file_ext == \".gtf\":\n        return _load_gtf\n    if file_ext in (\".vcf\", \".vcf.gz\"):\n        return _load_vcf\n    raise ValueError(file_ext)\n</code></pre>"},{"location":"api/io/#isoslam.io._load_bam","title":"<code>_load_bam(bam_file)</code>","text":"<p>Load <code>.bam</code> file.</p> <p><code>.bam</code> files are the sequence data that is to be analysed.</p> <p>Parameters:</p> Name Type Description Default <code>bam_file</code> <code>str | Path</code> <p>Path, as string or pathlib Path, to a '.bam' file that is to be loaded.</p> required <p>Returns:</p> Type Description <code>AlignmentFile</code> <p>Loads the specified alignment file.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _load_bam(bam_file: str | Path) -&gt; pysam.libcalignmentfile.AlignmentFile:\n    \"\"\"\n    Load ``.bam`` file.\n\n    ``.bam`` files are the sequence data that is to be analysed.\n\n    Parameters\n    ----------\n    bam_file : str | Path\n        Path, as string or pathlib Path, to a '.bam' file that is to be loaded.\n\n    Returns\n    -------\n    pysam.libcalignmentfile.AlignmentFile\n        Loads the specified alignment file.\n    \"\"\"\n    try:\n        return pysam.AlignmentFile(bam_file)\n    except FileNotFoundError as e:\n        raise e\n</code></pre>"},{"location":"api/io/#isoslam.io._load_bed","title":"<code>_load_bed(bed_file)</code>","text":"<p>Open <code>.bed</code> file for reading, supports gzip compressed formats.</p> <p><code>.bed</code> files contain the locations of introns/splice junctions.</p> <p>Parameters:</p> Name Type Description Default <code>bed_file</code> <code>str | Path</code> <p>Path, as string or pathlib Path, to a '.bed' or '.bed.gz' file that is to be loaded.</p> required <p>Returns:</p> Type Description <code>TextIO</code> <p>Returns a connection to an open file object.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _load_bed(bed_file: str | Path) -&gt; TextIO:\n    \"\"\"\n    Open ``.bed`` file for reading, supports gzip compressed formats.\n\n    ``.bed`` files contain the locations of introns/splice junctions.\n\n    Parameters\n    ----------\n    bed_file : str | Path\n        Path, as string or pathlib Path, to a '.bed' or '.bed.gz' file that is to be loaded.\n\n    Returns\n    -------\n    TextIO\n        Returns a connection to an open file object.\n    \"\"\"\n    try:\n        if Path(bed_file).suffix == \".gz\":\n            return gzip.open(bed_file, \"rt\", encoding=\"utf-8\")\n        return Path(bed_file).open(mode=\"r\", encoding=\"utf-8\")\n    except OSError as e:\n        raise e\n</code></pre>"},{"location":"api/io/#isoslam.io._load_gtf","title":"<code>_load_gtf(gtf_file)</code>","text":"<p>Load <code>.gtf</code> file and return as an iterable.</p> <p><code>.gtf</code> files contain the transcript structures from which the <code>.bed</code> file is derived.</p> <p>Parameters:</p> Name Type Description Default <code>gtf_file</code> <code>str | Path</code> <p>Path, as string or pathlib Path, to a '.gtf' file that is to be loaded.</p> required <p>Returns:</p> Type Description <code>tabix_generic_iterator</code> <p>Iterator of GTF file.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _load_gtf(gtf_file: str | Path) -&gt; pysam.libctabix.tabix_generic_iterator:\n    \"\"\"\n    Load ``.gtf`` file and return as an iterable.\n\n    ``.gtf`` files contain the transcript structures from which the ``.bed`` file is derived.\n\n    Parameters\n    ----------\n    gtf_file : str | Path\n        Path, as string or pathlib Path, to a '.gtf' file that is to be loaded.\n\n    Returns\n    -------\n    pysam.libctabix.tabix_generic_iterator\n        Iterator of GTF file.\n    \"\"\"\n    try:\n        return pysam.tabix_iterator(Path(gtf_file).open(encoding=\"utf8\"), parser=pysam.asGTF())\n    except FileNotFoundError as e:\n        raise e\n</code></pre>"},{"location":"api/io/#isoslam.io._load_vcf","title":"<code>_load_vcf(vcf_file)</code>","text":"<p>Load <code>.vcf</code> file.</p> <p><code>.vcf</code> files contain the locations of known sequences difference from the reference sequence. Any <code>T &gt; C</code> (e.g. SNPs) conversions that match to the location of known seuqence variations will be removed. These can be obtained from a reference collection of variation data (such as <code>dbSNP &lt;https://www.ncbi.nlm.nih.gov/projects/SNP/get_html.cgi?whichHtml=overview&gt;</code>_) or derived directly from the RNAseq reads.</p> <p>Parameters:</p> Name Type Description Default <code>vcf_file</code> <code>str | Path</code> <p>Path, as string or pathlib Path, to a '.vcf' file that is to be loaded.</p> required <p>Returns:</p> Type Description <code>VariantFile</code> <p>Loads the specified VCF file.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _load_vcf(vcf_file: str | Path) -&gt; pysam.libcbcf.VariantFile:\n    \"\"\"\n    Load ``.vcf`` file.\n\n    ``.vcf`` files contain the locations of known sequences difference from the reference sequence. Any ``T &gt; C``\n    (e.g. SNPs) conversions that match to the location of known seuqence variations will be removed. These can be\n    obtained from a reference collection of variation data (such as `dbSNP\n    &lt;https://www.ncbi.nlm.nih.gov/projects/SNP/get_html.cgi?whichHtml=overview&gt;`_) or derived directly from the RNAseq\n    reads.\n\n    Parameters\n    ----------\n    vcf_file : str | Path\n        Path, as string or pathlib Path, to a '.vcf' file that is to be loaded.\n\n    Returns\n    -------\n    pysam.libcbcf.VariantFile\n        Loads the specified VCF file.\n    \"\"\"\n    try:\n        return pysam.VariantFile(vcf_file)\n    except FileNotFoundError as e:\n        raise e\n</code></pre>"},{"location":"api/io/#isoslam.io._path_to_str","title":"<code>_path_to_str(config)</code>","text":"<p>Recursively traverse a dictionary and convert any Path() objects to strings for writing to YAML.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary to be converted.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>dict[str, Any]</code> <p>The same dictionary with any Path() objects converted to string.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _path_to_str(config: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Recursively traverse a dictionary and convert any Path() objects to strings for writing to YAML.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary to be converted.\n\n    Returns\n    -------\n    Dict:\n        The same dictionary with any Path() objects converted to string.\n    \"\"\"\n    for key, value in config.items():\n        if isinstance(value, dict):\n            _path_to_str(value)\n        elif isinstance(value, Path):\n            config[key] = str(value)\n    return config\n</code></pre>"},{"location":"api/io/#isoslam.io._str_to_path","title":"<code>_str_to_path(path)</code>","text":"<p>Ensure path is a Path object.</p> <p>Returns the current directory of passed './'.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to be converted.</p> required <p>Returns:</p> Type Description <code>Path</code> <p>Pathlib object of supplied path.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _str_to_path(path: str | Path) -&gt; Path:\n    \"\"\"\n    Ensure path is a Path object.\n\n    Returns the current directory of passed './'.\n\n    Parameters\n    ----------\n    path : str | Path\n        Path to be converted.\n\n    Returns\n    -------\n    Path\n        Pathlib object of supplied path.\n    \"\"\"\n    return Path().cwd() if path == \"./\" else Path(path).expanduser()\n</code></pre>"},{"location":"api/io/#isoslam.io._type_schema","title":"<code>_type_schema(schema)</code>","text":"<p>Convert schemas to types.</p> <p>When read from a YAML file the schema's values are strings, they need converting to types to work with Polars.</p> <p>Parameters:</p> Name Type Description Default <code>schema</code> <code>dict[str, str]</code> <p>Dictionary of schema types with values as strings.</p> required <p>Returns:</p> Type Description <code>dict[str, Type]</code> <p>Returns dictionary with schema types as such.</p> Source code in <code>isoslam/io.py</code> <pre><code>def _type_schema(schema: dict[str, str]) -&gt; dict[str, type]:\n    \"\"\"\n    Convert schemas to types.\n\n    When read from a YAML file the schema's values are strings, they need converting to types to work with Polars.\n\n    Parameters\n    ----------\n    schema : dict[str, str]\n        Dictionary of schema types with values as strings.\n\n    Returns\n    -------\n    dict[str, Type]\n        Returns dictionary with schema types as such.\n    \"\"\"\n    return {key: eval(value) for key, value in schema.items()}  # pylint: disable=eval-used  # noqa: S307\n</code></pre>"},{"location":"api/io/#isoslam.io.create_config","title":"<code>create_config(args=None)</code>","text":"<p>Write the default configuration file to disk.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace | None</code> <p>Optional arguments to parse.</p> <code>None</code> Source code in <code>isoslam/io.py</code> <pre><code>def create_config(args: argparse.Namespace | None = None) -&gt; None:\n    \"\"\"\n    Write the default configuration file to disk.\n\n    Parameters\n    ----------\n    args : argparse.Namespace | None\n        Optional arguments to parse.\n    \"\"\"\n    filename = \"config\" if args.filename is None else args.filename  # type: ignore [union-attr]\n    output_dir = Path(\"./\") if args.output_dir is None else Path(args.output_dir)  # type: ignore [union-attr]\n    output_dir.mkdir(parents=True, exist_ok=True)\n    config_path = resources.files(__package__) / \"default_config.yaml\"\n    config = config_path.read_text()\n\n    if \".yaml\" not in str(filename) and \".yml\" not in str(filename):\n        create_config_path = output_dir / f\"{filename}.yaml\"\n    else:\n        create_config_path = output_dir / filename\n\n    with create_config_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(f\"# Config file generated {_get_date_time()}\\n\")\n        f.write(f\"{CONFIG_DOCUMENTATION_REFERENCE}\")\n        f.write(config)\n    logger.info(f\"A sample configuration file has been written to : {str(create_config_path)}\")\n    logger.info(CONFIG_DOCUMENTATION_REFERENCE)\n</code></pre>"},{"location":"api/io/#isoslam.io.data_frame_to_file","title":"<code>data_frame_to_file(data, output_dir='./output/', outfile='summary_counts.tsv', sep='\\t', **kwargs)</code>","text":"<p>Write a Pandas DataFrame to disk.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Pandas DataFrame to write to disk.</p> required <code>output_dir</code> <code>str | Path</code> <p>Location to write the output to, default is ''./output''.capitalize.</p> <code>'./output/'</code> <code>outfile</code> <code>str</code> <p>Filename to write data to.</p> <code>'summary_counts.tsv'</code> <code>sep</code> <code>str</code> <p>Separator to use in output file.</p> <code>'\\t'</code> <code>**kwargs</code> <code>dict[Any, Any]</code> <p>Dictionary of keyword arguments to pass to ''pandas.DataFrame.to_csv()''.</p> <code>{}</code> Source code in <code>isoslam/io.py</code> <pre><code>def data_frame_to_file(\n    data: pd.DataFrame | pl.DataFrame,\n    output_dir: str | Path = \"./output/\",\n    outfile: str = \"summary_counts.tsv\",\n    sep: str = \"\\t\",\n    **kwargs: dict[Any, Any],\n) -&gt; None:\n    \"\"\"\n    Write a Pandas DataFrame to disk.\n\n    Parameters\n    ----------\n    data : pd.DataFrame\n        Pandas DataFrame to write to disk.\n    output_dir : str | Path\n        Location to write the output to, default is ''./output''.capitalize.\n    outfile : str\n        Filename to write data to.\n    sep : str\n        Separator to use in output file.\n    **kwargs\n        Dictionary of keyword arguments to pass to ''pandas.DataFrame.to_csv()''.\n    \"\"\"\n    outdir_file = Path(output_dir) / f\"{outfile}\"\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    if isinstance(data, pl.DataFrame):\n        try:\n            if re.search(r\"parquet$\", str(outfile)):\n                data.write_parquet(outdir_file, **kwargs)\n            elif re.search(r\"\\..sv$\", str(outfile)):\n                data.write_csv(outdir_file, separator=sep, **kwargs)\n            logger.debug(f\"File written to : {outdir_file}\")\n        except Exception as e:\n            raise e\n    elif isinstance(data, pd.DataFrame):\n        try:\n            if re.search(r\"parquet\", str(outfile)):\n                data.to_parquet(outdir_file, **kwargs)\n            elif re.search(r\"\\..sv$\", str(outfile)):\n                data.to_csv(outdir_file, sep=sep, **kwargs)\n            logger.debug(f\"File written to : {outdir_file}\")\n        except Exception as e:\n            raise e\n    else:\n        raise TypeError(f\"Can not write output Pandas or Polar Dataframe object not supplied = {type(data)=}\")\n</code></pre>"},{"location":"api/io/#isoslam.io.load_and_update_config","title":"<code>load_and_update_config(args)</code>","text":"<p>Load a configuration file to dictionary and update entries with user supplied arguments.</p> <p>If ''args'' does not contain any value for ''args.config_file'' the default configuration (''isoslam/default_config.yaml'') is loaded, otherwise the user specified configuration is loaded.</p> <p>Once the configuration is loaded any user specified options update the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Arguments supplied by user.</p> required <p>Returns:</p> Type Description <code>dict[str:Any]</code> <p>Dictionary of configuration optionsupdated with user specified options.</p> Source code in <code>isoslam/io.py</code> <pre><code>def load_and_update_config(args: argparse.Namespace | None) -&gt; dict[str, Any]:\n    \"\"\"\n    Load a configuration file to dictionary and update entries with user supplied arguments.\n\n    If ''args'' does not contain any value for ''args.config_file'' the default configuration\n    (''isoslam/default_config.yaml'') is loaded, otherwise the user specified configuration is loaded.\n\n    Once the configuration is loaded any user specified options update the dictionary.\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        Arguments supplied by user.\n\n    Returns\n    -------\n    dict[str: Any]\n        Dictionary of configuration optionsupdated with user specified options.\n    \"\"\"\n    config = read_yaml() if vars(args)[\"config_file\"] is None else read_yaml(vars(args)[\"config_file\"])\n    config[\"schema\"] = _type_schema(config[\"schema\"])  # type: ignore[index]\n    return utils.update_config(config, vars(args))  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/io/#isoslam.io.load_file","title":"<code>load_file(file_path)</code>","text":"<p>Load files of different types.</p> <p>Supports the following file types...</p> <ul> <li><code>.bam</code> - The sequence data that is to be analysed.</li> <li><code>.bed</code> - The locations of introns/splice junctions.</li> <li><code>.gtf</code> - Transcript structures from which the <code>.bed</code> file is derived.</li> <li><code>.vcf</code> - Locations of known sequences difference from the reference sequence.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to file to load.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Returns the loaded file as an object.</p> Source code in <code>isoslam/io.py</code> <pre><code>def load_file(file_path: str | Path) -&gt; Any:\n    \"\"\"\n    Load files of different types.\n\n    Supports the following file types...\n\n    * ``.bam`` - The sequence data that is to be analysed.\n    * ``.bed`` - The locations of introns/splice junctions.\n    * ``.gtf`` - Transcript structures from which the ``.bed`` file is derived.\n    * ``.vcf`` - Locations of known sequences difference from the reference sequence.\n\n    Parameters\n    ----------\n    file_path : str | Path\n        Path to file to load.\n\n    Returns\n    -------\n    Any\n        Returns the loaded file as an object.\n    \"\"\"\n    file_suffix = Path(file_path).suffix\n    if file_suffix == \".gz\":\n        file_suffix = \"\".join(Path(file_path).suffixes)\n    loader = _get_loader(file_suffix)\n    return loader(file_path)\n</code></pre>"},{"location":"api/io/#isoslam.io.load_files","title":"<code>load_files(pattern='**/*.tsv', sep='\\t')</code>","text":"<p>Read a set of files into a list of Pandas DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>File name pattern to search for.</p> <code>'**/*.tsv'</code> <code>sep</code> <code>str</code> <p>Separator/delimiter used in files.</p> <code>'\\t'</code> <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of Pandas DataFrames of each file found.</p> Source code in <code>isoslam/io.py</code> <pre><code>def load_files(pattern: str = \"**/*.tsv\", sep: str = \"\\t\") -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Read a set of files into a list of Pandas DataFrames.\n\n    Parameters\n    ----------\n    pattern : str\n        File name pattern to search for.\n    sep : str\n        Separator/delimiter used in files.\n\n    Returns\n    -------\n    list[pd.DataFrame]\n        A list of Pandas DataFrames of each file found.\n    \"\"\"\n    return {x.stem: pd.read_csv(x, sep=sep) for x in _find_files(pattern)}\n</code></pre>"},{"location":"api/io/#isoslam.io.read_yaml","title":"<code>read_yaml(filename=None)</code>","text":"<p>Read a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Union[str, Path]</code> <p>YAML file to read.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary of the file.</p> Source code in <code>isoslam/io.py</code> <pre><code>def read_yaml(filename: str | Path | None = None) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Read a YAML file.\n\n    Parameters\n    ----------\n    filename : Union[str, Path]\n        YAML file to read.\n\n    Returns\n    -------\n    Dict\n        Dictionary of the file.\n    \"\"\"\n    if filename is None:\n        filename = resources.files(__package__) / \"default_config.yaml\"  # type: ignore[assignment]\n    with Path(filename).open(encoding=\"utf-8\") as f:  # type: ignore[arg-type]\n        try:\n            yaml_file = YAML(typ=\"safe\")\n            return yaml_file.load(f)  # type: ignore[no-any-return]\n        except YAMLError as exception:\n            logger.error(exception)\n            return {}\n</code></pre>"},{"location":"api/io/#isoslam.io.write_assigned_conversions","title":"<code>write_assigned_conversions(assigned_conversions, coverage_counts, read_uid, assignment, outfile, delim)</code>","text":"<p>Write assigned conversions to files.</p> <p>Combines the ''coverage_counts'' with the ''assigned_conversions'' and outputs to disk at the specified location and filename with configurable delimiter.</p> <p>Parameters:</p> Name Type Description Default <code>assigned_conversions</code> <code>set[list[Any]]</code> <p>A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).</p> required <code>coverage_counts</code> <code>dict[str, int] dest_dir: str | Path</code> <p>A dictionary of coverage counts indexed by CHECK.</p> required <code>read_uid</code> <code>int</code> <p>Integer representing the unique read ID.</p> required <code>assignment</code> <code>str</code> <p>Type of assignment, either ''Rep'' or ''Spl'' (for Splice).</p> required <code>outfile</code> <code>Any</code> <p>Open connection to write results to.</p> required <code>delim</code> <code>str</code> <p>Delimiter to be used between fields, typically '','' for ''.csv'' or ''\\t'' for ''.tsv'' output.</p> required Source code in <code>isoslam/io.py</code> <pre><code>def write_assigned_conversions(  # pylint: disable=too-many-positional-arguments\n    assigned_conversions: set[list[Any]],\n    coverage_counts: dict[str, int],\n    read_uid: int,\n    assignment: str,\n    outfile: TextIOWrapper,\n    delim: str,\n) -&gt; None:\n    r\"\"\"\n    Write assigned conversions to files.\n\n    Combines the ''coverage_counts'' with the ''assigned_conversions'' and outputs to disk at the specified location and\n    filename with configurable delimiter.\n\n    Parameters\n    ----------\n    assigned_conversions : set[list[Any]]\n        A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).\n    coverage_counts : dict[str, int] dest_dir: str | Path\n        A dictionary of coverage counts indexed by CHECK.\n    read_uid : int\n        Integer representing the unique read ID.\n    assignment : str\n        Type of assignment, either ''Rep'' or ''Spl'' (for Splice).\n    outfile : Any\n        Open connection to write results to.\n    delim : str\n        Delimiter to be used between fields, typically '','' for ''.csv'' or ''\\t'' for ''.tsv'' output.\n    \"\"\"\n    for transcript_id, position in assigned_conversions:\n        start, end, chromosome, strand = position\n        outfile.write(\n            f\"{read_uid}{delim}{transcript_id}{delim}\"\n            f\"{start}{delim}{end}{delim}{chromosome}{delim}\"\n            f\"{strand}{delim}{assignment}{delim}{coverage_counts['converted_position']}{delim}\"\n            f\"{coverage_counts['convertible']}{delim}{coverage_counts['coverage']}\\n\"\n        )\n</code></pre>"},{"location":"api/io/#isoslam.io.write_yaml","title":"<code>write_yaml(config, output_dir, config_file='config.yaml', header_message=None)</code>","text":"<p>Write a configuration (stored as a dictionary) to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary.</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Path to save the dictionary to as a YAML file (it will be called 'config.yaml').</p> required <code>config_file</code> <code>str</code> <p>Filename to write to.</p> <code>'config.yaml'</code> <code>header_message</code> <code>str</code> <p>String to write to the header message of the YAML file.</p> <code>None</code> Source code in <code>isoslam/io.py</code> <pre><code>def write_yaml(\n    config: dict,  # type: ignore[type-arg]\n    output_dir: str | Path,\n    config_file: str = \"config.yaml\",\n    header_message: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write a configuration (stored as a dictionary) to a YAML file.\n\n    Parameters\n    ----------\n    config : dict\n        Configuration dictionary.\n    output_dir : Union[str, Path]\n        Path to save the dictionary to as a YAML file (it will be called 'config.yaml').\n    config_file : str\n        Filename to write to.\n    header_message : str\n        String to write to the header message of the YAML file.\n    \"\"\"\n    output_config = Path(output_dir) / config_file\n    # Revert PosixPath items to string\n    config = _path_to_str(config)\n\n    if header_message:\n        header = f\"# {header_message} : {_get_date_time()}\\n\" + CONFIG_DOCUMENTATION_REFERENCE\n    else:\n        header = f\"# Configuration from IsoSLAM run completed : {_get_date_time()}\\n\" + CONFIG_DOCUMENTATION_REFERENCE\n    output_config.write_text(header, encoding=\"utf-8\")\n\n    yaml = YAML(typ=\"safe\")\n    with output_config.open(\"a\", encoding=\"utf-8\") as f:\n        try:\n            yaml.dump(config, f)\n        except YAMLError as exception:\n            logger.error(exception)\n</code></pre>"},{"location":"api/isoslam/","title":"IsoSLAM","text":"<p>IsoSLAM module.</p>"},{"location":"api/isoslam/#isoslam.isoslam.append_data","title":"<code>append_data(assigned_conversions, coverage_counts, read_uid, assignment, results, schema)</code>","text":"<p>Create a Polars dataframe combining the ''assigned_conversions'' and ''coverage_counts''.</p> <p>Adds ''assignment'' to the resulting dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>assigned_conversions</code> <code>set[list[Any]]</code> <p>A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).</p> required <code>coverage_counts</code> <code>dict[str, int] dest_dir: str | Path</code> <p>A dictionary of coverage counts indexed by CHECK.</p> required <code>read_uid</code> <code>int</code> <p>Integer representing the unique read ID.</p> required <code>assignment</code> <code>str</code> <p>Type of assignment, either ''Rep'' or ''Spl'' (for Splice).</p> required <code>results</code> <code>DataFrame</code> <p>Polars DataFrame to append data to. This will initially be empty but the schema matches the variables that are added.</p> required <code>schema</code> <code>dict[str, type]</code> <p>Schema dictionary for data frame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Returns a Polars DataFrame of the data structure.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def append_data(  # pylint: disable=too-many-positional-arguments\n    assigned_conversions: frozenset[list[Any]],\n    coverage_counts: dict[str, int],\n    read_uid: int,\n    assignment: str,\n    results: pl.DataFrame,\n    schema: dict[str, type],\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Create a Polars dataframe combining the ''assigned_conversions'' and ''coverage_counts''.\n\n    Adds ''assignment'' to the resulting dataframe.\n\n    Parameters\n    ----------\n    assigned_conversions : set[list[Any]]\n        A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).\n    coverage_counts : dict[str, int] dest_dir: str | Path\n        A dictionary of coverage counts indexed by CHECK.\n    read_uid : int\n        Integer representing the unique read ID.\n    assignment : str\n        Type of assignment, either ''Rep'' or ''Spl'' (for Splice).\n    results : pl.DataFrame\n        Polars DataFrame to append data to. This will initially be empty but the schema matches the variables that are\n        added.\n    schema : dict[str, type]\n        Schema dictionary for data frame.\n\n    Returns\n    -------\n    pl.DataFrame\n        Returns a Polars DataFrame of the data structure.\n    \"\"\"\n    if results is None:\n        results = pl.DataFrame(schema=schema)\n    for transcript_id, position in assigned_conversions:\n        start, end, chromosome, strand = position\n        row = pl.DataFrame(\n            data={\n                \"read_uid\": read_uid,\n                \"transcript_id\": transcript_id,\n                \"start\": start,\n                \"end\": end,\n                \"chr\": chromosome,\n                \"strand\": strand,\n                \"assignment\": assignment,\n                \"conversions\": coverage_counts[\"converted_position\"],\n                \"convertible\": coverage_counts[\"convertible\"],\n                \"coverage\": coverage_counts[\"coverage\"],\n            },\n            schema=schema,\n        )\n        results = pl.concat([results, row])\n    return results.sort(by=[\"read_uid\", \"transcript_id\", \"chr\", \"start\", \"end\"])\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.conversions_per_read","title":"<code>conversions_per_read(read, conversion_from, conversion_to, convertible, converted_position, coverage, vcf_file)</code>","text":"<p>Build sets of genome position for conversions, converted positions and coverage for a given read.</p> <p>Parameters:</p> Name Type Description Default <code>read</code> <code>dict[str, dict[str, Any]]</code> <p>Aligned read.</p> required <code>conversion_from</code> <code>str</code> <p>The base pair the conversion is from, typically either ''T'' or ''C''.</p> required <code>conversion_to</code> <code>str</code> <p>The base pair the conversion is to, typically the opposite pairing of ''from'', i.e. ''A'' or ''C'' respectively.</p> required <code>convertible</code> <code>set</code> <p>Set, possibly empty, to which the genome position is added if the sequence at a given location matches ''conversion_from''.</p> required <code>converted_position</code> <code>set</code> <p>Set, possibly, empty, to which the genome position is added if a conversion has occurred.</p> required <code>coverage</code> <code>set</code> <p>Set, possibly empty, to which the genome position is added for all aligned pairs of a read.</p> required <code>vcf_file</code> <code>VariantFile</code> <p>VCF file.</p> required <p>Returns:</p> Type Description <code>tuple[set[str], set[str], set[str]]</code> <p>Three sets of the ''convertible'', ''converted_position'' and ''coverage''.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def conversions_per_read(  # pylint: disable=too-many-positional-arguments\n    read: AlignedSegment,\n    conversion_from: str,\n    conversion_to: str,\n    convertible: set[str],\n    converted_position: set[str],\n    coverage: set[str],\n    vcf_file: VariantFile,\n) -&gt; tuple[set[str], set[str], set[str]]:\n    \"\"\"\n    Build sets of genome position for conversions, converted positions and coverage for a given read.\n\n    Parameters\n    ----------\n    read : dict[str, dict[str, Any]]\n        Aligned read.\n    conversion_from : str\n        The base pair the conversion is from, typically either ''T'' or ''C''.\n    conversion_to : str\n        The base pair the conversion is to, typically the opposite pairing of ''from'', i.e. ''A'' or ''C''\n        respectively.\n    convertible : set\n        Set, possibly empty, to which the genome position is added if the sequence at a given location matches\n        ''conversion_from''.\n    converted_position : set\n        Set, possibly, empty, to which the genome position is added if a conversion has occurred.\n    coverage : set\n        Set, possibly empty, to which the genome position is added for all aligned pairs of a read.\n    vcf_file : VariantFile\n        VCF file.\n\n    Returns\n    -------\n    tuple[set[str], set[str], set[str]]\n        Three sets of the ''convertible'', ''converted_position'' and ''coverage''.\n    \"\"\"\n    # Ensure we have upper case conversions to compare\n    conversion_from = conversion_from.upper()\n    conversion_to = conversion_to.upper()\n    for read_position, genome_position, genome_sequence in read.get_aligned_pairs(with_seq=True):\n        if None in (read_position, genome_position, genome_sequence):\n            continue\n        coverage.add(genome_position)\n        if genome_sequence.upper() == conversion_from:\n            convertible.add(genome_position)\n\n        # If the sequence at this position has been converted compared to the genome sequence...\n        if read.query_sequence[read_position].upper() == conversion_to and genome_sequence.upper() == conversion_from:\n            # ...check that this is a new variant at this position? Question : Is this the correctinterpretation?\n            variants_at_position = list(vcf_file.fetch(read.reference_name, genome_position, genome_position + 1))\n            if variants_at_position:\n                if any(variant.alts[0].upper() == conversion_to.upper() for variant in variants_at_position):\n                    pass\n                else:\n                    converted_position.add(genome_position)\n            else:\n                converted_position.add(genome_position)\n    # logger.debug(f\"convertible : {convertible}\\nconverted_position : {converted_position}\\ncoverage : {coverage}\")\n    return (convertible, converted_position, coverage)\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.count_conversions_across_pairs","title":"<code>count_conversions_across_pairs(forward_read, reverse_read, vcf_file, forward_conversion=None, reverse_conversion=None)</code>","text":"<p>Count conversions across paired reads.</p> <p>Parameters:</p> Name Type Description Default <code>forward_read</code> <code>dict[str, dict[str, Any]]</code> <p>Aligned segment for forward read.</p> required <code>reverse_read</code> <code>dict[str, dict[str, Any]]</code> <p>Aligned segment for reversed read.</p> required <code>vcf_file</code> <code>VariantFile</code> <p>Variant File.</p> required <code>forward_conversion</code> <code>dict</code> <p>Forward conversion dictionary typically ''{\"from\": \"A\", \"to\": \"G\"}''.</p> <code>None</code> <code>reverse_conversion</code> <code>dict</code> <p>Reverse conversion, typically ''{\"from\": \"T\", \"to\": \"C\"}''.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>Tuple of the number of convertible base pairs, the number of conversions and the coverage of the paired alignments.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>ValueError is raised if either ''forward_conversion'' or ''reverse_conversion'' is ''None''.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def count_conversions_across_pairs(\n    forward_read: dict[str, dict[str, Any]],\n    reverse_read: dict[str, dict[str, Any]],\n    vcf_file: VariantFile,\n    forward_conversion: dict[str, str] | None = None,\n    reverse_conversion: dict[str, str] | None = None,\n) -&gt; dict[str, int]:\n    \"\"\"\n    Count conversions across paired reads.\n\n    Parameters\n    ----------\n    forward_read : dict[str, dict[str, Any]]\n        Aligned segment for forward read.\n    reverse_read : dict[str, dict[str, Any]]\n        Aligned segment for reversed read.\n    vcf_file : VariantFile\n        Variant File.\n    forward_conversion : dict, optional\n        Forward conversion dictionary typically ''{\"from\": \"A\", \"to\": \"G\"}''.\n    reverse_conversion : dict, optional\n        Reverse conversion, typically ''{\"from\": \"T\", \"to\": \"C\"}''.\n\n    Returns\n    -------\n    tuple[int, int, int]\n        Tuple of the number of convertible base pairs, the number of conversions and the coverage of the paired\n      alignments.\n\n    Raises\n    ------\n    ValueError\n        ValueError is raised if either ''forward_conversion'' or ''reverse_conversion'' is ''None''.\n    \"\"\"\n    if forward_conversion is None:\n        raise ValueError(\"forward_conversion can not be empty.\")\n    if reverse_conversion is None:\n        raise ValueError(\"reverse_conversion can not be empty.\")\n\n    # Count conversions on the forward read\n    convertible, converted_position, coverage = conversions_per_read(\n        forward_read,\n        forward_conversion[\"from\"],\n        forward_conversion[\"to\"],\n        convertible=set(),\n        converted_position=set(),\n        coverage=set(),\n        vcf_file=vcf_file,\n    )\n    # Count conversions on the reverse read\n    convertible, converted_position, coverage = conversions_per_read(\n        reverse_read,\n        reverse_conversion[\"from\"],\n        reverse_conversion[\"to\"],\n        convertible,\n        converted_position,\n        coverage,\n        vcf_file,\n    )\n    # logger.debug(\"Counted conversions paired reads\")\n    return {\"convertible\": len(convertible), \"converted_position\": len(converted_position), \"coverage\": len(coverage)}\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_features_from_pair","title":"<code>extract_features_from_pair(pair)</code>","text":"<p>Extract features from a pair of reads.</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>list[AlignedSegment]</code> <p>A list of two aligned segments from <code>pysam</code>.</p> required <p>Returns:</p> Type Description <code>dic[str, dict[str, Any]]</code> <p>Returns a nested dictionaries of the <code>start</code>, <code>end</code> and <code>length</code> of each read.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_features_from_pair(pair: list[AlignedSegment]) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extract features from a pair of reads.\n\n    Parameters\n    ----------\n    pair : list[AlignedSegment]\n        A list of two aligned segments from ``pysam``.\n\n    Returns\n    -------\n    dic[str, dict[str, Any]]\n        Returns a nested dictionaries of the ``start``, ``end`` and ``length`` of each read.\n    \"\"\"\n    return {\n        \"read1\": extract_features_from_read(pair[0]),\n        \"read2\": extract_features_from_read(pair[1]),\n    }\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_features_from_read","title":"<code>extract_features_from_read(read)</code>","text":"<p>Extract start, end and length from an aligned segment read.</p> <p>Parameters:</p> Name Type Description Default <code>read</code> <code>AlignedSegment</code> <p>An aligned segment read from <code>pysam</code>.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of <code>start</code>, <code>end</code> and <code>length</code> of the segment.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_features_from_read(read: AlignedSegment) -&gt; dict[str, int | str | None | tuple[int, int]]:\n    \"\"\"\n    Extract start, end and length from an aligned segment read.\n\n    Parameters\n    ----------\n    read : AlignedSegment\n        An aligned segment read from ``pysam``.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary of ``start``, ``end`` and ``length`` of the segment.\n    \"\"\"\n    block_start, block_end = zip(*read.get_blocks())\n    try:\n        status = read.get_tag(\"XS\")\n    except KeyError:\n        status = None\n    try:\n        transcript = read.get_tag(\"XT\")\n    except KeyError:\n        transcript = None\n    try:\n        reverse = read.is_reverse\n    except KeyError:\n        reverse = None\n    return {\n        \"start\": read.reference_start,\n        \"end\": read.reference_end,\n        \"length\": read.query_length,\n        \"status\": status,\n        \"transcript\": transcript,\n        \"block_start\": block_start,\n        \"block_end\": block_end,\n        \"reverse\": reverse,\n    }\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_segment_pairs","title":"<code>extract_segment_pairs(bam_file)</code>","text":"<p>Extract pairs of AlignedSegments from a <code>.bam</code> file.</p> <p>When there are two adjacent <code>AlignedSegments</code> with the same <code>query_name</code> only the first is paired, subsequent segments are dropped.</p> <p>Parameters:</p> Name Type Description Default <code>bam_file</code> <code>str | Path</code> <p>Path to a <code>.bam</code> file.</p> required <p>Yields:</p> Type Description <code>Generator</code> <p>Itterable of paired segments.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_segment_pairs(bam_file: str | Path) -&gt; Generator[AlignedSegment]:\n    \"\"\"\n    Extract pairs of AlignedSegments from a ``.bam`` file.\n\n    When there are two adjacent ``AlignedSegments`` with the same ``query_name`` only the first is paired, subsequent\n    segments are dropped.\n\n    Parameters\n    ----------\n    bam_file : str | Path\n        Path to a ``.bam`` file.\n\n    Yields\n    ------\n    Generator\n        Itterable of paired segments.\n    \"\"\"\n    previous_read: str | None = None\n    pair: list[AlignedSegment] = []\n    for read in io.load_file(bam_file):\n        # Return pairs of reads, i.e. not on first pass, nor if query_name matches the previous read\n        # Can lead to len(pair) be &gt; 2\n        if previous_read is not None and previous_read != read.query_name:\n            yield pair\n            pair = []\n            previous_read = read.query_name\n        previous_read = read.query_name\n        pair.append(read)\n    # Don't forget to return the last pair!\n    yield pair\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_strand_transcript","title":"<code>extract_strand_transcript(gtf_file)</code>","text":"<p>Extract strand and transcript ID data from <code>.gtf</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>gtf_file</code> <code>Path | str</code> <p>Path to a 'gtf' file.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, tuple[str]], dict[str, tuple[str]]]</code> <p>Two dictionaries are returned, one of the <code>strand</code> the other of the <code>transcript_id</code> both using the <code>gene_id</code> as the key.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_strand_transcript(gtf_file: str | Path) -&gt; tuple[defaultdict[Any, Any], defaultdict[Any, list[Any]]]:\n    \"\"\"\n    Extract strand and transcript ID data from ``.gtf`` file.\n\n    Parameters\n    ----------\n    gtf_file : Path | str\n        Path to a 'gtf' file.\n\n    Returns\n    -------\n    tuple[dict[str, tuple[str]], dict[str, tuple[str]]]\n        Two dictionaries are returned, one of the ``strand`` the other of the ``transcript_id`` both using the\n        ``gene_id`` as the key.\n    \"\"\"\n    strand = defaultdict(str)\n    transcript = defaultdict(list)\n    for entry in io.load_file(gtf_file):\n        if not entry.feature == \"transcript\":\n            continue\n        strand[entry.gene_id] = entry.strand\n        transcript[entry.gene_id].append(entry.transcript_id)\n    logger.info(f\"Extracted features from : {gtf_file}\")\n    return (strand, transcript)\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_transcripts","title":"<code>extract_transcripts(bed_file)</code>","text":"<p>Extract features from <code>.bed</code> file and return as a dictionary indexed by <code>transcript_id</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bed_file</code> <code>str | Path</code> <p>Path, as string or pathlib Path, to a <code>.bed</code> file.</p> required <p>Returns:</p> Type Description <code>dict[Any, list[tuple[Any, int, int, Any, Any]]]</code> <p>Dictionary of <code>chromosome</code>, <code>start</code>, <code>end</code>, <code>transcript_id</code> and <code>bedstrand</code> indexed by <code>transcript_id</code>.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_transcripts(bed_file: str | Path) -&gt; dict[Any, list[tuple[Any, int, int, Any, Any]]]:\n    \"\"\"\n    Extract features from ``.bed`` file and return as a dictionary indexed by ``transcript_id``.\n\n    Parameters\n    ----------\n    bed_file : str | Path\n        Path, as string or pathlib Path, to a ``.bed`` file.\n\n    Returns\n    -------\n    dict[Any, list[tuple[Any, int, int, Any, Any]]]\n        Dictionary of ``chromosome``, ``start``, ``end``, ``transcript_id`` and ``bedstrand`` indexed by\n        ``transcript_id``.\n    \"\"\"\n    coordinates = defaultdict(list)\n    for line in io.load_file(bed_file):\n        contents = line.strip().split(\"\\t\")\n        transcript_id = contents[3].replace(\"_intron\", \"\")\n        coordinates[transcript_id].append(\n            (\n                contents[0],\n                int(contents[1]),\n                int(contents[2]),\n                transcript_id,\n                contents[5],\n            )\n        )\n    logger.info(f\"Extracted features from : {bed_file}\")\n    return coordinates\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_utron","title":"<code>extract_utron(features, gene_transcript, coordinates)</code>","text":"<p>Extract and sum the utrons based on tag.</p> <p>ACTION : This function needs better documentation, my guess is that its extracting the transcripts to genes and then getting some related information (what I'm not sure) from the .bed file and adding these up.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>str</code> <p>A tag from an assigned read.</p> required <code>gene_transcript</code> <code>TextIO</code> <p>Transcript to gene from a <code>.gtf</code> file.</p> required <code>coordinates</code> <code>Any</code> <p>Untranslated region coordinates from a <code>.bed</code> file.</p> required <p>Returns:</p> Type Description <code>list | None</code> <p>List of the length of assigned regions.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_utron(features: dict[str, Any], gene_transcript: Any, coordinates: Any) -&gt; list[tuple[int | str]] | None:\n    \"\"\"\n    Extract and sum the utrons based on tag.\n\n    ACTION : This function needs better documentation, my guess is that its extracting the transcripts to genes and\n    then getting some related information (what I'm not sure) from the .bed file and adding these up.\n\n    Parameters\n    ----------\n    features : str\n        A tag from an assigned read.\n    gene_transcript : TextIO\n        Transcript to gene from a ``.gtf`` file.\n    coordinates : Any\n        Untranslated region coordinates from a ``.bed`` file.\n\n    Returns\n    -------\n    list | None\n        List of the length of assigned regions.\n    \"\"\"\n    if features[\"status\"] == \"Assigned\":\n        untranslated_regions = [coordinates[transcript] for transcript in gene_transcript[features[\"transcript\"]]]\n        return sum(untranslated_regions, [])\n    return []\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.filter_spliced_utrons","title":"<code>filter_spliced_utrons(pair_features, blocks, read='read1')</code>","text":"<p>Filter utrons where start is in the block ends or end is in the block start.</p> <p>Parameters:</p> Name Type Description Default <code>pair_features</code> <code>dict[str, dict]</code> <p>Dictionary of extracted features and utron in both read directions.</p> required <code>blocks</code> <code>dic[str:dict[str, set]]</code> <p>Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.</p> required <code>read</code> <code>str</code> <p>Direction of read to filter on, default is ''read1'' but can also use ''read2''.</p> <code>'read1'</code> <p>Returns:</p> Type Description <code>dict[str, tuple(Any)]</code> <p>Dictionary of the chromosome, start, end and strand of transcripts that are within introns.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def filter_spliced_utrons(\n    pair_features: dict[str, dict[str, Any]],\n    blocks: dict[str, dict[str, tuple[Any, ...]]],\n    read: str = \"read1\",\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Filter utrons where start is in the block ends or end is in the block start.\n\n    Parameters\n    ----------\n    pair_features : dict[str, dict]\n        Dictionary of extracted features and utron in both read directions.\n    blocks : dic[str: dict[str, set]]\n        Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.\n    read : str\n        Direction of read to filter on, default is ''read1'' but can also use ''read2''.\n\n    Returns\n    -------\n    dict[str, tuple(Any)]\n        Dictionary of the chromosome, start, end and strand of transcripts that are within introns.\n    \"\"\"\n    spliced_3ui: dict[str, list[Any]] = {}\n    for chromosome, start, end, transcript_id, strand in pair_features[read][\"utron\"]:\n        if start in blocks[read][\"ends\"] and end in blocks[read][\"starts\"]:\n            # Why add an empty list and append a tuple?\n            if transcript_id not in spliced_3ui:\n                spliced_3ui[transcript_id] = []\n            spliced_3ui[transcript_id].append((start, end, chromosome, strand))\n    return spliced_3ui\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.filter_within_introns","title":"<code>filter_within_introns(pair_features, blocks, read='read1')</code>","text":"<p>Filter utrons that are within introns.</p> <p>Parameters:</p> Name Type Description Default <code>pair_features</code> <code>dict[str, dict]</code> <p>Dictionary of extracted features and utron in both read directions.</p> required <code>blocks</code> <code>dic[str:dict[str, set]]</code> <p>Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.</p> required <code>read</code> <code>str</code> <p>Direction of read to filter on, default is ''read1'' but can also use ''read2''.</p> <code>'read1'</code> <p>Returns:</p> Type Description <code>dict[str, tuple(Any)]</code> <p>Dictionary of the chromosome, start, end and strand of transcripts that are within introns.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def filter_within_introns(\n    pair_features: dict[str, dict[str, Any]],\n    blocks: dict[str, dict[str, tuple[Any, ...]]],\n    read: str = \"read1\",\n) -&gt; dict[str, tuple[Any]]:\n    \"\"\"\n    Filter utrons that are within introns.\n\n    Parameters\n    ----------\n    pair_features : dict[str, dict]\n        Dictionary of extracted features and utron in both read directions.\n    blocks : dic[str: dict[str, set]]\n        Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.\n    read : str\n        Direction of read to filter on, default is ''read1'' but can also use ''read2''.\n\n    Returns\n    -------\n    dict[str, tuple(Any)]\n        Dictionary of the chromosome, start, end and strand of transcripts that are within introns.\n    \"\"\"\n    within_intron: dict[str, Any] = {}\n    for chromosome, start, end, transcript_id, strand in pair_features[read][\"utron\"]:\n        start_end_within_intron = (\n            start &lt;= pair_features[read][\"start\"] &lt;= end or start &lt;= pair_features[read][\"end\"] &lt;= end\n        )\n        spans_intron = (\n            pair_features[read][\"start\"] &lt; start\n            and pair_features[read][\"end\"] &gt; end\n            and (end - start) &lt; pair_features[read][\"length\"]\n        )\n        if (  # pylint: disable=too-many-boolean-expressions\n            (start_end_within_intron or spans_intron)\n            # Start should not be in ends and ends should not be in start, can we combine the start and end block\n            # sets I wonder?\n            and start not in blocks[\"read1\"][\"ends\"]\n            and end not in blocks[\"read1\"][\"starts\"]\n            and start not in blocks[\"read2\"][\"ends\"]\n            and end not in blocks[\"read2\"][\"starts\"]\n        ):\n            # Why add an empty list and append a tuple?\n            if transcript_id not in within_intron:\n                within_intron[transcript_id] = []\n            within_intron[transcript_id].append((start, end, chromosome, strand))\n    return within_intron\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.remove_common_reads","title":"<code>remove_common_reads(retained, spliced)</code>","text":"<p>Remove reads that are common to both retained and spliced sets.</p> <p>Parameters:</p> Name Type Description Default <code>retained</code> <code>set[list[Any]]</code> <p>Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'', ''chromosome'' and ''strand''.</p> required <code>spliced</code> <code>set[list[Any]]</code> <p>Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'', ''chromosome'' and ''strand''.</p> required <p>Returns:</p> Type Description <code>tuple[set[list[Any]], set[list[Any]]]</code> <p>A tuple of the ''retained'' (first) and ''spliced'' reads with common items removed.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def remove_common_reads(\n    retained: frozenset[list[Any]], spliced: frozenset[list[Any]]\n) -&gt; tuple[frozenset[list[Any]], frozenset[list[Any]]]:\n    \"\"\"\n    Remove reads that are common to both retained and spliced sets.\n\n    Parameters\n    ----------\n    retained : set[list[Any]]\n        Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'',\n        ''chromosome'' and ''strand''.\n    spliced : set[list[Any]]\n        Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'',\n        ''chromosome'' and ''strand''.\n\n    Returns\n    -------\n    tuple[set[list[Any]], set[list[Any]]]\n        A tuple of the ''retained'' (first) and ''spliced'' reads with common items removed.\n    \"\"\"\n    common = retained &amp; spliced\n    retained -= common\n    spliced -= common\n    # logger.debug(\"Removed common elements from retained and spliced.\")\n    return (retained, spliced)\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.unique_conversions","title":"<code>unique_conversions(reads1, reads2)</code>","text":"<p>Create a unique set of conversions that are to be retained.</p> <p>Parameters:</p> Name Type Description Default <code>reads1</code> <code>dict[str, list[tuple[Any]]]</code> <p>A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'', ''chromsome'' and ''strand'' recorded.</p> required <code>reads2</code> <code>dict[str, list[tuple[Any]]]</code> <p>A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'', ''chromsome'' and ''strand'' recorded.</p> required <p>Returns:</p> Type Description <code>set[list[Any]]</code> <p>Combines the two sets of observations and de-duplicates them, returning only the unique assigned conversions.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def unique_conversions(\n    reads1: dict[str, Any],\n    reads2: dict[str, Any],\n) -&gt; frozenset[list[Any]]:\n    \"\"\"\n    Create a unique set of conversions that are to be retained.\n\n    Parameters\n    ----------\n    reads1 : dict[str, list[tuple[Any]]]\n        A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'',\n       ''chromsome'' and ''strand'' recorded.\n    reads2 : dict[str, list[tuple[Any]]]\n        A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'',\n       ''chromsome'' and ''strand'' recorded.\n\n    Returns\n    -------\n    set[list[Any]]\n        Combines the two sets of observations and de-duplicates them, returning only the unique assigned conversions.\n    \"\"\"\n    flat1 = [(key, nested_list) for key, values in reads1.items() for nested_list in values]\n    flat2 = [(key, nested_list) for key, values in reads2.items() for nested_list in values]\n    # logger.debug(\"Extracted unique conversions in both reads, combining to unique set.\")\n    return frozenset(flat1 + flat2)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.zip_blocks","title":"<code>zip_blocks(read)</code>","text":"<p>Zip the block starts and ends into two lists.</p> <p>Parameters:</p> Name Type Description Default <code>read</code> <code>AlignedSegment</code> <p>An individual aligned segment read from a ''.bam'' file.</p> required <p>Returns:</p> Type Description <code>tuple[list[int], list[int]]</code> <p>Tuple of two lists of integers the first is start location, the second is the end location.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def zip_blocks(read: AlignedSegment) -&gt; Iterator[tuple[Any, ...]]:\n    \"\"\"\n    Zip the block starts and ends into two lists.\n\n    Parameters\n    ----------\n    read : AlignedSegment\n        An individual aligned segment read from a ''.bam'' file.\n\n    Returns\n    -------\n    tuple[list[int], list[int]]\n        Tuple of two lists of integers the first is start location, the second is the end location.\n    \"\"\"\n    return zip(*read.get_blocks())\n</code></pre>"},{"location":"api/logging/","title":"Logging","text":"<p>Setup and configure logging.</p>"},{"location":"api/logging/#isoslam.logging.setup","title":"<code>setup(level='INFO')</code>","text":"<p>Loguru setup with the required logging level and format.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Log level, default is \"INFO\", other options \"WARNING\", \"DEBUG\" etc.</p> <code>'INFO'</code> Source code in <code>isoslam/logging.py</code> <pre><code>def setup(level: str = \"INFO\") -&gt; None:\n    \"\"\"\n    Loguru setup with the required logging level and format.\n\n    Parameters\n    ----------\n    level : str\n        Log level, default is \"INFO\", other options \"WARNING\", \"DEBUG\" etc.\n    \"\"\"\n    logger.remove()\n    logger.add(sys.stderr)\n    logger.add(\n        sys.stderr,\n        colorize=True,\n        level=level.upper(),\n        format=\"&lt;green&gt;{time:HH:mm:ss}&lt;/green&gt; \"\n        \"| &lt;level&gt;{level}&lt;/level&gt; | \"\n        \"&lt;magenta&gt;{file}&lt;/magenta&gt;:&lt;magenta&gt;{module}&lt;/magenta&gt;:&lt;magenta&gt;{function}&lt;/magenta&gt;:&lt;magenta&gt;{line}&lt;/magenta&gt;\"\n        \" | &lt;level&gt;{message}&lt;/level&gt;\",\n    )\n</code></pre>"},{"location":"api/plotting/","title":"Plotting","text":"<p>Functions for plotting output.</p>"},{"location":"api/processing/","title":"Processing","text":"<p>Entry point, sub-parsers and arguments and processing functions.</p>"},{"location":"api/processing/#isoslam.processing.create_parser","title":"<code>create_parser()</code>","text":"<p>Create a parser for reading options.</p> <p>Parser is created with multiple sub-parsers for eading options to run <code>isoslam</code>.</p> <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>Argument parser.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def create_parser() -&gt; arg.ArgumentParser:\n    \"\"\"\n    Create a parser for reading options.\n\n    Parser is created with multiple sub-parsers for eading options to run ``isoslam``.\n\n    Returns\n    -------\n    arg.ArgumentParser\n        Argument parser.\n    \"\"\"\n    parser = arg.ArgumentParser(\n        description=\"Run various programs related to IsoSLAM. Add the name of the program you wish to run.\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        action=\"version\",\n        version=f\"Installed version of IsoSlam : {__version__}\",\n        help=\"Report the installed version of IsoSLAM.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        type=Path,\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--base-dir\",\n        dest=\"base_dir\",\n        type=Path,\n        required=False,\n        help=\"Base directory to run isoslam on.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output-dir\",\n        dest=\"output_dir\",\n        type=Path,\n        required=False,\n        help=\"Output directory to write results to.\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--log-level\",\n        dest=\"log_level\",\n        type=str,\n        required=False,\n        help=\"Logging level to use, default is 'info' for verbose output use 'debug'.\",\n    )\n\n    subparsers = parser.add_subparsers(title=\"program\", description=\"Available programs listed below:\", dest=\"program\")\n\n    # Create sub-parsers for different stages\n\n    # Process processes all files\n    process_parser = subparsers.add_parser(\n        \"process\",\n        description=\"Process all files and run all summary plotting and statistics.\",\n        help=\"Process all files and run all summary plotting and statistics.\",\n    )\n    process_parser.add_argument(\n        \"-b\",\n        \"--bam-file\",\n        dest=\"bam_file\",\n        type=Path,\n        required=False,\n        help=\"Path to '.bam' file that has undergone read assignment with 'featureCount'.\",\n    )\n    process_parser.add_argument(\n        \"-g\", \"--gtf-file\", dest=\"gtf_file\", type=Path, required=False, help=\"Path to '.gtf' transcript assembly file.\"\n    )\n    process_parser.add_argument(\n        \"-d\",\n        \"--bed-file\",\n        dest=\"bed_file\",\n        type=Path,\n        required=False,\n        help=\"Path to '.bed' utron file. Must be bed6 format.\",\n    )\n    process_parser.add_argument(\n        \"-v\", \"--vcf-file\", dest=\"vcf_file\", type=Path, required=False, help=\"Path to '.vcf.gz' file.\"\n    )\n    process_parser.add_argument(\n        \"-u\",\n        \"--upper-pairs-limit\",\n        dest=\"upper_pairs_limit\",\n        type=int,\n        required=False,\n        help=\"Upper limit of pairs to be processed.\",\n    )\n    process_parser.add_argument(\n        \"-f\",\n        \"--first-matched-limit\",\n        dest=\"first_matched_limit\",\n        type=int,\n        required=False,\n        help=\"Limit of matches.\",\n    )\n    process_parser.add_argument(\n        \"--delim\",\n        dest=\"delim\",\n        type=str,\n        required=False,\n        help=\"Delimiter to use in output.\",\n    )\n    process_parser.add_argument(\n        \"--output-file\",\n        dest=\"output_file\",\n        type=str,\n        required=False,\n        help=\"File to write results to.\",\n    )\n    process_parser.set_defaults(func=process)\n\n    # Create configuration sub-parser\n    create_config_parser = subparsers.add_parser(\n        \"create-config\",\n        description=\"Create a configuration file using the defaults.\",\n        help=\"Create a configuration file using the defaults.\",\n    )\n    create_config_parser.add_argument(\n        \"-f\",\n        \"--filename\",\n        dest=\"filename\",\n        type=Path,\n        required=False,\n        default=\"config.yaml\",\n        help=\"Name of YAML file to save configuration to (default 'config.yaml').\",\n    )\n    create_config_parser.add_argument(\n        \"-o\",\n        \"--output-dir\",\n        dest=\"output_dir\",\n        type=Path,\n        required=False,\n        default=\"./\",\n        help=\"Path to where the YAML file should be saved (default './' the current directory).\",\n    )\n    create_config_parser.set_defaults(func=io.create_config)\n\n    # Summarise counts sub-parser\n    summary_counts_parser = subparsers.add_parser(\n        \"summary-counts\",\n        description=\"Summarise the counts.\",\n        help=\"Summarise the counts.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--file-pattern\",\n        dest=\"file_pattern\",\n        type=str,\n        required=False,\n        default=\"*_summarized.tsv\",\n        help=\"Regular expression for summarized files to process.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--outfile\",\n        dest=\"outfile\",\n        type=Path,\n        required=False,\n        default=\"summary_counts.tsv\",\n        help=\"Output filename to save results to, will be nested under 'output_dir'.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--separator\",\n        dest=\"sep\",\n        type=str,\n        required=False,\n        default=\"\\t\",\n        help=\"Field separator to use in output file, default is '\\t' but other values (e.g. ',' are allowed).\",\n    )\n    summary_counts_parser.set_defaults(func=summarise_counts)\n\n    # Additional parsers for future functionality\n    # summarize_counts_parser = subparsers.add_parser(\n    #     \"summarize\",\n    #     description=\"Summarize counts.\",\n    #     help=\"Summarize counts.\",\n    # )\n    # summarize_counts_parser.set_defaults(func=summarize_counts)\n    # plot_conversions_parser = subparsers.add_parser(\n    #     \"plot_conversions\",\n    #     description=\"Plot conversions.\",\n    #     help=\"Plot conversions.\",\n    # )\n    # plot_conversions_parser.set_defaults(func=plot_conversions)\n    return parser\n</code></pre>"},{"location":"api/processing/#isoslam.processing.entry_point","title":"<code>entry_point(manually_provided_args=None, testing=False)</code>","text":"<p>Entry point for all IsoSLAM programs.</p> <p>Main entry point for running 'isoslam' which allows the different processing, plotting and testing modules to be run.</p> <p>Parameters:</p> Name Type Description Default <code>manually_provided_args</code> <code>None</code> <p>Manually provided arguments.</p> <code>None</code> <code>testing</code> <code>bool</code> <p>Whether testing is being carried out.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Function does not return anything.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def entry_point(manually_provided_args: list[Any] | None = None, testing: bool = False) -&gt; None | arg.Namespace:\n    \"\"\"\n    Entry point for all IsoSLAM programs.\n\n    Main entry point for running 'isoslam' which allows the different processing, plotting and testing modules to be\n    run.\n\n    Parameters\n    ----------\n    manually_provided_args : None\n        Manually provided arguments.\n    testing : bool\n        Whether testing is being carried out.\n\n    Returns\n    -------\n    None\n        Function does not return anything.\n    \"\"\"\n    parser = create_parser()\n    args = parser.parse_args() if manually_provided_args is None else parser.parse_args(manually_provided_args)\n\n    # If no module has been specified print help and exit\n    if not args.program:\n        parser.print_help()\n        sys.exit()\n\n    if testing:\n        return args\n\n    # Run the specified module(s)\n    args.func(args)\n\n    return None\n</code></pre>"},{"location":"api/processing/#isoslam.processing.process","title":"<code>process(args)</code>","text":"<p>Process a set of files.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace | None</code> <p>Arguments function was invoked with.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars Dataframe of results.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def process(\n    args: arg.Namespace | None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Process a set of files.\n\n    Parameters\n    ----------\n    args : arg.Namespace | None\n        Arguments function was invoked with.\n\n    Returns\n    -------\n    pl.DataFrame\n        Polars Dataframe of results.\n    \"\"\"\n    config = io.load_and_update_config(args)\n    logger.remove()\n    if \"log_level\" in vars(args) and vars(args)[\"log_level\"] is not None:\n        logging.setup(level=vars(args)[\"log_level\"])\n    else:\n        logging.setup(level=config[\"log_level\"])\n    validation.validate_config(config=config, schema=validation.DEFAULT_CONFIG_SCHEMA, config_type=\"configuration\")\n\n    # Load files...\n    vcffile = io.load_file(config[\"vcf_file\"])\n    utron_coords = isoslam.extract_transcripts(config[\"bed_file\"])\n    strand_dict, tx2gene = isoslam.extract_strand_transcript(config[\"gtf_file\"])\n    # Setup Polars dataframe\n    results = pl.DataFrame(schema=config[\"schema\"])\n    pairs_processed = 0\n    first_matched = 0\n    read_uid = 1\n    # Process the BAM file\n    for pair in isoslam.extract_segment_pairs(config[\"bam_file\"]):\n        # If there are an excessive number of pairs and/or matches per file we break out\n        if pairs_processed &gt;= config[\"upper_pairs_limit\"]:\n            break\n        if first_matched &gt; config[\"first_matched_limit\"]:\n            break\n        # Perform a number of checks as to whether we should proceed with processing\n        # 1. If we don't have a pair skip\n        #    @ns-rse : I tried unpacking and found there were indeed instances where the \"pairs\" could got upto 8 in\n        #    length?\n        if len(pair) != 2:\n            continue\n        # 2. If either reads of the pair are unmapped skip.\n        read1, read2 = pair\n        if read1.is_unmapped or read2.is_unmapped:\n            continue\n        # Use the intersection of sets, this skips if either read1 or read2 aren't assigned, - or +\n        pair_features = isoslam.extract_features_from_pair(pair)\n        if not {\"Assigned\", \"+\", \"-\"} &amp; {pair_features[\"read1\"][\"status\"], pair_features[\"read2\"][\"status\"]}:\n            continue\n        # Check that only Assigned, + and - status are included\n        status_list = [pair_features[\"read1\"][\"status\"], pair_features[\"read2\"][\"status\"]]\n        if all(status in [\"Assigned\", \"+\", \"-\"] for status in status_list):\n            # If strands are equal assign one to strand\n            if strand_dict[pair_features[\"read1\"][\"transcript\"]] == strand_dict[pair_features[\"read2\"][\"transcript\"]]:\n                strand = strand_dict[pair_features[\"read1\"][\"transcript\"]]\n            # ...if not we skip this pair\n            else:\n                continue\n        # If pairs are not both Assigned/+/- we set strand to the transcript from read1, otherwise its read2\n        else:\n            strand = (\n                strand_dict[pair_features[\"read1\"][\"transcript\"]]\n                if pair_features[\"read1\"][\"status\"] == \"Assigned\"\n                else strand_dict[pair_features[\"read2\"][\"transcript\"]]\n            )\n        # Set forward and reverse reads, if they don't match then we skip\n        if read1.is_reverse and not read2.is_reverse:\n            reverse_read = read1\n            forward_read = read2\n        elif read2.is_reverse and not read1.is_reverse:\n            reverse_read = read2\n            forward_read = read1\n        else:\n            # Not proper pair\n            continue\n\n        # We _haven't_ skipped any pairs so increment the pair counter\n        pairs_processed += 1\n        # Processing now begins...\n        # Extract utron for the gene\n        pair_features[\"read1\"][\"utron\"] = isoslam.extract_utron(\n            features=pair_features[\"read1\"], gene_transcript=tx2gene, coordinates=utron_coords\n        )\n        pair_features[\"read2\"][\"utron\"] = isoslam.extract_utron(\n            features=pair_features[\"read2\"], gene_transcript=tx2gene, coordinates=utron_coords\n        )\n        # Get blocks\n        block_starts1, block_ends1 = isoslam.zip_blocks(read1)\n        block_starts2, block_ends2 = isoslam.zip_blocks(read2)\n        blocks = {\n            \"read1\": {\"starts\": block_starts1, \"ends\": block_ends1},\n            \"read2\": {\"starts\": block_starts2, \"ends\": block_ends2},\n        }\n        # Retain within introns\n        read1_within_intron = isoslam.filter_within_introns(pair_features, blocks, read=\"read1\")\n        read2_within_intron = isoslam.filter_within_introns(pair_features, blocks, read=\"read2\")\n        # Retain spliced\n        read1_spliced_3UI = isoslam.filter_spliced_utrons(pair_features, blocks, read=\"read1\")\n        read2_spliced_3UI = isoslam.filter_spliced_utrons(pair_features, blocks, read=\"read2\")\n        # List of all dictionaries\n        retained_reads = [read1_within_intron, read2_within_intron, read1_spliced_3UI, read2_spliced_3UI]\n        # Check that there are some retained regions (all dictionaries would be empty if there are none retained and\n        # not {} evaluates to True, hence wrapping in all())\n        if all(not contents for contents in retained_reads):\n            continue\n        # We have got a match so increment counts\n        first_matched += 1\n\n        # Unique conversions within introns to be retained\n        assign_conversions_to_retained = isoslam.unique_conversions(read1_within_intron, read2_within_intron)\n        # Unique conversions within 3UI to be retained\n        assign_conversions_to_spliced = isoslam.unique_conversions(read1_spliced_3UI, read2_spliced_3UI)\n        ## If there are any events in both we want to remove them - this should be rare\n        assign_conversions_to_retained, assign_conversions_to_spliced = isoslam.remove_common_reads(\n            assign_conversions_to_retained, assign_conversions_to_spliced\n        )\n        # If we are mapped to a +ve stranded transcript, then count T&gt;C in the forward read and A&gt;G in the reverse read.\n        if strand == \"+\":\n            coverage_counts = isoslam.count_conversions_across_pairs(\n                forward_read=forward_read,\n                reverse_read=reverse_read,\n                vcf_file=vcffile,\n                forward_conversion=config[\"forward_reads\"],\n                reverse_conversion=config[\"reverse_reads\"],\n            )\n        elif strand == \"-\":\n            # If we are mapped to a -ve stranded transcript, count T&gt;C in the reverse read and A&gt;G in the forward read.\n            # NB - can either flip the reads or the conversion that are passed in, here we flip the read\n            coverage_counts = isoslam.count_conversions_across_pairs(\n                forward_read=reverse_read,\n                reverse_read=forward_read,\n                vcf_file=vcffile,\n                forward_conversion=config[\"forward_reads\"],\n                reverse_conversion=config[\"reverse_reads\"],\n            )\n        else:\n            # should not be possible - but just in case\n            pass\n\n        results = isoslam.append_data(\n            assigned_conversions=assign_conversions_to_retained,\n            coverage_counts=coverage_counts,  # pylint: disable=possibly-used-before-assignment\n            read_uid=read_uid,\n            assignment=\"Ret\",\n            results=results,\n            schema=config[\"schema\"],\n        )\n        results = isoslam.append_data(\n            assigned_conversions=assign_conversions_to_spliced,\n            coverage_counts=coverage_counts,  # pylint: disable=possibly-used-before-assignment\n            read_uid=read_uid,\n            assignment=\"Spl\",\n            results=results,\n            schema=config[\"schema\"],\n        )\n        read_uid += 1\n\n    results = results.sort(by=[\"read_uid\", \"transcript_id\", \"chr\", \"start\", \"end\"])\n    io.data_frame_to_file(data=results, output_dir=config[\"output_dir\"], outfile=config[\"output_file\"])\n\n    return results\n</code></pre>"},{"location":"api/processing/#isoslam.processing.summarise_counts","title":"<code>summarise_counts(args)</code>","text":"<p>Take a set of output files and summarise the number of conversions.</p> <p>Counts are made within file, chromosome, transcript, start, end, assignment and whether there is one or more conversion observed.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace | None</code> <p>Arguments function was invoked with.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Function does not return anything.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def summarise_counts(args: arg.Namespace | None) -&gt; None:\n    \"\"\"\n    Take a set of output files and summarise the number of conversions.\n\n    Counts are made within file, chromosome, transcript, start, end, assignment and whether there is one or more\n    conversion observed.\n\n    Parameters\n    ----------\n    args : arg.Namespace | None\n        Arguments function was invoked with.\n\n    Returns\n    -------\n    None\n        Function does not return anything.\n    \"\"\"\n    # Load the configuration file (default or user) and update with supplied flags\n    config = io.load_and_update_config(args)\n    logger.remove()\n    if vars(args)[\"log_level\"] is not None:\n        logging.setup(level=vars(args)[\"log_level\"])\n    else:\n        logging.setup(level=config[\"log_level\"])\n    summary_counts_config = config[\"summary_counts\"]\n    output_config = summary_counts_config.pop(\"output\")\n    output_config[\"output_dir\"] = config[\"output_dir\"]\n    summary_counts = summary.summary_counts(**summary_counts_config)\n    summary_counts.sort_values(by=[\"Chr\", \"Transcript_id\", \"Start\"], inplace=True)\n    io.data_frame_to_file(summary_counts, **output_config)\n    logger.info(f\"Summary counts file written to : {output_config['output_dir']}/{output_config['outfile']}\")\n</code></pre>"},{"location":"api/summary/","title":"Summary","text":"<p>Functions for summarising output.</p>"},{"location":"api/summary/#isoslam.summary.append_files","title":"<code>append_files(pattern='**/*.tsv', separator='\\t')</code>","text":"<p>Append a set of files into a Pandas DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>File name pattern to search for.</p> <code>'**/*.tsv'</code> <code>separator</code> <code>str</code> <p>Separator/delimiter used in files.</p> <code>'\\t'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrames of each file found.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def append_files(pattern: str = \"**/*.tsv\", separator: str = \"\\t\") -&gt; pd.DataFrame:\n    \"\"\"\n    Append a set of files into a Pandas DataFrames.\n\n    Parameters\n    ----------\n    pattern : str\n        File name pattern to search for.\n    separator : str\n        Separator/delimiter used in files.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrames of each file found.\n    \"\"\"\n    _data = io.load_files(pattern, separator)\n    all_data = [data.assign(filename=key) for key, data in _data.items()]\n    return pd.concat(all_data)\n</code></pre>"},{"location":"api/summary/#isoslam.summary.summary_counts","title":"<code>summary_counts(file_pattern='**/*.tsv', separator='\\t', groupby=None, dropna=True)</code>","text":"<p>Count the number of assigned read pairs.</p> <p>Groups the data by</p> <p>Parameters:</p> Name Type Description Default <code>file_pattern</code> <code>str</code> <p>File name pattern to search for.</p> <code>'**/*.tsv'</code> <code>separator</code> <code>str</code> <p>Separator/delimiter used in files.</p> <code>'\\t'</code> <code>groupby</code> <code>list[str]</code> <p>List of variables to group the counts by.</p> <code>None</code> <code>dropna</code> <code>book</code> <p>Whether to drop rows with <code>NA</code> values.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Pandas DataFrames of each file found.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def summary_counts(\n    file_pattern: str = \"**/*.tsv\",\n    separator: str = \"\\t\",\n    groupby: list[str] | None = None,\n    dropna: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Count the number of assigned read pairs.\n\n    Groups the data by\n\n    Parameters\n    ----------\n    file_pattern : str\n        File name pattern to search for.\n    separator : str\n        Separator/delimiter used in files.\n    groupby : list[str]\n        List of variables to group the counts by.\n    dropna : book\n        Whether to drop rows with ``NA`` values.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrames of each file found.\n    \"\"\"\n    if groupby is None:\n        groupby = [\"Transcript_id\", \"Chr\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"Conversions\", \"filename\"]\n    _data = append_files(file_pattern, separator)\n    _data[\"one_or_more_conversion\"] = _data[\"Conversions\"] &gt;= 1\n    groupby.append(\"one_or_more_conversion\")\n    return _data.value_counts(subset=groupby, dropna=dropna).reset_index()\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":"<p>Utilities and helper functionsfor IsoSLAM.</p>"},{"location":"api/utils/#isoslam.utils.update_config","title":"<code>update_config(config, args)</code>","text":"<p>Update the configuration with any arguments.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary of configuration (typically read from YAML file specified with '-c/--config '). required <code>args</code> <code>Namespace</code> <p>Command line arguments.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary updated with command arguments.</p> Source code in <code>isoslam/utils.py</code> <pre><code>def update_config(config: dict[str, Any], args: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Update the configuration with any arguments.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary of configuration (typically read from YAML file specified with '-c/--config &lt;filename&gt;').\n    args : Namespace\n        Command line arguments.\n\n    Returns\n    -------\n    dict\n        Dictionary updated with command arguments.\n    \"\"\"\n    args = vars(args) if isinstance(args, Namespace) else args\n    args_keys = args.keys()\n    for config_key, config_value in config.items():\n        if isinstance(config_value, dict):\n            update_config(config_value, args)\n        else:\n            if config_key in args_keys and args[config_key] is not None and config_value is not args[config_key]:\n                original_value = config[config_key]\n                config[config_key] = args[config_key]\n                logger.info(f\"Updated config config[{config_key}] : {original_value} &gt; {args[config_key]} \")\n    if \"base_dir\" in config.keys():\n        config[\"base_dir\"] = io._str_to_path(config[\"base_dir\"])  # pylint: disable=protected-access\n    if \"output_dir\" in config.keys():\n        config[\"output_dir\"] = io._str_to_path(config[\"output_dir\"])  # pylint: disable=protected-access\n    return config\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>This document describes how to contribute to the development of this software.</p>"},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>If you find a bug we need to know about it so we can fix it. Please report your bugs on our GitHub Issues page.</p>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>If you find IsoSLAM useful but think it can be improved you can make a feature request.</p>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<p>If you would like to fix a bug or add a new feature that is great, Pull Requests are very welcome.</p> <p>However, we have adopted a number of good software development practises that ensure the code and documentation is linted and that unit and regression tests pass both locally and on Continuous Integration. The rest of this page helps explain how to set yourself up with these various tools.</p>"},{"location":"contributing/#virtual-environments","title":"Virtual Environments","text":"<p>Use of virtual environments, particularly during development of Python packages, is encouraged. There are lots of options out there for you to choose from including...</p> <ul> <li>Miniconda</li> <li>venv</li> <li>virtualenvwrapper</li> </ul> <p>Which you choose is up to you, although you should be wary of using the Miniconda distribution from Anaconda if any of your work is carried out for or in conjunction with a commercial entity.</p>"},{"location":"contributing/#uv","title":"uv","text":"<p>Developers are using the uv package manager to setup and control environments to which end a <code>uv.lock</code> file is included in the repository. uv supports managing virtual environments so you may wish to install and use this tool at the system level to manage your virtual environments for this package.</p>"},{"location":"contributing/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Once you have setup your virtual environment you should clone the repository from GitHub</p> <pre><code>cd ~/path/you/want/to/clone/to\ngit clone https://github.com/sudlab/IsoSLAM\n</code></pre>"},{"location":"contributing/#install-development","title":"Install development","text":"<p>Once you have clone the IsoSLAM repository you should install all the package along with all development and documentation dependencies in \"editable\" mode. This means you can test the changes you make in real time.</p> <pre><code>cd IsoSLAM\npip install --no-cache-dir -e .[docs,dev]\n</code></pre>"},{"location":"contributing/#git","title":"Git","text":"<p>Git is used to version control development of the package. The <code>main</code> branch on GitHub and the pre-commit hooks have protections in place that prevent committing/pushing directly to the <code>main</code> branch. This means you should create a branch to undertake development or fix bugs.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Ideally an issue should have been created detailing the feature request. If it is a large amount of work this should be captured in an issue labelled \"Epic\" and the steps taken to achieve all work broken down into smaller issues.</p>"},{"location":"contributing/#branch-nomenclature","title":"Branch nomenclature","text":"<p>When undertaking work on a particular issue it is useful to use informative branch names. These convey information about what the branch is for beyond simply \"<code>adding-feature-x</code>\". You should create the branch using your GitHub username, followed by the issue number and a short description of the work being undertaken. For example <code>ns-rse/31-merge-slam-3uis</code> as this allows others to know who has been undertaking the work, what issue the work relates to and has an informative name as to the nature of that work.</p>"},{"location":"contributing/#ignoring-revisions","title":"Ignoring Revisions","text":"<p>If you make small stylistic changes to code, e.g. during linting, and you wish the git blame to attribute the code to the original author you can ignore your commit by adding it to the repositories <code>.git-blame-ignore-revs</code> file and enabling the use of it in your repositories configuration with the following command.</p> <pre><code>git config blame.ignoreRevsFile .git-blame-ignore-revs\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p>Linting is the practice of following a consistent coding style. For Python that style is defined in PEP8. By following a consistent style across a code base it is easier to read and understand the code written by others (including your past self!). We use the following linters implemented as pre-commit hooks</p> <ul> <li>Python</li> <li>Black</li> <li>Blacken-docs</li> <li>flake8</li> <li>Numpydoc</li> <li>Ruff</li> <li>Other</li> <li>Codespell (Spelling across all filesyy)</li> <li>markdownlint-cli2 (Markdown)</li> <li>prettier (Markdown, YAML)</li> </ul> <p>NB It is important that Python files lint with <code>black</code> otherwise the API section of this website will not automatically build the web-pages for that section.</p>"},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>Style checks are made using the pre-commit framework which is one of the development dependencies and should have been installed in the previous step. You can check if its is installed in your virtual environment with <code>pip show pre-commit</code>. If you have pre-commit installed install the hook using...</p> <pre><code>pre-commit install\n</code></pre> <p>This adds a file to <code>.git/hooks/pre-commit</code> that will run all of the hooks specified in <code>.pre-commit-config.yaml</code>. The first time these are run it will take a little while as a number of virtual environments are downloaded for the first time. It might be a good time to run these manually on the code base you have just cloned which should pass all checks.</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>These should all pass. Now whenever you try to make a <code>git commit</code> these checks will run before the commit is made and if any fail you will be advised of what has failed. Some of the linters such as Black and Ruff will automatically correct any errors that they find and you will have to stage the files that have changed again. Not all errors can be automatically corrected (e.g. Numpydoc validation and Pylint) and you will have to manually correct these.</p>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>It is sensible and easiest to write informative docstrings when first defining your modules, classes and methods/functions. Doing so is a useful adie-memoire not only for others but your future self and with modern Language Servers that will, on configuration, show you the docstrings when using the functions it helps save time.</p> <p>You will find your commits fail the numpydoc-validation pre-commit hook if you do not write docstrings and will be prompted to add one.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>We use the pytest framework with various plugins in our testing suite. When correcting bugs and adding features at a bare minimum the existing tests should not fail. Where possible we would be grateful of contributions to the test suite. This means if an edge case has been identified and a solution derived a test is added that checks the edge case is correctly handled. For new features would ideally mean writing unit-tests to ensure each function or method works as intended and for larger classes that behaviour is as expected. Sometimes tests will need updating in light of bug fixes and features which is to be expected, but remember to commit updates to tests as well as to code to ensure the Continuous Integration tests pass.</p>"},{"location":"contributing/#pytest-testmon","title":"Pytest-testmon","text":"<p>To shorten the feedback loop during development the pytest-testmon plugin is used as a pre-commit hook so that only the tests affected by the changes that are being committed are run. This requires that on first installing the package you create a local database of the state of the tests by running the following...</p> <pre><code>pytest --test-mon\n</code></pre> <p>This creates the files <code>.testmondata</code> which stores the current state of tests. Once created commits will only run affected tests. However if your environment has changed, such as adding new packages or updating installed packages you will have to recreate the database.</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Once you have made your changes and committed them you will at some point wish to make a Pull Request to merge them into the <code>main</code> branch.</p> <p>In order to keep Git history clean and easier to understand you can perform an interactive <code>git rebase -i</code> on your feature branch to squash related commits and tidy up your commit history.</p> <p>When your branch is ready for merging with <code>main</code> open a Pull Request. You can use the GitHub keywords of <code>close[s|d]</code>/<code>fix[es|ed]</code> / <code>resolve[s|d]</code> followed by the issue number in the body of your commit message which will change the status of the issue to \"Closed\" when the Pull Request is merged.</p> <p>Pull Requests will be reviewed in a timely and hopefully constructive manner.</p>"},{"location":"contributing/adding_modules/","title":"Adding modules","text":"<p>This document describes how to add additional post-processing modules to summarise, model or plot the results. As the processing pipeline uses Python these steps are undertaken using Pandas, Polars, plotnine and statsmodels but other frameworks can be used to extend functionality such as Scikit-learn (or any other Python module!).</p> <p>You should set yourself up with a development environment as described in the contributing section so that linting and pre-commit hooks will run locally, shortening the development feedback loop.</p>"},{"location":"contributing/adding_modules/#organising-modules","title":"Organising Modules","text":"<p>Typically the functionality that is likely to be added will be some aspect that summarises or plots the post-processing results. To which end there is the <code>isoslam.summary</code> and <code>isoslam.plotting</code> modules to which functionality should be added. There is no harm in adding a new module, particularly if there are a large number of functions that are required but you may wish to consider adding the entry point function to one of these modules.</p>"},{"location":"contributing/adding_modules/#parameters","title":"Parameters","text":"<p>The parameters should be clearly defined with typehints as the pre-commit hooks will fail (if not locally then in Continuous Integration which blocks merging). The nomenclature used for parameters should be the basis of configuration options used (see next section). This makes it possible to leverage <code>**kwargs</code> to pass options loaded from the configuration dictionary and updated from the command line, through to the functions.</p>"},{"location":"contributing/adding_modules/#configuration","title":"Configuration","text":"<p>Configuration options should be added to <code>isoslam/default_config.yaml</code>. A section should be defined for the module you are adding, in this worked example we are creating the <code>plot_conversions</code> and so we would add a section that corresponds to the arguments required for plotting. The function name is the top-level and options for this module are nested within.</p> <pre><code>plot_conversions:\n  group_by: \"read\"\n  theme: \"classic\"\n</code></pre>"},{"location":"contributing/adding_modules/#validation","title":"Validation","text":"<p>There is a validation module in place <code>isoslam.validation</code> which checks that the parameters in the <code>default_config.yaml</code>, a user supplied configuration or command line options are of the expected type. You need to add the options you have added to <code>isoslam/default_config.yaml</code> to the <code>DEFAULT_CONFIG_SCHEMA</code> that is defined in the <code>isoslam.validation</code> module. The examples there should be informative for writing/adding new dictionary entries. The keys are the fields expected in the configuration, the values are the expected types or the <code>schema.Or()</code> function which states the type(s)/values that are permitted and lists an <code>error=\"&lt;error message&gt;\"</code> that is displayed if the condition is not met. For the above additional configuration you would add the following to the <code>DEFAULT_CONFIG_SCHEMA</code>, nesting the options as reflected in the configuration structure.</p> <pre><code>DEFAULT_CONFIG_SCHEMA = Schema(\n    {\n        \"plot_conversions\": {\n            \"group_by\": Or(\n                \"read\",\n                \"pair\",\n                error=\"Invalid value in config for plot.conversions.group_by, valid values are 'read' or 'pair'\",\n            ),\n            \"theme\": Or(\n                \"classic\",\n                \"bw\",\n                error=\"Invalid value in config for plot.theme, valid values are 'classic' or 'bw'\",\n            ),\n        }\n    }\n)\n</code></pre>"},{"location":"contributing/adding_modules/#entry-points","title":"Entry Points","text":"<p>To make the module available at the command line, and in turn possible to integrate into the CGAT pipeline with you need what is known in Python packaging as an entry point. This is a method of providing a simple command line interface to access your program and sub-modules so that they do not need prefixing with <code>python -m</code>. The module where this is setup is <code>isoslam.processing</code> where you will see there is a <code>create_config()</code> function which creates an argument parser along with sub-parsers. The new function you are adding will be added as a sub-parser, in the example below we add a sub-parser for plotting the number of conversions per read.</p> <p>There should be one argument for every configuration option defined in <code>default_config.yaml</code>, which in turn mirrors the options used in the functions that you call, and each <code>dest</code> should match these names. The updating of the configuration based on command line options is contingent on these aligning.</p> <pre><code>def create_parser() -&gt; arg.ArgumentParser:\n    \"\"\"\n    Create a parser for reading options.\n\n    Parser is created with multiple sub-parsers for eading options to run ``isoslam``.\n\n    Returns\n    -------\n    arg.ArgumentParser\n        Argument parser.\n    \"\"\"\n    ...\n\n    # Plot conversions per read\n    plot_conversions = subparsers.add_parser(\n        \"plot-conversions-per-read\",\n        description=\"Plot the conversions per read.\",\n        help=\"Plot the conversions per read.\",\n    )\n    plot_conversions_parser.add_argument(\n        \"--file-pattern\",\n        dest=\"file_pattern\",\n        type=str,\n        required=False,\n        default=\"*_summarized.tsv\",\n        help=\"Regular expression for summarized files to process.\",\n    )\n    plot_conversions_parser.add_argument(\n        \"--outfile\",\n        dest=\"outfile\",\n        type=Path,\n        required=False,\n        default=\"conversions.png\",\n        help=\"Output filename to save results to, will be nested under 'output_dir'.\",\n    )\n    plot_conversions_parser.add_argument(\n        \"--separator\",\n        dest=\"sep\",\n        type=str,\n        required=False,\n        default=\"\\t\",\n        help=\"Field separator to use in output file, default is '\\t' but other values (e.g. ',' are allowed).\",\n    )\n    plot_conversions.set_defaults(func=plot_conversions)\n</code></pre> <p>This sets up the subparser <code>plot_conversions_parser</code> which has three optional arguments to specify the <code>file_pattern</code> to be searched for and subsequently loaded, the <code>outfile</code> name which will be nested under the <code>output_dir</code> (which is an argument to the <code>isoslam</code> entry point) and the <code>separator</code> that is used in the files. Finally a default function is set, in this case <code>plot_conversions</code>.</p> <p>The <code>plot_conversions()</code> function, which we will define within the processing module does the work of calling the module you have added. The only argument it needs is <code>args</code> which will be the <code>arguments.Namespace</code> that is created by the argument parser. These are used to update the default options which read from the <code>isoslam/default_config.yaml</code> with values the user enters which is the validated with a call to <code>validation.validate_config()</code>.</p> <p>The task of plotting conversions requires that we first load a series of files, combine them and summarise them which are the first set of steps taken, including some subsetting of configuration options. This data is then summarized and plotted using the functions defined and imported from the <code>isoslam.summary</code> module and the <code>isoslam.plotting</code> modules.</p> <pre><code>from isoslam import plotting as plot\nfrom isoslam import summary, validation\n\n\ndef plot_conversions(args: arg.Namespace | None) -&gt; None:\n    \"\"\"\n    Take a set of output files and summarise the number of conversions.\n\n    Counts are made within file, chromosome, transcript, start, end, assignment and whether there is one or more\n    conversion observed.\n\n    Parameters\n    ----------\n    args : arg.Namespace | None\n        Arguments function was invoked with.\n\n    Returns\n    -------\n    None\n        Function does not return anything.\n    \"\"\"\n    config = io.load_and_update_config(args)\n    logger.remove()\n    if vars(args)[\"log_level\"] is not None:\n        logging.setup(level=vars(args)[\"log_level\"])\n    else:\n        logging.setup(level=config[\"log_level\"])\n    # Validate the configuration\n    validation.validate_config(\n        config=config,\n        schema=validation.DEFAULT_CONFIG_SCHEMA,\n        config_type=\"configuration\",\n    )\n    # Load and summarise the data\n    plot_conversions_config = config[\"plot_conversions\"]\n    output_config = summary_counts_config.pop(\"output\")\n    output_config[\"output_dir\"] = config[\"output_dir\"]\n    plot.conversions(**plot_conversions_config)\n    logger.info(\n        f\"Conversions per read plotted to : {output_config['output_dir]}/{output_config['outfile]}\"\n    )\n</code></pre>"}]}