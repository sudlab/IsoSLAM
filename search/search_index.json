{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to IsoSLAM","text":"<ul> <li>Introduction</li> <li>Installation</li> <li>Usage</li> <li>Workflow</li> <li>Contributing</li> <li>Extending</li> <li>API</li> </ul>"},{"location":"extending/","title":"Extending IsoSLAM","text":"<p>The modular nature of IsoSLAM and its use of <code>ruffus</code> and <code>cgat</code> mean that it is relatively straight-forward to add additional steps or processing.</p>"},{"location":"extending/#overview","title":"Overview","text":"<ol> <li>Add a new module to <code>isoslam/&lt;module_name&gt;.py</code> and write functions, include numpydoc    strings so the functions/classes are documented.</li> <li>Add all options to <code>isoslam/default_config.yaml</code>.</li> <li>Add a <code>sub-parser</code> to <code>isoslam/processing.py</code> with command line options for all arguments to your function.</li> <li>Add a <code>process_&lt;module_name&gt;</code> function to <code>isoslam/processing.py</code>.</li> <li>Add an entry to the documentation to build the API documentation automatically.</li> </ol> <p>By way of example the implementation of the <code>isoslam summary</code> sub-command is explained.</p>"},{"location":"extending/#adding-a-module","title":"Adding a module","text":"<p>This is probably the most flexible part, you can add the module as you see fit. You can use Object Orientated approach and write a class or classes or functional programming and a series.</p> <p>However you will need a single function that takes the input and various options.</p>"},{"location":"extending/#example","title":"Example","text":"<p>The <code>summary</code> module appends multiple files produced from running IsoSLAM on a series of inputs and appends the data. These are then summarised by a set of variables to give the number of counts.</p>"},{"location":"extending/#isoslamsummarypy","title":"<code>isoslam/summary.py</code>","text":"<pre><code>\"\"\"Functions for summarising output.\"\"\"\n\nimport pandas as pd\n\nfrom isoslam import io\n\n\ndef append_files(pattern: str = \"**/*.tsv\", separator: str = \"\\t\") -&gt; pd.DataFrame:\n    \"\"\"\n    Append a set of files into a Pandas DataFrames.\n\n    Parameters\n    ----------\n    pattern : str\n        File name pattern to search for.\n    separator : str\n        Separator/delimiter used in files.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrames of each file found.\n    \"\"\"\n    _data = io.load_files(pattern, separator)\n    all_data = [data.assign(filename=key) for key, data in _data.items()]\n    return pd.concat(all_data)\n\n\ndef summary_counts(\n    file_pattern: str = \"**/*.tsv\",\n    separator: str = \"\\t\",\n    groupby: list[str] | None = None,\n    dropna: bool = True,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Count the number of assigned read pairs.\n\n    Groups the data by\n\n    Parameters\n    ----------\n    file_pattern : str\n        File name pattern to search for.\n    separator : str\n        Separator/delimiter used in files.\n    groupby : list[str]\n        List of variables to group the counts by.\n    dropna : book\n        Whether to drop rows with ``NA`` values.\n\n    Returns\n    -------\n    pd.DataFrame\n        A Pandas DataFrames of each file found.\n    \"\"\"\n    if groupby is None:\n        groupby = [\n            \"Transcript_id\",\n            \"Chr\",\n            \"Strand\",\n            \"Start\",\n            \"End\",\n            \"Assignment\",\n            \"Conversions\",\n            \"filename\",\n        ]\n    _data = append_files(file_pattern, separator)\n    _data[\"one_or_more_conversion\"] = _data[\"Conversions\"] &gt;= 1\n    groupby.append(\"one_or_more_conversion\")\n    return _data.value_counts(subset=groupby, dropna=dropna).reset_index()\n</code></pre> <p>This included writing a function to search for files with a given <code>pattern</code> and load them using the specified <code>separator</code>. As this is an Input/Output operation the functions were added to the <code>isoslam/io.py</code> module.</p>"},{"location":"extending/#iopy","title":"<code>io.py</code>","text":"<pre><code>def _find_files(pattern: str = \"**/*.tsv\") -&gt; Generator:  # type: ignore[type-arg]\n    \"\"\"\n    Find files that match the given pattern.\n\n    Parameters\n    ----------\n    pattern : str\n        Pattern (regular expression) of files to search for.\n\n    Returns\n    -------\n    Generator[_P, None, None]\n        A generator of files found that match the given pattern.\n    \"\"\"\n    pwd = Path.cwd()\n    return pwd.rglob(pattern)\n\n\ndef load_files(pattern: str = \"**/*.tsv\", sep: str = \"\\t\") -&gt; dict[str, pd.DataFrame]:\n    \"\"\"\n    Read a set of files into a list of Pandas DataFrames.\n\n    Parameters\n    ----------\n    pattern : str\n        File name pattern to search for.\n    sep : str\n        Separator/delimiter used in files.\n\n    Returns\n    -------\n    list[pd.DataFrame]\n        A list of Pandas DataFrames of each file found.\n    \"\"\"\n    return {x.stem: pd.read_csv(x, sep=sep) for x in _find_files(pattern)}\n</code></pre>"},{"location":"extending/#add-options-to-isoslamdefault_configyaml","title":"Add options to <code>isoslam/default_config.yaml</code>","text":"<p>We want to be consistent across the configuration file, which resides in <code>isoslam/default_config.yaml</code> and is used when generating configurations using <code>isoslam create-config</code>. To do so the function parameters, in this example <code>summary_counts()</code>, should be used as entries in the <code>isoslam/default_config.yaml</code>.</p>"},{"location":"extending/#example_1","title":"Example","text":"<p>The top level of a modules configuration should match the module name, here <code>summary_counts</code>. Each entry is a key/value pair that corresponds to the arguments of the function, and so we have <code>file_pattern</code>, <code>separator</code>, <code>groupby</code> and <code>output</code> with their various options.</p> <pre><code>summary_counts:\n  file_pattern: \"**/*.tsv\"\n  separator: \"\\t\"\n  groupby:\n    - Transcript_id\n    - Chr\n    - Strand\n    - Start\n    - End\n    - Assignment\n    - Conversions\n    - filename\n  output:\n    outfile: summary_counts.tsv\n    sep: \"\\t\"\n    index: false\n</code></pre>"},{"location":"extending/#add-a-sub-parser-to-isoslamprocessingpy","title":"Add a sub-parser to <code>isoslam/processing.py</code>","text":"<p>The function <code>create_parser()</code> is responsible for creating the <code>isoslam</code> arguments and sub-parsers and their associated arguments.</p> <p>Define a sub-parser and <code>add_argument()</code> for each option that is available. Keep names consistent with the arguments of the main function you have written above and in turn the configuration values in <code>isoslam/default_config.yaml</code>.</p>"},{"location":"extending/#example_2","title":"Example","text":"<pre><code># Summarise counts sub-parser\nsummary_counts_parser = subparsers.add_parser(\n    \"summary-counts\",\n    description=\"Summarise the counts.\",\n    help=\"Summarise the counts.\",\n)\nsummary_counts_parser.add_argument(\n    \"--file-pattern\",\n    dest=\"file_pattern\",\n    type=str,\n    required=False,\n    default=\"*_summarized.tsv\",\n    help=\"Regular expression for summarized files to process.\",\n)\nsummary_counts_parser.add_argument(\n    \"--outfile\",\n    dest=\"outfile\",\n    type=Path,\n    required=False,\n    default=\"summary_counts.tsv\",\n    help=\"Output filename to save results to, will be nested under 'output_dir'.\",\n)\nsummary_counts_parser.add_argument(\n    \"--separator\",\n    dest=\"sep\",\n    type=str,\n    required=False,\n    default=\"\\t\",\n    help=\"Field separator to use in output file, default is '\\t' but other values (e.g. ',' are allowed).\",\n)\nsummary_counts_parser.set_defaults(func=summarise_counts)\n</code></pre> <p>The last line here <code>.set_defaults(func=summarise_counts)</code> is the function that will be called when running the subcommand and corresponds to the function</p>"},{"location":"extending/#documentation","title":"Documentation","text":"<p>To have the documentation automatically built from the docstrings you have written for your functions you need to add a <code>docs/api/&lt;module_name&gt;.md</code> that corresponds to each of the modules you have introduced and add a title, short description and <code>isoslam.&lt;module_name&gt;</code>. If you have introduced more than one module then you will have to add a corresponding file for each module/sub-module you have introduced. If these are nested please mirror the nesting structure in the documentation.</p>"},{"location":"extending/#example_3","title":"Example","text":"<pre><code># Summary\n\n::: isoslam.summary\nhandler: python\noptions:\ndocstring_style:\nnumpy\nrendering:\nshow_signature_annotations: true\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Ideally you should install IsoSLAM under a Python Virtual Environment. Details of how to work with and use these is beyond the scope of this documentation but some advice can be found in the contributing section.</p>"},{"location":"installation/#isoslam","title":"IsoSLAM","text":""},{"location":"installation/#pypi","title":"PyPI","text":"<p>Whilst not complete (what software is?) IsoSLAM is available to install from the Python Package Index (PyPI).</p> <pre><code>pip install isoslam\n</code></pre>"},{"location":"installation/#condabioconda","title":"Conda/BioConda","text":"<p>If you use Conda or BioConda to manage your virtual environments and wish to document the installation of IsoSLAM in a YAML file you can add it as a <code>pip</code> dependency as shown in the sample <code>isoslam.yaml</code> file below which includes cgatcore and ruffus as dependencies which will be installed from one of the listed Conda channels.</p> <pre><code>name: isoslam\n\nchannels:\n  - conda-forge\n  - bioconda\n  - default\n\ndependencies:\n  - cgatcore\n  - ruffus\n  - pip\n  - pip:\n      - isoslam\n</code></pre> <p>You can then create an environment using the following.</p> <pre><code>conda env create --name isoslam --file isoslam.yaml\n</code></pre>"},{"location":"installation/#github","title":"GitHub","text":"<p>There are two methods of installing IsoSLAM from its GitHub repository. Which you should use depends on whether you wish to hack on the code/contribute to the development.</p>"},{"location":"installation/#pip-from-github","title":"<code>pip</code> from GitHub","text":"<p>If you only wish to try out the latest features and have no intention of modifying any of the code then you can use the package installer for Python pip to install packages directly from their version control homepage.</p> <pre><code>pip install git+ssh://git@github.com/IsoSLAM\n</code></pre> <p>If you want to install a specific branch or commit you can do so.</p> <pre><code>pip install git+ssh://git@github.com/IsoSLAM@&lt;branch-name&gt;\npip install git+ssh://git@github.com/IsoSLAM@&lt;commit-hash&gt;\n</code></pre>"},{"location":"installation/#cloning","title":"Cloning","text":"<p>If you wish to hack at the code and possibly contribute to development then you should clone the repository and install from there in \"editable\" mode along with the <code>dev</code> / <code>docs</code> / <code>tests</code> optional dependencies.</p> <pre><code>git clone git@github.com:sudlab/IsoSLAM.git\ncd IsoSLAM\npip install -e .[dev,docs,tests]\n</code></pre> <p>By using the <code>-e</code> (editable) flag it means you can switch branches or make changes to the code and they will be reflected in the code base when you run commands.</p> <p>NB - If you are not a member of the sudlab organisation on GitHub you should first fork the repository and make Pull Requests from your fork to the original repository.</p>"},{"location":"installation/#bioconda","title":"Bioconda","text":"<p>NB IsoSLAM is NOT currently available on Bioconda but it is planned.</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>There are a number of external dependencies required for running IsoSLAM as part of a ruffus pipeline, which is typically essential to prepare the files for processing with IsoSlam.</p> <ul> <li><code>samtools</code> / <code>bcftools</code> are both required and can be downloaded from htslib</li> <li>VarScan (documentation)</li> <li>subread (documentation)</li> </ul>"},{"location":"installation/#indirect-dependencies","title":"Indirect Dependencies","text":"<p>The pipeline for running the various steps in processing data rely on the cgat tools. These have two external dependencies themselves and if you wish to use such a pipeline will have to install these.</p> <ul> <li>bedtools</li> <li>wigToBigWig - the minimal tools may suffice.</li> </ul>"},{"location":"installation/#gnulinux","title":"GNU/Linux","text":""},{"location":"installation/#arch-linux","title":"Arch Linux","text":"<p>If you use Arch Linux the packages are available in the Arch Linux User Repository (AUR)</p> <pre><code>mkdir ~/aur &amp;&amp; cd ~/aur\ngit clone https://aur.archlinux.org/htslib.git\ngit clone https://aur.archlinux.org/samtools.git\ngit clone https://aur.archlinux.org/bcftools.git\ngit clone https://aur.archlinux.org/subread.git\ngit clone https://aur.archlinux.org/bedtools.git\ncd htslib\nmakepkg -sri\ncd ../bcftools\nmakepkg -sri\ncd ../samtools\nmakepkg -sri\ncd ../subread\nmakepkg -sri\ncd ../bedtools\nmakepkg -sri\n</code></pre> <p>varscan is written in Java, you need to download the latest release</p> <p>You can make a wrapper to run this, assuming you have saved the file to <code>~/.local/jar/VarScan.v2.4.6.jar</code> (adjust for the version you have downloaded), you can create the following short script and make it executable, placing it in your <code>$PATH</code> (the example below uses <code>~/.local/bin/</code>)</p> <pre><code>#!/bin/bash\n\njava -jar ~/.local/jar/VarScan.v2.4.6.jar\n</code></pre> <p>Make the file executable and you can then run <code>varscan</code></p> <pre><code>chmod 755 ~/.local/bin/varscan\nvarscan --version\nVarScan v2.4.6\n\n***NON-COMMERCIAL VERSION***\n\nUSAGE: java -jar VarScan.jar [COMMAND] [OPTIONS]\n\nCOMMANDS:\n pileup2snp  Identify SNPs from a pileup file\n pileup2indel  Identify indels a pileup file\n pileup2cns  Call consensus and variants from a pileup file\n mpileup2snp  Identify SNPs from an mpileup file\n mpileup2indel  Identify indels an mpileup file\n mpileup2cns  Call consensus and variants from an mpileup file\n\n somatic   Call germline/somatic variants from tumor-normal pileups\n mpileup2somatic  Call germline/somatic variants in multi-tumor-normal mpileup (beta feature in v2.4.5)\n copynumber  Determine relative tumor copy number from tumor-normal pileups\n readcounts  Obtain read counts for a list of variants from a pileup file\n\n filter   Filter SNPs by coverage, frequency, p-value, etc.\n somaticFilter  Filter somatic variants for clusters/indels\n fpfilter  Apply the false-positive filter\n\n processSomatic  Isolate Germline/LOH/Somatic calls from output\n copyCaller  GC-adjust and process copy number changes from VarScan copynumber output\n compare   Compare two lists of positions/variants\n limit   Restrict pileup/snps/indels to ROI positions\n</code></pre>"},{"location":"installation/#gentoo","title":"Gentoo","text":"<p><code>samtools</code> and <code>bcftools</code> are available in Portage, to install.</p> <pre><code>emerge --sync &amp;&amp; emerge -av samtools bcftools bedtools ucsc-genome-browser\n</code></pre>"},{"location":"installation/#ubuntudebian","title":"Ubuntu/Debian","text":"<p>All three packages are available for Debian based repositories.</p> <pre><code>sudo apt-get update\nsudo apt-get install samtools bcftools bedtools\n</code></pre>"},{"location":"installation/#ucsc-genome-browser","title":"UCSC Genome Browser","text":"<p>This needs installing from source across all distributions. The minimal should suffice.</p> <pre><code>git clone git@github.com:ucscGenomeBrowser/kent-core.git\ncd kent-core\nsudo make\n</code></pre>"},{"location":"installation/#source-install","title":"Source Install","text":"<p>The releases pages includes instructions on how to build the package from source, but note that you will then have to manually update the packages when new releases are made.</p>"},{"location":"installation/#windows","title":"Windows","text":"<p>To be written at some point.</p>"},{"location":"installation/#osx","title":"OSX","text":"<p>To be written at some point.</p>"},{"location":"installation/#conda","title":"Conda","text":"<p>If you don't have the ability to install these programmes at the system level an alternative is to use a Conda environment.</p> <pre><code>conda create -n isoslam python==3.12\nconda activate isoslam\nconda install mamba\nmamba install -c conda-forge -c bioconda bedtools\nmamba install -c conda-forge -c bioconda cgat-apps\nmamba install -c conda-forge -c bioconda samtools bcftools\nmamba install -c conda-forge -c bioconda subread\nmamba install -c conda-forge -c bioconda varscan\n</code></pre>"},{"location":"installation/#hpc","title":"HPC","text":"<p>To be written at some point.</p>"},{"location":"introduction/","title":"Introduction","text":"<p>IsoSLAM is a Python package for processing the output of SLAM-seq of RNA degradation rates and can be used in processing pipelines such as ruffus or NextFlow</p> <p>It is developed by the Sudlab at the University of Sheffield with support from the the Research Software Engineering group.</p> <p>To be written - expand on what is being done.</p>"},{"location":"links/","title":"Links","text":"<p>Links to software, documentation and related topics.</p> <ul> <li>Bedtools</li> <li>CGAT</li> <li>htslib</li> <li>Ruffus</li> <li>varscan (GitHub : varscan)</li> </ul>"},{"location":"links/#related-software","title":"Related Software","text":""},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#stand-alone-usage","title":"Stand alone usage","text":"<p>Once installed the <code>isolsam</code> command is should be available and you can view the options and sub-commands with</p> <pre><code>\u2771 isoslam --help\n\nusage: isoslam [-h] [-v] [-c CONFIG_FILE] [-b BASE_DIR] [-o OUTPUT_DIR] [-l LOG_LEVEL] {process,create-config,summary-counts} ...\n\nRun various programs related to IsoSLAM. Add the name of the program you wish to run.\n\noptions:\n  -h, --help            show this help message and exit\n  -v, --version         Report the installed version of IsoSLAM.\n  -c, --config-file CONFIG_FILE\n                        Path to a YAML configuration file.\n  -b, --base-dir BASE_DIR\n                        Base directory to run isoslam on.\n  -o, --output-dir OUTPUT_DIR\n                        Output directory to write results to.\n  -l, --log-level LOG_LEVEL\n                        Logging level to use, default is 'info' for verbose output use 'debug'.\n\nprogram:\n  Available programs listed below:\n\n  {process,create-config,summary-counts}\n    process             Process all files and run all summary plotting and statistics.\n    create-config       Create a configuration file using the defaults.\n    summary-counts      Summarise the counts.\n</code></pre> <p>Global options that set the configuration file to use, the base and output directory and log-level can be specified. Users then have a number of programs from IsoSLAM.</p> <ul> <li><code>process</code></li> <li><code>create-config</code></li> <li><code>summary-counts</code></li> </ul>"},{"location":"usage/#processing","title":"Processing","text":"<p>Help is available on using the <code>isoslam process</code> command and can be viewed with the <code>--help</code> option.</p> <pre><code>\u2771 isoslam process --help\nusage: isoslam process [-h] [-b BAM_FILE] [-g GTF_FILE] [-d BED_FILE] [-v VCF_FILE] [-u UPPER_PAIRS_LIMIT] [-f FIRST_MATCHED_LIMIT] [--delim DELIM]\n                       [--output-file OUTPUT_FILE]\n\nProcess all files and run all summary plotting and statistics.\n\noptions:\n  -h, --help            show this help message and exit\n  -b, --bam-file BAM_FILE\n                        Path to '.bam' file that has undergone read assignment with 'featureCount'.\n  -g, --gtf-file GTF_FILE\n                        Path to '.gtf' transcript assembly file.\n  -d, --bed-file BED_FILE\n                        Path to '.bed' utron file. Must be bed6 format.\n  -v, --vcf-file VCF_FILE\n                        Path to '.vcf.gz' file.\n  -u, --upper-pairs-limit UPPER_PAIRS_LIMIT\n                        Upper limit of pairs to be processed.\n  -f, --first-matched-limit FIRST_MATCHED_LIMIT\n                        Limit of matches.\n  --delim DELIM         Delimiter to use in output.\n  --output-file OUTPUT_FILE\n                        File to write results to.\n</code></pre> <p>It is important to remember that the <code>.bam</code> file requires pre-processing and instructions on how to do that can be found below.</p> <p>Three additional input files are required.</p> <ul> <li><code>.gtf</code> : DESCRIPTION</li> <li><code>.bed</code> : DESCRIPTION</li> <li><code>.vcf</code> : DESCRIPTION</li> </ul> <p>By default output is written to the <code>./output</code> directory which will be created if it does not exist and the default output file is <code>results.parquet</code> which is in Parquet format. The output directory is configurable, as is the output file name. If you would rather your output is saved as ASCII delimited file it is easy to do so by specifying the <code>--delim ,</code> (for comma-delimited) or <code>--delim \\t</code> (for tab-delimited) and using the appropriate file extension in the <code>--output-file</code>. For example the following outputs to <code>new_results</code> directory in a file <code>output_20250213.csv</code></p> <pre><code>\u2771 isoslam --output-dir new_results process \\\n         --bam-file d0_no4sU_EKRN230046545-1A_HFWGNDSX7_L4.star.bam \\\n         --gtf-file test_wash1.gtf \\\n         --bed-file test_coding_introns.bed \\\n         --vcf-file d0.vcf.gz \\\n         --delim \",\" \\\n         --output-file output_20250213.csv\n</code></pre>"},{"location":"usage/#configuration-file","title":"Configuration File","text":"<p>It is possible to place all configuration options in a YAML configuration file. A sample configuration file can be generated using the <code>isoslam create-config</code> command and there are options to specify the output filename, the default is <code>config.yaml</code> in the current directory (see <code>isoslam create-config --help</code> for all options).</p> <pre><code>\u2771 isoslam create-config --help\nusage: isoslam create-config [-h] [-f FILENAME] [-o OUTPUT_DIR]\n\nCreate a configuration file using the defaults.\n\noptions:\n  -h, --help            show this help message and exit\n  -f, --filename FILENAME\n                        Name of YAML file to save configuration to (default 'config.yaml').\n  -o, --output-dir OUTPUT_DIR\n                        Path to where the YAML file should be saved (default './' the current directory).\n</code></pre> <p>Once created you can edit the <code>config.yaml</code> to specify all fields and parameters and then invoke processing using this file.</p> <pre><code>\u2771 isoslam --config-file config.yaml process\n</code></pre> <p>Any command line options given such as the <code>--upper-pairs-limit</code> or <code>--first-matched-limit</code> will override those details in the custom <code>config.yaml</code>. This means it is particularly useful for usage in the full pipeline as <code>.gtf</code>, <code>.bed</code> and <code>.vcf</code> files are often common and it is the <code>.bam</code> input file that changes between runs. Thus you can specify the common files in your configuration file and then use the <code>--bam-file &lt;file_path&gt;</code> to override the field in the configuration file.</p>"},{"location":"usage/#preparing-files","title":"Preparing files","text":"<p>Assuming there are two <code>.bam</code> input files, a <code>.gtf</code> and Fasta file with the names shown below the following steps must be performed on the files prior to processing.</p> <ul> <li><code>d0_no4sU_file1.bam</code></li> <li><code>d0_0hr1_file2.bam</code></li> <li><code>test_wash1.gtf</code></li> <li><code>hg38_noalt.fa</code></li> </ul> <pre><code># Sort both files via read name (insert _sorted before .bam in file name)\nsamtools sort -n d0_no4sU_file1.bam -o d0_no4sU_file1_sorted.bam\nsamtools sort -n d0_0hr1_file2.bam -o d0_ohr1_file2_sorted.bam\n\n# Add XT tag to indicate trnascript reads map to and XS tag for which strand the gene is transcribed from\n# The -p flag explicitly says to assume libraries contain paired-end reads\nmkdir counts\nfeatureCounts -p -a test_wash1.gtf -o counts/d0_no4sU_counts.txt -T2 -R BAM d0_no4sU_file1_sorted.bam\nmv counts/d0_no4sU_file1_sorted.bam.featureCounts.bam d0_no4sU_file1_sorted_assigned.bam\nfeatureCounts -p -a test_wash1.gtf -o counts/d0_0hr1_counts.txt -T2 -R BAM d0_0hr1_file2_sorted.bam\nmv counts/d0_0hr1_file2_sorted.bam.featureCounts.bam d0_0hr1_file2_sorted_assigned.bam\n\n# Resort the files\nsamtools sort -n d0_no4sU_file1_sorted_assigned.bam -o d0_no4sU_file1_sorted_assigned_sorted.bam\nsamtools sort -n d0_0hr1_file2_sorted_assigned.bam -o d0_0hr1_file2_sorted_assigned_sorted.bam\n\n# Take just the negative control (no4sU) and create a SNP VCF file using Varscan\nsamtools mpileup -B -A -f hg38_noalt.fa d0_no4sU_file_sorted_assigned_sorted.bam |\n  varscan mpileup2snp --variants 1 --output-vcf 1 &gt; snp.vcf &amp;&amp;\n  bcftools view snp.vcf -Oz -o snp.vcf.gz\n\n# Index the VCF file for reading by PySam\ntabix -p vcf snp.vcf.gz\n\n# Run isoslam on the two files\nisoslam process --bam-file d0_no4sU_file1_sorted_assigned_sorted.bam --bed test_coding_introns.bed --gtf test_wash1.gtf \\\n  --vcf-file snp.vcf.gz\nisoslam process --bam-file d0_0hr1_file2_sorted_assigned_sorted.bam --bed test_coding_introns.bed --gtf test_wash1.gtf \\\n  --vcf-file snp.vcf.gz\n</code></pre>"},{"location":"usage/#ruffuscgat-pipeline","title":"Ruffus/CGAT pipeline","text":"<p>Typically <code>isoslam</code> is part of a workflow pipeline that processes a multiple files. The example below uses the ruffus package to control the pipeline but workflow software could be used such as Nextflow.</p> <p>Ruffus uses decorators that control the input and output of functions at each step. Several are used in this example pipeline.</p> <ul> <li><code>@ruffus.follows()</code> : ensures that the task, or list of tasks specified are run before the current   function is run. Includes the ability to <code>mkdir(\"dir_name\")</code> to create directories if they don't already exist.</li> <li><code>@ruffus.transforms()</code> : Takes input from a previous task/function, applies a filter which is   typically a regular expression of some description to identify one or more files, and produces corresponding output   for each matched file.</li> </ul> <p>This allows multiple files to be processed in parallel on HPC systems using the Slurm workload manager without having to explicitly specify any Slurm jobs or calls.</p> <p>To be written EXPAND THIS SECTION WITH WORKED EXAMPLES.</p>"},{"location":"workflow/","title":"Workflow","text":"<p>This page gives an overview of the workflow undertaken by IsoSLAM, it is a WORK IN PROGRESS as the code base is underoing refactoring.</p> <ul> <li>This is very much a work in progress and is not yet complete. Contributions are welcome.</li> </ul> flowchart TB     subgraph Input         BAM[(BAM Files)]         VCF[(\"VCF Files\")]         GTF[(\"GTF Files\")]         Config[(\"Config Files\")]     end      subgraph Core[\"Core Processing\"]         IO[\"Input/Output Handler\"]:::core         Process[\"Processing Engine\"]:::core         Pipeline[\"SLAM Pipeline\"]:::core         Summary[\"Summary Generator\"]:::core     end      subgraph Support[\"Support Components\"]         Logger[\"Logging System\"]:::support         Utils[\"Utility Functions\"]:::support         DefaultConfig[\"Configuration Manager\"]:::support     end      subgraph Testing[\"Testing &amp; Documentation\"]         TestInfra[\"Testing Infrastructure\"]:::test         TestRes[\"Test Resources\"]:::test         APIDoc[\"API Documentation\"]:::doc     end      subgraph Integration[\"External Integration\"]         RScript[\"R Script Integration\"]:::integration         CICD[\"CI/CD Pipeline\"]:::integration         PipeConfig[\"Pipeline Configuration\"]:::integration     end      %% Main Data Flow     BAM --&gt; IO     VCF --&gt; IO     GTF --&gt; IO     Config --&gt; DefaultConfig      IO --&gt; Process     Process --&gt; Pipeline     Pipeline --&gt; Summary      %% Support Flow     DefaultConfig -.-&gt; IO     DefaultConfig -.-&gt; Process     DefaultConfig -.-&gt; Pipeline      Logger -.-&gt; IO     Logger -.-&gt; Process     Logger -.-&gt; Pipeline     Logger -.-&gt; Summary      Utils --&gt; IO     Utils --&gt; Process     Utils --&gt; Pipeline      %% Integration Flow     Pipeline --&gt; RScript     PipeConfig -.-&gt; Pipeline     CICD -.-&gt; TestInfra      %% Testing Flow     TestRes --&gt; TestInfra     TestInfra -.-&gt; Core      %% Click Events     click IO \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/io.py\"     click Process \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/processing.py\"     click Pipeline \"https://github.com/sudlab/IsoSLAM/tree/main/isoslam/pipeline_slam_3UIs/\"     click Summary \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/summary.py\"     click Logger \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/logging.py\"     click Utils \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/utils.py\"     click DefaultConfig \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/default_config.yaml\"     click TestInfra \"https://github.com/sudlab/IsoSLAM/tree/main/tests/\"     click APIDoc \"https://github.com/sudlab/IsoSLAM/tree/main/docs/api/\"     click CICD \"https://github.com/sudlab/IsoSLAM/tree/main/.github/workflows/\"     click PipeConfig \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/pipeline_slam_3UIs/pipeline.yml\"     click RScript \"https://github.com/sudlab/IsoSLAM/blob/main/isoslam/pipeline_slam_3UIs/summarize_counts.R\"     click TestRes \"https://github.com/sudlab/IsoSLAM/tree/main/tests/resources/\"      %% Styling     classDef core fill:#90EE90,stroke:#333,stroke-width:2px     classDef support fill:#FFE4B5,stroke:#333,stroke-width:2px     classDef test fill:#DDA0DD,stroke:#333,stroke-width:2px     classDef doc fill:#87CEEB,stroke:#333,stroke-width:2px     classDef integration fill:#F08080,stroke:#333,stroke-width:2px      %% Legend     subgraph Legend         L1[\"Core Components\"]:::core         L2[\"Support Components\"]:::support         L3[\"Testing Components\"]:::test         L4[\"Documentation\"]:::doc         L5[\"Integration\"]:::integration     end  <p>The above diagram is written in Mermaid and generated using GitDiagram. You can view the source code in the IsoSLAM repository and develop/modify it using the Mermaid Live Editor and make pull-requests to update this documentation.</p>"},{"location":"workflow/#isoslam","title":"IsoSLAM","text":"<p>A number of pre-processing steps are undertaken prior to IsoSLAM work being done. The following is work in progress as the code is refactored.</p> <ol> <li>Iterate over <code>.bam</code> file and pair segments. If two or more <code>AlignedSegments</code> with the same <code>query_name</code> are found    then <code>n &gt; 1</code> segments are dropped.</li> <li>Pairs of segments (individual <code>AlignedSegments</code>) are then assessed and if they are <code>Assigned</code> the <code>start</code>, <code>end</code>,    <code>length</code>, <code>status</code> (i.e. <code>Assigned</code>), <code>transcript_id</code>, <code>block_start</code> and <code>block_end</code> are extracted.</li> </ol>"},{"location":"workflow/#descriptive-workflow","title":"Descriptive Workflow","text":""},{"location":"workflow/#read-alignments-are-loaded","title":"Read alignments are loaded","text":""},{"location":"workflow/#the-gene-transcript-these-are-within-are-identified","title":"The gene transcript these are within are identified","text":""},{"location":"workflow/#introns-within-these-genes-are-identified","title":"Introns within these genes are identified","text":""},{"location":"workflow/#retain-reads-that-are-overlap-with-introns-or-splice-ends","title":"Retain reads that are overlap with introns or splice ends","text":"<pre><code>                                                 Genome -----------------------&gt;\n\nRead Alignment Blocks                     |&gt;&gt;&gt;&gt;|                     |&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;|\nTranscript 1:                      |===========|---------------------|=========|-------------------|==========|\nTranscript 2:              |====|------------------------------------|=========|-------------------|==========|\n\nIntrons[0]                                      ---------------------\nIntrons[1]                                                                      -------------------\nIntrons[2]                       ------------------------------------\nIntrons[3]                                                                      -------------------\n\nExon : |========|   Read alignment block:  |&gt;&gt;&gt;&gt;&gt;|\n</code></pre>"},{"location":"workflow/#statistical-analysis","title":"Statistical Analysis","text":"<p>Non-linear exponential regression models are fitted for each transcript and isoform (in terms of <code>start</code>/<code>end</code>/<code>strand</code>/<code>assignment</code>). Because replicate analyses are performed some data preparation is required before fitting the model to determine the \"average\" percentage of conversions and to derive appropriate weights since we can be more confident in this percentage change when we have observed more events in total.</p>"},{"location":"workflow/#data-preparation","title":"Data Preparation","text":"<p>Described below are the different steps in preparing the data. The `Statistics`` dataclass exists to which loads and aggregates the data and performs the necessary steps described below.</p> <p>NB The excerpts below are from the regression test suite and can be found in the GitHub repository under <code>tests/_regtest_outputs</code> and correspond to the relevant test.</p>"},{"location":"workflow/#initial-data-structure","title":"Initial Data Structure","text":"<p>After initial processing with <code>isoslam</code> there are multiple files in either <code>csv</code>, <code>tsv</code> or <code>parquet</code> format with the columns <code>Read_UID</code>, <code>Transcript_id</code>, <code>Start</code>, <code>End</code>, <code>Chr</code>, <code>Strand</code>, <code>Assignment</code>, <code>Conversions</code>, <code>Convertible</code>, <code>Coverage</code>. The transcript <code>ENST00000313949</code> is shown below.</p> <p>NB For convenience the filenames have been truncated to show only the day, hour and replicate, in real live the filenames contain more metadata about the experiments.</p> <pre><code>\u2771 grep -n ENST00000313949 ../resources/tsv/output/d*\n../resources/tsv/output/d0_0hr1.tsv:150:45      ENST00000313949    58910081        58910333        chr20   +       Spl     0       62      300\\n\n../resources/tsv/output/d0_0hr1.tsv:156:45      ENST00000313949    58909804        58909950        chr20   +       Spl     0       62      300\n../resources/tsv/output/d0_0hr1.tsv:157:45      ENST00000313949    58909579        58909683        chr20   +       Spl     0       62      300\n../resources/tsv/output/d0_0hr4.tsv:52:16       ENST00000313949    58910081        58910333        chr20   +       Spl     0       69      291\n../resources/tsv/output/d0_0hr4.tsv:54:16       ENST00000313949    58910401        58910682        chr20   +       Spl     0       69      291\n../resources/tsv/output/d0_0hr4.tsv:57:16       ENST00000313949    58909804        58909950        chr20   +       Spl     0       69      291\n../resources/tsv/output/d0_0hr4.tsv:86:27       ENST00000313949    58910401        58910682        chr20   +       Spl     1       87      299\n../resources/tsv/output/d0_12hr4.tsv:69:19      ENST00000313949    58909216        58909349        chr20   +       Spl     0       66      300\n../resources/tsv/output/d0_12hr4.tsv:75:19      ENST00000313949    58903791        58905382        chr20   +       Spl     0       66      300\n../resources/tsv/output/d0_12hr4.tsv:77:19      ENST00000313949    58909423        58909520        chr20   +       Spl     0       66      300\n../resources/tsv/output/d0_3hr1.tsv:12:7        ENST00000313949    58903791        58905382        chr20   +       Spl     0       65      232\n../resources/tsv/output/d0_3hr1.tsv:16:7        ENST00000313949    58905480        58909161        chr20   +       Spl     0       65      232\n../resources/tsv/output/d0_3hr2.tsv:140:20      ENST00000313949    58909804        58909950        chr20   +       Spl     1       70      218\n../resources/tsv/output/d16_0hr1.tsv:47:11      ENST00000313949    58909579        58909683        chr20   +       Spl     0       68      297\n../resources/tsv/output/d16_0hr1.tsv:54:11      ENST00000313949    58909804        58909950        chr20   +       Spl     0       68      297\n../resources/tsv/output/d16_0hr1.tsv:55:11      ENST00000313949    58909423        58909520        chr20   +       Spl     0       68      297\n../resources/tsv/output/d16_0hr2.tsv:31:12      ENST00000313949    58903791        58905382        chr20   +       Spl     1       68      197\n../resources/tsv/output/d16_0hr2.tsv:33:12      ENST00000313949    58905480        58909161        chr20   +       Spl     1       68      197\n../resources/tsv/output/d16_0hr3.tsv:85:26      ENST00000313949    58910081        58910333        chr20   +       Spl     0       70      300\n../resources/tsv/output/d16_0hr3.tsv:86:26      ENST00000313949    58909423        58909520        chr20   +       Spl     0       70      300\n../resources/tsv/output/d16_0hr3.tsv:91:26      ENST00000313949    58909579        58909683        chr20   +       Spl     0       70      300\n../resources/tsv/output/d16_0hr3.tsv:98:28      ENST00000313949    58909804        58909950        chr20   +       Spl     0       72      300\n../resources/tsv/output/d16_0hr3.tsv:99:28      ENST00000313949    58909423        58909520        chr20   +       Spl     0       72      300\n../resources/tsv/output/d16_0hr3.tsv:104:28     ENST00000313949    58909579        58909683        chr20   +       Spl     0       72      300\n../resources/tsv/output/d16_3hr1.tsv:165:56     ENST00000313949    58909579        58909683        chr20   +       Spl     0       68      300\n../resources/tsv/output/d16_3hr1.tsv:166:56     ENST00000313949    58909423        58909520        chr20   +       Spl     0       68      300\n../resources/tsv/output/d16_3hr1.tsv:169:56     ENST00000313949    58909804        58909950        chr20   +       Spl     0       68      300\n../resources/tsv/output/d16_3hr3.tsv:11:4       ENST00000313949    58910401        58910682        chr20   +       Spl     1       65      289\n../resources/tsv/output/d16_3hr3.tsv:16:4       ENST00000313949    58910081        58910333        chr20   +       Spl     1       65      289\n../resources/tsv/output/d16_3hr3.tsv:69:22      ENST00000313949    58903791        58905382        chr20   +       Spl     0       57      266\n../resources/tsv/output/d16_3hr3.tsv:71:22      ENST00000313949    58903585        58903671        chr20   +       Spl     0       57      266\n../resources/tsv/output/d16_no4sU.tsv:56:19     ENST00000313949    58910401        58910682        chr20   +       Spl     0       64      235\n../resources/tsv/output/d16_no4sU.tsv:57:19     ENST00000313949    58910081        58910333        chr20   +       Spl     0       64      235\n../resources/tsv/output/d2_12hr1.tsv:119:41     ENST00000313949    58905480        58909161        chr20   +       Spl     1       65      249\n../resources/tsv/output/d2_12hr1.tsv:121:41     ENST00000313949    58909216        58909349        chr20   +       Spl     1       65      249\n../resources/tsv/output/d2_12hr1.tsv:122:41     ENST00000313949    58909423        58909520        chr20   +       Spl     1       65      249\n</code></pre>"},{"location":"workflow/#summarise-counts","title":"Summarise Counts","text":"<p>Having processed multiple files the data is aggregated into a single large dataframe by the <code>summary.summary_counts()</code> function. The <code>Chr</code> column has been dropped as it is consistent with the <code>Transcript_id</code>, it is not lost however and is retained and added back in at the end, it just removes a level of indexing when performing <code>group_by()</code> operations. Note also after this aggregation process the data is not sorted by <code>day</code> or <code>hour</code>.</p> <p>We can see that for transcript <code>ENST00000313949</code> (on Chromosome 20) starting at <code>58903791</code> and ending at <code>58905382</code> there are observed events at day 0, hours 3 and 12 and day 16 at hours 0 and 3, but only one had an observed conversion.</p> <p>TODO Check why <code>conversion_count</code> is `1** for these events, surely it should be zero?</p> <p>TODO Possibly remove other lines to make it easier to read, but leave a few around it in place.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,filename,one_or_more_conversion,conversion_count,conversion_total,conversion_percent,day,hour,replicate\nENST00000313949,+,58903585,58903671,Spl,d16_3hr3,false,1,1,1.0,16,3,3\nENST00000313949,+,58903791,58905382,Spl,d0_12hr4,false,1,1,1.0,0,12,4\nENST00000313949,+,58903791,58905382,Spl,d0_3hr1,false,1,1,1.0,0,3,1\nENST00000313949,+,58903791,58905382,Spl,d16_0hr2,true,1,1,1.0,16,0,2\nENST00000313949,+,58903791,58905382,Spl,d16_3hr3,false,1,1,1.0,16,3,3\nENST00000313949,+,58905480,58909161,Spl,d0_3hr1,false,1,1,1.0,0,3,1\nENST00000313949,+,58905480,58909161,Spl,d16_0hr2,true,1,1,1.0,16,0,2\nENST00000313949,+,58905480,58909161,Spl,d2_12hr1,true,1,1,1.0,2,12,1\nENST00000313949,+,58909216,58909349,Spl,d0_12hr4,false,1,1,1.0,0,12,4\nENST00000313949,+,58909216,58909349,Spl,d2_12hr1,true,1,1,1.0,2,12,1\nENST00000313949,+,58909423,58909520,Spl,d0_12hr4,false,1,1,1.0,0,12,4\nENST00000313949,+,58909423,58909520,Spl,d16_0hr1,false,1,1,1.0,16,0,1\nENST00000313949,+,58909423,58909520,Spl,d16_0hr3,false,2,2,1.0,16,0,3\nENST00000313949,+,58909423,58909520,Spl,d16_3hr1,false,1,1,1.0,16,3,1\nENST00000313949,+,58909423,58909520,Spl,d2_12hr1,true,1,1,1.0,2,12,1\nENST00000313949,+,58909579,58909683,Spl,d0_0hr1,false,1,1,1.0,0,0,1\nENST00000313949,+,58909579,58909683,Spl,d16_0hr1,false,1,1,1.0,16,0,1\nENST00000313949,+,58909579,58909683,Spl,d16_0hr3,false,2,2,1.0,16,0,3\nENST00000313949,+,58909579,58909683,Spl,d16_3hr1,false,1,1,1.0,16,3,1\nENST00000313949,+,58909804,58909950,Spl,d0_0hr1,false,1,1,1.0,0,0,1\nENST00000313949,+,58909804,58909950,Spl,d0_0hr4,false,1,1,1.0,0,0,4\nENST00000313949,+,58909804,58909950,Spl,d0_3hr2,true,1,1,1.0,0,3,2\nENST00000313949,+,58909804,58909950,Spl,d16_0hr1,false,1,1,1.0,16,0,1\nENST00000313949,+,58909804,58909950,Spl,d16_0hr3,false,1,1,1.0,16,0,3\nENST00000313949,+,58909804,58909950,Spl,d16_3hr1,false,1,1,1.0,16,3,1\nENST00000313949,+,58910081,58910333,Spl,d0_0hr1,false,1,1,1.0,0,0,1\nENST00000313949,+,58910081,58910333,Spl,d0_0hr4,false,1,1,1.0,0,0,4\nENST00000313949,+,58910081,58910333,Spl,d16_0hr3,false,1,1,1.0,16,0,3\nENST00000313949,+,58910081,58910333,Spl,d16_3hr3,true,1,1,1.0,16,3,3\nENST00000313949,+,58910401,58910682,Spl,d0_0hr4,false,1,2,0.5,0,0,4\nENST00000313949,+,58910401,58910682,Spl,d0_0hr4,true,1,2,0.5,0,0,4\nENST00000313949,+,58910401,58910682,Spl,d16_3hr3,true,1,1,1.0,16,3,3\n</code></pre>"},{"location":"workflow/#aggregate-counts","title":"Aggregate Counts","text":"<pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,replicate,len,one_or_more_conversion\nENST00000313949,+,58903585,58903671,Spl,16,3,3,1,false\nENST00000313949,+,58903791,58905382,Spl,0,3,1,1,false\nENST00000313949,+,58903791,58905382,Spl,0,12,4,1,false\nENST00000313949,+,58903791,58905382,Spl,16,0,2,1,true\nENST00000313949,+,58903791,58905382,Spl,16,3,3,1,false\nENST00000313949,+,58905480,58909161,Spl,0,3,1,1,false\nENST00000313949,+,58905480,58909161,Spl,2,12,1,1,true\nENST00000313949,+,58905480,58909161,Spl,16,0,2,1,true\nENST00000313949,+,58909216,58909349,Spl,0,12,4,1,false\nENST00000313949,+,58909216,58909349,Spl,2,12,1,1,true\nENST00000313949,+,58909423,58909520,Spl,0,12,4,1,false\nENST00000313949,+,58909423,58909520,Spl,2,12,1,1,true\nENST00000313949,+,58909423,58909520,Spl,16,0,1,1,false\nENST00000313949,+,58909423,58909520,Spl,16,0,3,1,false\nENST00000313949,+,58909423,58909520,Spl,16,3,1,1,false\nENST00000313949,+,58909579,58909683,Spl,0,0,1,1,false\nENST00000313949,+,58909579,58909683,Spl,16,0,1,1,false\nENST00000313949,+,58909579,58909683,Spl,16,0,3,1,false\nENST00000313949,+,58909579,58909683,Spl,16,3,1,1,false\nENST00000313949,+,58909804,58909950,Spl,0,0,1,1,false\nENST00000313949,+,58909804,58909950,Spl,0,0,4,1,false\nENST00000313949,+,58909804,58909950,Spl,0,3,2,1,true\nENST00000313949,+,58909804,58909950,Spl,16,0,1,1,false\nENST00000313949,+,58909804,58909950,Spl,16,0,3,1,false\nENST00000313949,+,58909804,58909950,Spl,16,3,1,1,false\nENST00000313949,+,58910081,58910333,Spl,0,0,1,1,false\nENST00000313949,+,58910081,58910333,Spl,0,0,4,1,false\nENST00000313949,+,58910081,58910333,Spl,16,0,3,1,false\nENST00000313949,+,58910081,58910333,Spl,16,3,3,1,true\nENST00000313949,+,58910401,58910682,Spl,0,0,4,2,false\nENST00000313949,+,58910401,58910682,Spl,16,3,3,1,true\n</code></pre>"},{"location":"workflow/#filter-no-conversions","title":"Filter No Conversions","text":"<p>The aggregation process can result in some missing observations where there have been observed reads but no conversions observed. Such data points need including and so we derive a subset of instances and make dummies.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,replicate,one_or_more_conversion\nENST00000313949,+,58903585,58903671,Spl,16,3,3,false\nENST00000313949,+,58903791,58905382,Spl,0,3,1,false\nENST00000313949,+,58903791,58905382,Spl,0,12,4,false\nENST00000313949,+,58903791,58905382,Spl,16,3,3,false\nENST00000313949,+,58905480,58909161,Spl,0,3,1,false\nENST00000313949,+,58909216,58909349,Spl,0,12,4,false\nENST00000313949,+,58909423,58909520,Spl,0,12,4,false\nENST00000313949,+,58909423,58909520,Spl,16,0,1,false\nENST00000313949,+,58909423,58909520,Spl,16,0,3,false\nENST00000313949,+,58909423,58909520,Spl,16,3,1,false\nENST00000313949,+,58909579,58909683,Spl,0,0,1,false\nENST00000313949,+,58909579,58909683,Spl,16,0,1,false\nENST00000313949,+,58909579,58909683,Spl,16,0,3,false\nENST00000313949,+,58909579,58909683,Spl,16,3,1,false\nENST00000313949,+,58909804,58909950,Spl,0,0,1,false\nENST00000313949,+,58909804,58909950,Spl,0,0,4,false\nENST00000313949,+,58909804,58909950,Spl,16,0,1,false\nENST00000313949,+,58909804,58909950,Spl,16,0,3,false\nENST00000313949,+,58909804,58909950,Spl,16,3,1,false\nENST00000313949,+,58910081,58910333,Spl,0,0,1,false\nENST00000313949,+,58910081,58910333,Spl,0,0,4,false\nENST00000313949,+,58910081,58910333,Spl,16,0,3,false\n</code></pre>"},{"location":"workflow/#extract-one-or-more-conversions","title":"Extract One or More Conversions","text":"<p>We flip the <code>one_or_more_conversion</code> to be <code>true</code> here and the <code>conversion_count</code> and <code>convesrion_percent</code> is set to <code>0</code> and <code>0.0</code> for the day/hour/replicates we observed zero conversions and the data is combined with those that did have conversions so that we have a complete data set.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,replicate,one_or_more_conversion,conversion_count,conversion_total,conversion_percent\nENST00000313949,+,58903585,58903671,Spl,16,3,3,true,0,1,0.0\nENST00000313949,+,58903791,58905382,Spl,0,3,1,true,0,1,0.0\nENST00000313949,+,58903791,58905382,Spl,0,12,4,true,0,1,0.0\nENST00000313949,+,58903791,58905382,Spl,16,0,2,true,1,1,1.0\nENST00000313949,+,58903791,58905382,Spl,16,3,3,true,0,1,0.0\nENST00000313949,+,58905480,58909161,Spl,0,3,1,true,0,1,0.0\nENST00000313949,+,58905480,58909161,Spl,2,12,1,true,1,1,1.0\nENST00000313949,+,58905480,58909161,Spl,16,0,2,true,1,1,1.0\nENST00000313949,+,58909216,58909349,Spl,0,12,4,true,0,1,0.0\nENST00000313949,+,58909216,58909349,Spl,2,12,1,true,1,1,1.0\nENST00000313949,+,58909423,58909520,Spl,0,12,4,true,0,1,0.0\nENST00000313949,+,58909423,58909520,Spl,2,12,1,true,1,1,1.0\nENST00000313949,+,58909423,58909520,Spl,16,0,1,true,0,1,0.0\nENST00000313949,+,58909423,58909520,Spl,16,0,3,true,0,2,0.0\nENST00000313949,+,58909423,58909520,Spl,16,3,1,true,0,1,0.0\nENST00000313949,+,58909579,58909683,Spl,0,0,1,true,0,1,0.0\nENST00000313949,+,58909579,58909683,Spl,16,0,1,true,0,1,0.0\nENST00000313949,+,58909579,58909683,Spl,16,0,3,true,0,2,0.0\nENST00000313949,+,58909579,58909683,Spl,16,3,1,true,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,0,0,1,true,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,0,0,4,true,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,0,3,2,true,1,1,1.0\nENST00000313949,+,58909804,58909950,Spl,16,0,1,true,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,16,0,3,true,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,16,3,1,true,0,1,0.0\nENST00000313949,+,58910081,58910333,Spl,0,0,1,true,0,1,0.0\nENST00000313949,+,58910081,58910333,Spl,0,0,4,true,0,1,0.0\nENST00000313949,+,58910081,58910333,Spl,16,0,3,true,0,1,0.0\nENST00000313949,+,58910081,58910333,Spl,16,3,3,true,1,1,1.0\nENST00000313949,+,58910401,58910682,Spl,0,0,4,true,1,2,0.5\nENST00000313949,+,58910401,58910682,Spl,16,3,3,true,1,1,1.0\n</code></pre>"},{"location":"workflow/#calculate-percentage-of-conversions-across-replicates","title":"Calculate Percentage of Conversions Across Replicates","text":"<p>Note that for the transcript with start/end <code>58909804</code>/<code>58909950</code> we have four rows as there was a replicated observation on day <code>0</code> at hour <code>0</code>.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,conversion_count,conversion_total,conversion_percent\nENST00000313949,+,58903585,58903671,Spl,16,3,0,1,0.0\nENST00000313949,+,58903791,58905382,Spl,0,3,0,1,0.0\nENST00000313949,+,58903791,58905382,Spl,0,12,0,1,0.0\nENST00000313949,+,58903791,58905382,Spl,16,0,1,1,100.0\nENST00000313949,+,58903791,58905382,Spl,16,3,0,1,0.0\nENST00000313949,+,58905480,58909161,Spl,0,3,0,1,0.0\nENST00000313949,+,58905480,58909161,Spl,2,12,1,1,100.0\nENST00000313949,+,58905480,58909161,Spl,16,0,1,1,100.0\nENST00000313949,+,58909216,58909349,Spl,0,12,0,1,0.0\nENST00000313949,+,58909216,58909349,Spl,2,12,1,1,100.0\nENST00000313949,+,58909423,58909520,Spl,0,12,0,1,0.0\nENST00000313949,+,58909423,58909520,Spl,2,12,1,1,100.0\nENST00000313949,+,58909423,58909520,Spl,16,0,0,3,0.0\nENST00000313949,+,58909423,58909520,Spl,16,3,0,1,0.0\nENST00000313949,+,58909579,58909683,Spl,0,0,0,1,0.0\nENST00000313949,+,58909579,58909683,Spl,16,0,0,3,0.0\nENST00000313949,+,58909579,58909683,Spl,16,3,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,0,0,0,2,0.0\nENST00000313949,+,58909804,58909950,Spl,0,3,1,1,100.0\nENST00000313949,+,58909804,58909950,Spl,16,0,0,2,0.0\nENST00000313949,+,58909804,58909950,Spl,16,3,0,1,0.0\nENST00000313949,+,58910081,58910333,Spl,0,0,0,2,0.0\nENST00000313949,+,58910081,58910333,Spl,16,0,0,1,0.0\nENST00000313949,+,58910081,58910333,Spl,16,3,1,1,100.0\nENST00000313949,+,58910401,58910682,Spl,0,0,1,2,50.0\nENST00000313949,+,58910401,58910682,Spl,16,3,1,1,100.0\n</code></pre>"},{"location":"workflow/#select-base-levels","title":"Select Base Levels","text":"<p>We want to normalise the data and scale the percentage of changes relative to those at baseline (<code>t = 0</code>)</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,baseline_count,baseline_total,baseline_percent\nENST00000313949,+,58909579,58909683,Spl,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,0,2,0.0\nENST00000313949,+,58910081,58910333,Spl,0,2,0.0\nENST00000313949,+,58910401,58910682,Spl,1,2,50.0\n</code></pre>"},{"location":"workflow/#merge-baseline-with-all-averages","title":"Merge baseline with all averages","text":"<p>Prior to normalising the data we can merge the baseline parameters (<code>summary._merge_average_with_baseline()</code>)</p> <p>At this stage we can also remove instances where the percentage change at baseline is zero using the <code>remove_zero_baseline</code> which we would typically want to do because in the next step of normalising the data a division by zero leads to <code>NaN</code> in the dataframe and models can not be fitted to such data.</p> <p>In this example the <code>remove_zero_baseline = False</code> and we can see there are a number of locations where the <code>baseline_percent</code> (final column) is <code>0.0</code>.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,conversion_count,conversion_total,conversion_percent,baseline_count,baseline_total,baseline_percent\nENST00000313949,+,58909579,58909683,Spl,0,0,0,1,0.0,0,1,0.0\nENST00000313949,+,58909579,58909683,Spl,16,0,0,3,0.0,0,1,0.0\nENST00000313949,+,58909579,58909683,Spl,16,3,0,1,0.0,0,1,0.0\nENST00000313949,+,58909804,58909950,Spl,0,0,0,2,0.0,0,2,0.0\nENST00000313949,+,58909804,58909950,Spl,0,3,1,1,100.0,0,2,0.0\nENST00000313949,+,58909804,58909950,Spl,16,0,0,2,0.0,0,2,0.0\nENST00000313949,+,58909804,58909950,Spl,16,3,0,1,0.0,0,2,0.0\nENST00000313949,+,58910081,58910333,Spl,0,0,0,2,0.0,0,2,0.0\nENST00000313949,+,58910081,58910333,Spl,16,0,0,1,0.0,0,2,0.0\nENST00000313949,+,58910081,58910333,Spl,16,3,1,1,100.0,0,2,0.0\nENST00000313949,+,58910401,58910682,Spl,0,0,1,2,50.0,1,2,50.0\nENST00000313949,+,58910401,58910682,Spl,16,3,1,1,100.0,1,2,50.0\n</code></pre> <p>If we set <code>remove_zero_baseline = True</code> we get just the last instance where the percentage of conversions at baseline was <code>50.0</code>.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,conversion_count,conversion_total,conversion_percent,baseline_count,baseline_total,baseline_percent\nENST00000313949,+,58910401,58910682,Spl,0,0,1,2,50.0,1,2,50.0\nENST00000313949,+,58910401,58910682,Spl,16,3,1,1,100.0,1,2,50.0\n</code></pre>"},{"location":"workflow/#normalise-mean-percentages","title":"Normalise mean percentages","text":"<p>We can now normalise the data (<code>summary._normalise()</code>) by dividing the percentage change the baseline value that we merged in.</p> <p>If we don't remove the instances where the percentage change at baseline was <code>0.0</code> then the <code>normalised_percent</code> is <code>NaN</code> if a given time point is also <code>0.0</code> and <code>Inf</code> if any other.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,conversion_count,conversion_total,conversion_percent,baseline_count,baseline_total,baseline_percent,normalised_percent\nENST00000313949,+,58909579,58909683,Spl,0,0,0,1,0.0,0,1,0.0,NaN\nENST00000313949,+,58909579,58909683,Spl,16,0,0,3,0.0,0,1,0.0,NaN\nENST00000313949,+,58909579,58909683,Spl,16,3,0,1,0.0,0,1,0.0,NaN\nENST00000313949,+,58909804,58909950,Spl,0,0,0,2,0.0,0,2,0.0,NaN\nENST00000313949,+,58909804,58909950,Spl,0,3,1,1,100.0,0,2,0.0,inf\nENST00000313949,+,58909804,58909950,Spl,16,0,0,2,0.0,0,2,0.0,NaN\nENST00000313949,+,58909804,58909950,Spl,16,3,0,1,0.0,0,2,0.0,NaN\nENST00000313949,+,58910081,58910333,Spl,0,0,0,2,0.0,0,2,0.0,NaN\nENST00000313949,+,58910081,58910333,Spl,16,0,0,1,0.0,0,2,0.0,NaN\nENST00000313949,+,58910081,58910333,Spl,16,3,1,1,100.0,0,2,0.0,inf\nENST00000313949,+,58910401,58910682,Spl,0,0,1,2,50.0,1,2,50.0,1.0\nENST00000313949,+,58910401,58910682,Spl,16,3,1,1,100.0,1,2,50.0,2.0\n</code></pre> <p>If we have already filtered out instances where the percentage change at baseline was <code>0.0</code> then we only have genuine normalised values.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,conversion_count,conversion_total,conversion_percent,baseline_count,baseline_total,baseline_percent,normalised_percent\nENST00000313949,+,58910401,58910682,Spl,0,0,1,2,50.0,1,2,50.0,1.0\nENST00000313949,+,58910401,58910682,Spl,16,3,1,1,100.0,1,2,50.0,2.0\n</code></pre>"},{"location":"workflow/#derive-weights","title":"Derive Weights","text":"<p>If more reads have been observed then we have greater confidence in the percentage of conversions across replicates. We therefore use the total number of conversions across all time points to work out the proportion (/percentage) that are derived from a given time point within the grouping and set this as the <code>conversion_weight</code>.</p> <p>In the example below we have already filtered out instances where there were no conversions observed at baseline.</p> <pre><code>Transcript_id,Strand,Start,End,Assignment,day,hour,conversion_count,conversion_total,conversion_percent,baseline_count,baseline_total,baseline_percent,normalised_percent,conversion_total_all_time_points,conversion_weight\nENST00000313949,+,58910401,58910682,Spl,0,0,1,2,50.0,1,2,50.0,1.0,3,0.6666666666666666\nENST00000313949,+,58910401,58910682,Spl,16,3,1,1,100.0,1,2,50.0,2.0,3,0.3333333333333333\n</code></pre> <p>For convenience the above is shown in tabular format below. It may seem counter intuitive that the <code>conversion_total</code> exceed that of the <code>conversion_count</code> at baseline but it should be remembered that these are actually the total counts across replicates at each time point and so in one replicate there was a conversion, in another there wasn't.</p> Transcript_id Strand Start End Assignment day hour conversion_count conversion_total conversion_percent baseline_count baseline_total baseline_percent normalised_percent conversion_total_all_time_points conversion_weight ENST00000313949 + 58910401 58910682 Spl 0 0 1 2 50.0 1 2 50.0 1.0 3 0.6666666666666666 ENST00000313949 + 58910401 58910682 Spl 16 3 1 1 100.0 1 2 50.0 2.0 3 0.3333333333333333"},{"location":"api/","title":"API","text":"<ul> <li><code>io</code></li> <li><code>isoslam</code></li> <li><code>logging</code></li> <li><code>plotting</code></li> <li><code>processing</code></li> <li><code>summary</code></li> <li><code>utils</code></li> </ul>"},{"location":"api/io/","title":"IO Modules","text":"<p>Module for reading and writing files.</p>"},{"location":"api/io/#isoslam.io.create_config","title":"<code>create_config(args=None)</code>","text":"<p>Write the default configuration file to disk.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace | None</code> <p>Optional arguments to parse.</p> <code>None</code> Source code in <code>isoslam/io.py</code> <pre><code>def create_config(args: argparse.Namespace | None = None) -&gt; None:\n    \"\"\"\n    Write the default configuration file to disk.\n\n    Parameters\n    ----------\n    args : argparse.Namespace | None\n        Optional arguments to parse.\n    \"\"\"\n    filename = \"config\" if args.filename is None else args.filename  # type: ignore [union-attr]\n    output_dir = Path(\"./\") if args.output_dir is None else Path(args.output_dir)  # type: ignore [union-attr]\n    output_dir.mkdir(parents=True, exist_ok=True)\n    config_path = resources.files(__package__) / \"default_config.yaml\"\n    config = config_path.read_text()\n\n    if \".yaml\" not in str(filename) and \".yml\" not in str(filename):\n        create_config_path = output_dir / f\"{filename}.yaml\"\n    else:\n        create_config_path = output_dir / filename\n\n    with create_config_path.open(\"w\", encoding=\"utf-8\") as f:\n        f.write(f\"# Config file generated {_get_date_time()}\\n\")\n        f.write(f\"{CONFIG_DOCUMENTATION_REFERENCE}\")\n        f.write(config)\n    logger.info(f\"A sample configuration file has been written to : {str(create_config_path)}\")\n    logger.info(CONFIG_DOCUMENTATION_REFERENCE)\n</code></pre>"},{"location":"api/io/#isoslam.io.data_frame_to_file","title":"<code>data_frame_to_file(data, output_dir='./output/', outfile='summary_counts.tsv', sep='\\t', **kwargs)</code>","text":"<p>Write a Pandas DataFrame to disk.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame | DataFrame</code> <p>Pandas DataFrame to write to disk.</p> required <code>output_dir</code> <code>str | Path</code> <p>Location to write the output to, default is ''./output''.capitalize.</p> <code>'./output/'</code> <code>outfile</code> <code>str</code> <p>Filename to write data to.</p> <code>'summary_counts.tsv'</code> <code>sep</code> <code>str</code> <p>Separator to use in output file.</p> <code>'\\t'</code> <code>**kwargs</code> <code>dict[Any, Any]</code> <p>Dictionary of keyword arguments to pass to ''pandas.DataFrame.to_csv()''.</p> <code>{}</code> Source code in <code>isoslam/io.py</code> <pre><code>def data_frame_to_file(\n    data: pd.DataFrame | pl.DataFrame,\n    output_dir: str | Path = \"./output/\",\n    outfile: str = \"summary_counts.tsv\",\n    sep: str = \"\\t\",\n    **kwargs: dict[Any, Any],\n) -&gt; None:\n    \"\"\"\n    Write a Pandas DataFrame to disk.\n\n    Parameters\n    ----------\n    data : pd.DataFrame | pl.DataFrame\n        Pandas DataFrame to write to disk.\n    output_dir : str | Path\n        Location to write the output to, default is ''./output''.capitalize.\n    outfile : str\n        Filename to write data to.\n    sep : str\n        Separator to use in output file.\n    **kwargs\n        Dictionary of keyword arguments to pass to ''pandas.DataFrame.to_csv()''.\n    \"\"\"\n    outdir_file = Path(output_dir) / f\"{outfile}\"\n    Path(output_dir).mkdir(parents=True, exist_ok=True)\n    if isinstance(data, pl.DataFrame):\n        try:\n            if re.search(r\"parquet$\", str(outfile)):\n                data.write_parquet(outdir_file, **kwargs)\n            elif re.search(r\"\\..sv$\", str(outfile)):\n                data.write_csv(outdir_file, separator=sep, **kwargs)\n            logger.debug(f\"File written to : {outdir_file}\")\n        except Exception as e:\n            raise e\n    elif isinstance(data, pd.DataFrame):\n        try:\n            if re.search(r\"parquet\", str(outfile)):\n                data.to_parquet(outdir_file, **kwargs)\n            elif re.search(r\"\\..sv$\", str(outfile)):\n                data.to_csv(outdir_file, sep=sep, **kwargs)\n            logger.debug(f\"File written to : {outdir_file}\")\n        except Exception as e:\n            raise e\n    else:\n        raise TypeError(f\"Can not write output Pandas or Polar Dataframe object not supplied = {type(data)=}\")\n</code></pre>"},{"location":"api/io/#isoslam.io.load_and_update_config","title":"<code>load_and_update_config(args)</code>","text":"<p>Load a configuration file to dictionary and update entries with user supplied arguments.</p> <p>If ''args'' does not contain any value for ''args.config_file'' the default configuration (''isoslam/default_config.yaml'') is loaded, otherwise the user specified configuration is loaded.</p> <p>Once the configuration is loaded any user specified options update the dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace</code> <p>Arguments supplied by user.</p> required <p>Returns:</p> Type Description <code>dict[str:Any]</code> <p>Dictionary of configuration optionsupdated with user specified options.</p> Source code in <code>isoslam/io.py</code> <pre><code>def load_and_update_config(args: argparse.Namespace | None) -&gt; dict[str, Any]:\n    \"\"\"\n    Load a configuration file to dictionary and update entries with user supplied arguments.\n\n    If ''args'' does not contain any value for ''args.config_file'' the default configuration\n    (''isoslam/default_config.yaml'') is loaded, otherwise the user specified configuration is loaded.\n\n    Once the configuration is loaded any user specified options update the dictionary.\n\n    Parameters\n    ----------\n    args : argparse.Namespace\n        Arguments supplied by user.\n\n    Returns\n    -------\n    dict[str: Any]\n        Dictionary of configuration optionsupdated with user specified options.\n    \"\"\"\n    config = read_yaml() if vars(args)[\"config_file\"] is None else read_yaml(vars(args)[\"config_file\"])\n    config[\"schema\"] = _type_schema(config[\"schema\"])  # type: ignore[index]\n    return utils.update_config(config, vars(args))  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/io/#isoslam.io.load_file","title":"<code>load_file(file_path)</code>","text":"<p>Load files of different types.</p> <p>Supports the following file types...</p> <ul> <li><code>.bam</code> - The sequence data that is to be analysed.</li> <li><code>.bed</code> - The locations of introns/splice junctions.</li> <li><code>.gtf</code> - Transcript structures from which the <code>.bed</code> file is derived.</li> <li><code>.vcf</code> - Locations of known sequences difference from the reference sequence.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to file to load.</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Returns the loaded file as an object.</p> Source code in <code>isoslam/io.py</code> <pre><code>def load_file(file_path: str | Path) -&gt; Any:\n    \"\"\"\n    Load files of different types.\n\n    Supports the following file types...\n\n    * ``.bam`` - The sequence data that is to be analysed.\n    * ``.bed`` - The locations of introns/splice junctions.\n    * ``.gtf`` - Transcript structures from which the ``.bed`` file is derived.\n    * ``.vcf`` - Locations of known sequences difference from the reference sequence.\n\n    Parameters\n    ----------\n    file_path : str | Path\n        Path to file to load.\n\n    Returns\n    -------\n    Any\n        Returns the loaded file as an object.\n    \"\"\"\n    file_suffix = Path(file_path).suffix\n    if file_suffix == \".gz\":\n        file_suffix = \"\".join(Path(file_path).suffixes)\n    loader = _get_loader(file_suffix)\n    return loader(file_path)\n</code></pre>"},{"location":"api/io/#isoslam.io.load_output_files","title":"<code>load_output_files(file_ext='.tsv', directory=None)</code>","text":"<p>Read a set of files into a list of Polars DataFrames.</p> <p>Supports reading ''.parquet'', ''.tsv'' and <code>.csv</code>.</p> <p>Parameters:</p> Name Type Description Default <code>file_ext</code> <code>str</code> <p>File name pattern to search for.</p> <code>'.tsv'</code> <code>directory</code> <code>str | Path | None</code> <p>Directory to search for files.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[DataFrame]</code> <p>A list of Polars DataFrames of each file found.</p> Source code in <code>isoslam/io.py</code> <pre><code>def load_output_files(file_ext: str = \".tsv\", directory: str | Path | None = None) -&gt; dict[str, pl.DataFrame]:\n    \"\"\"\n    Read a set of files into a list of Polars DataFrames.\n\n    Supports reading ''.parquet'', ''.tsv'' and ``.csv``.\n\n    Parameters\n    ----------\n    file_ext : str\n        File name pattern to search for.\n    directory : str | Path | None\n        Directory to search for files.\n\n    Returns\n    -------\n    list[pl.DataFrame]\n        A list of Polars DataFrames of each file found.\n    \"\"\"\n    # This function could be refactored into a factory method with submethods for each file type\n    pattern = f\"*{file_ext}\"\n    if file_ext[file_ext.rfind(\".\") :] == \".parquet\":\n        results = {_file.stem: pl.read_parquet(_file) for _file in _find_files(pattern, directory)}\n    else:\n        if file_ext == \".tsv\":\n            separator = \"\\t\"\n        if file_ext == \".csv\":\n            separator = \",\"\n        results = {_file.stem: pl.read_csv(_file, separator=separator) for _file in _find_files(pattern, directory)}\n    return {key: df.with_columns(filename=pl.lit(key)) for key, df in results.items()}\n</code></pre>"},{"location":"api/io/#isoslam.io.read_yaml","title":"<code>read_yaml(filename=None)</code>","text":"<p>Read a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Union[str, Path]</code> <p>YAML file to read.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict</code> <p>Dictionary of the file.</p> Source code in <code>isoslam/io.py</code> <pre><code>def read_yaml(filename: str | Path | None = None) -&gt; dict[str, Any] | None:\n    \"\"\"\n    Read a YAML file.\n\n    Parameters\n    ----------\n    filename : Union[str, Path]\n        YAML file to read.\n\n    Returns\n    -------\n    Dict\n        Dictionary of the file.\n    \"\"\"\n    if filename is None:\n        filename = resources.files(__package__) / \"default_config.yaml\"  # type: ignore[assignment]\n    with Path(filename).open(encoding=\"utf-8\") as f:  # type: ignore[arg-type]\n        try:\n            yaml_file = YAML(typ=\"safe\")\n            return yaml_file.load(f)  # type: ignore[no-any-return]\n        except YAMLError as exception:\n            logger.error(exception)\n            return {}\n</code></pre>"},{"location":"api/io/#isoslam.io.write_assigned_conversions","title":"<code>write_assigned_conversions(assigned_conversions, coverage_counts, read_uid, assignment, outfile, delim)</code>","text":"<p>Write assigned conversions to files.</p> <p>Combines the ''coverage_counts'' with the ''assigned_conversions'' and outputs to disk at the specified location and filename with configurable delimiter.</p> <p>Parameters:</p> Name Type Description Default <code>assigned_conversions</code> <code>set[list[Any]]</code> <p>A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).</p> required <code>coverage_counts</code> <code>dict[str, int] dest_dir: str | Path</code> <p>A dictionary of coverage counts indexed by CHECK.</p> required <code>read_uid</code> <code>int</code> <p>Integer representing the unique read ID.</p> required <code>assignment</code> <code>str</code> <p>Type of assignment, either ''Rep'' or ''Spl'' (for Splice).</p> required <code>outfile</code> <code>Any</code> <p>Open connection to write results to.</p> required <code>delim</code> <code>str</code> <p>Delimiter to be used between fields, typically '','' for ''.csv'' or ''\\t'' for ''.tsv'' output.</p> required Source code in <code>isoslam/io.py</code> <pre><code>def write_assigned_conversions(  # pylint: disable=too-many-positional-arguments\n    assigned_conversions: set[list[Any]],\n    coverage_counts: dict[str, int],\n    read_uid: int,\n    assignment: str,\n    outfile: TextIOWrapper,\n    delim: str,\n) -&gt; None:\n    r\"\"\"\n    Write assigned conversions to files.\n\n    Combines the ''coverage_counts'' with the ''assigned_conversions'' and outputs to disk at the specified location and\n    filename with configurable delimiter.\n\n    Parameters\n    ----------\n    assigned_conversions : set[list[Any]]\n        A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).\n    coverage_counts : dict[str, int] dest_dir: str | Path\n        A dictionary of coverage counts indexed by CHECK.\n    read_uid : int\n        Integer representing the unique read ID.\n    assignment : str\n        Type of assignment, either ''Rep'' or ''Spl'' (for Splice).\n    outfile : Any\n        Open connection to write results to.\n    delim : str\n        Delimiter to be used between fields, typically '','' for ''.csv'' or ''\\t'' for ''.tsv'' output.\n    \"\"\"\n    for transcript_id, position in assigned_conversions:\n        start, end, chromosome, strand = position\n        outfile.write(\n            f\"{read_uid}{delim}{transcript_id}{delim}\"\n            f\"{start}{delim}{end}{delim}{chromosome}{delim}\"\n            f\"{strand}{delim}{assignment}{delim}{coverage_counts['converted_position']}{delim}\"\n            f\"{coverage_counts['convertible']}{delim}{coverage_counts['coverage']}\\n\"\n        )\n</code></pre>"},{"location":"api/io/#isoslam.io.write_yaml","title":"<code>write_yaml(config, output_dir, config_file='config.yaml', header_message=None)</code>","text":"<p>Write a configuration (stored as a dictionary) to a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Configuration dictionary.</p> required <code>output_dir</code> <code>Union[str, Path]</code> <p>Path to save the dictionary to as a YAML file (it will be called 'config.yaml').</p> required <code>config_file</code> <code>str</code> <p>Filename to write to.</p> <code>'config.yaml'</code> <code>header_message</code> <code>str</code> <p>String to write to the header message of the YAML file.</p> <code>None</code> Source code in <code>isoslam/io.py</code> <pre><code>def write_yaml(\n    config: dict,  # type: ignore[type-arg]\n    output_dir: str | Path,\n    config_file: str = \"config.yaml\",\n    header_message: str | None = None,\n) -&gt; None:\n    \"\"\"\n    Write a configuration (stored as a dictionary) to a YAML file.\n\n    Parameters\n    ----------\n    config : dict\n        Configuration dictionary.\n    output_dir : Union[str, Path]\n        Path to save the dictionary to as a YAML file (it will be called 'config.yaml').\n    config_file : str\n        Filename to write to.\n    header_message : str\n        String to write to the header message of the YAML file.\n    \"\"\"\n    output_config = Path(output_dir) / config_file\n    # Revert PosixPath items to string\n    config = _path_to_str(config)\n\n    if header_message:\n        header = f\"# {header_message} : {_get_date_time()}\\n\" + CONFIG_DOCUMENTATION_REFERENCE\n    else:\n        header = f\"# Configuration from IsoSLAM run completed : {_get_date_time()}\\n\" + CONFIG_DOCUMENTATION_REFERENCE\n    output_config.write_text(header, encoding=\"utf-8\")\n\n    yaml = YAML(typ=\"safe\")\n    with output_config.open(\"a\", encoding=\"utf-8\") as f:\n        try:\n            yaml.dump(config, f)\n        except YAMLError as exception:\n            logger.error(exception)\n</code></pre>"},{"location":"api/isoslam/","title":"IsoSLAM","text":"<p>IsoSLAM module.</p>"},{"location":"api/isoslam/#isoslam.isoslam.append_data","title":"<code>append_data(assigned_conversions, coverage_counts, read_uid, assignment, results, schema)</code>","text":"<p>Create a Polars dataframe combining the ''assigned_conversions'' and ''coverage_counts''.</p> <p>Adds ''assignment'' to the resulting dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>assigned_conversions</code> <code>set[list[Any]]</code> <p>A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).</p> required <code>coverage_counts</code> <code>dict[str, int] dest_dir: str | Path</code> <p>A dictionary of coverage counts indexed by CHECK.</p> required <code>read_uid</code> <code>int</code> <p>Integer representing the unique read ID.</p> required <code>assignment</code> <code>str</code> <p>Type of assignment, either ''Rep'' or ''Spl'' (for Splice).</p> required <code>results</code> <code>DataFrame</code> <p>Polars DataFrame to append data to. This will initially be empty but the schema matches the variables that are added.</p> required <code>schema</code> <code>dict[str, type]</code> <p>Schema dictionary for data frame.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Returns a Polars DataFrame of the data structure.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def append_data(  # pylint: disable=too-many-positional-arguments\n    assigned_conversions: frozenset[list[Any]],\n    coverage_counts: dict[str, int],\n    read_uid: int,\n    assignment: str,\n    results: pl.DataFrame,\n    schema: dict[str, type],\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Create a Polars dataframe combining the ''assigned_conversions'' and ''coverage_counts''.\n\n    Adds ''assignment'' to the resulting dataframe.\n\n    Parameters\n    ----------\n    assigned_conversions : set[list[Any]]\n        A set of assigned conversions. Each element of the set is a list of key features (CHECK WHAT THESE ARE).\n    coverage_counts : dict[str, int] dest_dir: str | Path\n        A dictionary of coverage counts indexed by CHECK.\n    read_uid : int\n        Integer representing the unique read ID.\n    assignment : str\n        Type of assignment, either ''Rep'' or ''Spl'' (for Splice).\n    results : pl.DataFrame\n        Polars DataFrame to append data to. This will initially be empty but the schema matches the variables that are\n        added.\n    schema : dict[str, type]\n        Schema dictionary for data frame.\n\n    Returns\n    -------\n    pl.DataFrame\n        Returns a Polars DataFrame of the data structure.\n    \"\"\"\n    if results is None:\n        results = pl.DataFrame(schema=schema)\n    for transcript_id, position in assigned_conversions:\n        start, end, chromosome, strand = position\n        row = pl.DataFrame(\n            data={\n                \"read_uid\": read_uid,\n                \"transcript_id\": transcript_id,\n                \"start\": start,\n                \"end\": end,\n                \"chr\": chromosome,\n                \"strand\": strand,\n                \"assignment\": assignment,\n                \"conversions\": coverage_counts[\"converted_position\"],\n                \"convertible\": coverage_counts[\"convertible\"],\n                \"coverage\": coverage_counts[\"coverage\"],\n            },\n            schema=schema,\n        )\n        results = pl.concat([results, row])\n    return results.sort(by=[\"read_uid\", \"transcript_id\", \"chr\", \"start\", \"end\"])\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.conversions_per_read","title":"<code>conversions_per_read(read, conversion_from, conversion_to, convertible, converted_position, coverage, vcf_file)</code>","text":"<p>Build sets of genome position for conversions, converted positions and coverage for a given read.</p> <p>Parameters:</p> Name Type Description Default <code>read</code> <code>dict[str, dict[str, Any]]</code> <p>Aligned read.</p> required <code>conversion_from</code> <code>str</code> <p>The base pair the conversion is from, typically either ''T'' or ''C''.</p> required <code>conversion_to</code> <code>str</code> <p>The base pair the conversion is to, typically the opposite pairing of ''from'', i.e. ''A'' or ''C'' respectively.</p> required <code>convertible</code> <code>set</code> <p>Set, possibly empty, to which the genome position is added if the sequence at a given location matches ''conversion_from''.</p> required <code>converted_position</code> <code>set</code> <p>Set, possibly, empty, to which the genome position is added if a conversion has occurred.</p> required <code>coverage</code> <code>set</code> <p>Set, possibly empty, to which the genome position is added for all aligned pairs of a read.</p> required <code>vcf_file</code> <code>VariantFile</code> <p>VCF file.</p> required <p>Returns:</p> Type Description <code>tuple[set[str], set[str], set[str]]</code> <p>Three sets of the ''convertible'', ''converted_position'' and ''coverage''.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def conversions_per_read(  # pylint: disable=too-many-positional-arguments\n    read: AlignedSegment,\n    conversion_from: str,\n    conversion_to: str,\n    convertible: set[str],\n    converted_position: set[str],\n    coverage: set[str],\n    vcf_file: VariantFile,\n) -&gt; tuple[set[str], set[str], set[str]]:\n    \"\"\"\n    Build sets of genome position for conversions, converted positions and coverage for a given read.\n\n    Parameters\n    ----------\n    read : dict[str, dict[str, Any]]\n        Aligned read.\n    conversion_from : str\n        The base pair the conversion is from, typically either ''T'' or ''C''.\n    conversion_to : str\n        The base pair the conversion is to, typically the opposite pairing of ''from'', i.e. ''A'' or ''C''\n        respectively.\n    convertible : set\n        Set, possibly empty, to which the genome position is added if the sequence at a given location matches\n        ''conversion_from''.\n    converted_position : set\n        Set, possibly, empty, to which the genome position is added if a conversion has occurred.\n    coverage : set\n        Set, possibly empty, to which the genome position is added for all aligned pairs of a read.\n    vcf_file : VariantFile\n        VCF file.\n\n    Returns\n    -------\n    tuple[set[str], set[str], set[str]]\n        Three sets of the ''convertible'', ''converted_position'' and ''coverage''.\n    \"\"\"\n    # Ensure we have upper case conversions to compare\n    conversion_from = conversion_from.upper()\n    conversion_to = conversion_to.upper()\n    for read_position, genome_position, genome_sequence in read.get_aligned_pairs(with_seq=True):\n        if None in (read_position, genome_position, genome_sequence):\n            continue\n        coverage.add(genome_position)\n        if genome_sequence.upper() == conversion_from:\n            convertible.add(genome_position)\n\n        # If the sequence at this position has been converted compared to the genome sequence...\n        if read.query_sequence[read_position].upper() == conversion_to and genome_sequence.upper() == conversion_from:\n            # ...check that this is a new variant at this position? Question : Is this the correctinterpretation?\n            variants_at_position = list(vcf_file.fetch(read.reference_name, genome_position, genome_position + 1))\n            if variants_at_position:\n                if any(variant.alts[0].upper() == conversion_to.upper() for variant in variants_at_position):\n                    pass\n                else:\n                    converted_position.add(genome_position)\n            else:\n                converted_position.add(genome_position)\n    # logger.debug(f\"convertible : {convertible}\\nconverted_position : {converted_position}\\ncoverage : {coverage}\")\n    return (convertible, converted_position, coverage)\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.count_conversions_across_pairs","title":"<code>count_conversions_across_pairs(forward_read, reverse_read, vcf_file, forward_conversion=None, reverse_conversion=None)</code>","text":"<p>Count conversions across paired reads.</p> <p>Parameters:</p> Name Type Description Default <code>forward_read</code> <code>dict[str, dict[str, Any]]</code> <p>Aligned segment for forward read.</p> required <code>reverse_read</code> <code>dict[str, dict[str, Any]]</code> <p>Aligned segment for reversed read.</p> required <code>vcf_file</code> <code>VariantFile</code> <p>Variant File.</p> required <code>forward_conversion</code> <code>dict</code> <p>Forward conversion dictionary typically ''{\"from\": \"A\", \"to\": \"G\"}''.</p> <code>None</code> <code>reverse_conversion</code> <code>dict</code> <p>Reverse conversion, typically ''{\"from\": \"T\", \"to\": \"C\"}''.</p> <code>None</code> <p>Returns:</p> Type Description <code>tuple[int, int, int]</code> <p>Tuple of the number of convertible base pairs, the number of conversions and the coverage of the paired alignments.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>ValueError is raised if either ''forward_conversion'' or ''reverse_conversion'' is ''None''.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def count_conversions_across_pairs(\n    forward_read: dict[str, dict[str, Any]],\n    reverse_read: dict[str, dict[str, Any]],\n    vcf_file: VariantFile,\n    forward_conversion: dict[str, str] | None = None,\n    reverse_conversion: dict[str, str] | None = None,\n) -&gt; dict[str, int]:\n    \"\"\"\n    Count conversions across paired reads.\n\n    Parameters\n    ----------\n    forward_read : dict[str, dict[str, Any]]\n        Aligned segment for forward read.\n    reverse_read : dict[str, dict[str, Any]]\n        Aligned segment for reversed read.\n    vcf_file : VariantFile\n        Variant File.\n    forward_conversion : dict, optional\n        Forward conversion dictionary typically ''{\"from\": \"A\", \"to\": \"G\"}''.\n    reverse_conversion : dict, optional\n        Reverse conversion, typically ''{\"from\": \"T\", \"to\": \"C\"}''.\n\n    Returns\n    -------\n    tuple[int, int, int]\n        Tuple of the number of convertible base pairs, the number of conversions and the coverage of the paired\n        alignments.\n\n    Raises\n    ------\n    ValueError\n        ValueError is raised if either ''forward_conversion'' or ''reverse_conversion'' is ''None''.\n    \"\"\"\n    if forward_conversion is None:\n        raise ValueError(\"forward_conversion can not be empty.\")\n    if reverse_conversion is None:\n        raise ValueError(\"reverse_conversion can not be empty.\")\n\n    # Count conversions on the forward read\n    convertible, converted_position, coverage = conversions_per_read(\n        forward_read,\n        forward_conversion[\"from\"],\n        forward_conversion[\"to\"],\n        convertible=set(),\n        converted_position=set(),\n        coverage=set(),\n        vcf_file=vcf_file,\n    )\n    # Count conversions on the reverse read\n    convertible, converted_position, coverage = conversions_per_read(\n        reverse_read,\n        reverse_conversion[\"from\"],\n        reverse_conversion[\"to\"],\n        convertible,\n        converted_position,\n        coverage,\n        vcf_file,\n    )\n    # logger.debug(\"Counted conversions paired reads\")\n    return {\"convertible\": len(convertible), \"converted_position\": len(converted_position), \"coverage\": len(coverage)}\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_features_from_pair","title":"<code>extract_features_from_pair(pair)</code>","text":"<p>Extract features from a pair of reads.</p> <p>Parameters:</p> Name Type Description Default <code>pair</code> <code>list[AlignedSegment]</code> <p>A list of two aligned segments from <code>pysam</code>.</p> required <p>Returns:</p> Type Description <code>dic[str, dict[str, Any]]</code> <p>Returns a nested dictionaries of the <code>start</code>, <code>end</code> and <code>length</code> of each read.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_features_from_pair(pair: list[AlignedSegment]) -&gt; dict[str, dict[str, Any]]:\n    \"\"\"\n    Extract features from a pair of reads.\n\n    Parameters\n    ----------\n    pair : list[AlignedSegment]\n        A list of two aligned segments from ``pysam``.\n\n    Returns\n    -------\n    dic[str, dict[str, Any]]\n        Returns a nested dictionaries of the ``start``, ``end`` and ``length`` of each read.\n    \"\"\"\n    return {\n        \"read1\": extract_features_from_read(pair[0]),\n        \"read2\": extract_features_from_read(pair[1]),\n    }\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_features_from_read","title":"<code>extract_features_from_read(read)</code>","text":"<p>Extract start, end and length from an aligned segment read.</p> <p>Parameters:</p> Name Type Description Default <code>read</code> <code>AlignedSegment</code> <p>An aligned segment read from <code>pysam</code>.</p> required <p>Returns:</p> Type Description <code>dict[str, Any]</code> <p>Dictionary of <code>start</code>, <code>end</code> and <code>length</code> of the segment.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_features_from_read(read: AlignedSegment) -&gt; dict[str, int | str | None | tuple[int, int]]:\n    \"\"\"\n    Extract start, end and length from an aligned segment read.\n\n    Parameters\n    ----------\n    read : AlignedSegment\n        An aligned segment read from ``pysam``.\n\n    Returns\n    -------\n    dict[str, Any]\n        Dictionary of ``start``, ``end`` and ``length`` of the segment.\n    \"\"\"\n    block_start, block_end = zip(*read.get_blocks())\n    try:\n        status = read.get_tag(\"XS\")\n    except KeyError:\n        status = None\n    try:\n        transcript = read.get_tag(\"XT\")\n    except KeyError:\n        transcript = None\n    try:\n        reverse = read.is_reverse\n    except KeyError:\n        reverse = None\n    return {\n        \"start\": read.reference_start,\n        \"end\": read.reference_end,\n        \"length\": read.query_length,\n        \"status\": status,\n        \"transcript\": transcript,\n        \"block_start\": block_start,\n        \"block_end\": block_end,\n        \"reverse\": reverse,\n    }\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_segment_pairs","title":"<code>extract_segment_pairs(bam_file)</code>","text":"<p>Extract pairs of AlignedSegments from a <code>.bam</code> file.</p> <p>When there are two adjacent <code>AlignedSegments</code> with the same <code>query_name</code> only the first is paired, subsequent segments are dropped.</p> <p>Parameters:</p> Name Type Description Default <code>bam_file</code> <code>str | Path</code> <p>Path to a <code>.bam</code> file.</p> required <p>Yields:</p> Type Description <code>Generator</code> <p>Itterable of paired segments.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_segment_pairs(bam_file: str | Path) -&gt; Generator[AlignedSegment]:\n    \"\"\"\n    Extract pairs of AlignedSegments from a ``.bam`` file.\n\n    When there are two adjacent ``AlignedSegments`` with the same ``query_name`` only the first is paired, subsequent\n    segments are dropped.\n\n    Parameters\n    ----------\n    bam_file : str | Path\n        Path to a ``.bam`` file.\n\n    Yields\n    ------\n    Generator\n        Itterable of paired segments.\n    \"\"\"\n    previous_read: str | None = None\n    pair: list[AlignedSegment] = []\n    for read in io.load_file(bam_file):\n        # Return pairs of reads, i.e. not on first pass, nor if query_name matches the previous read\n        # Can lead to len(pair) be &gt; 2\n        if previous_read is not None and previous_read != read.query_name:\n            yield pair\n            pair = []\n            previous_read = read.query_name\n        previous_read = read.query_name\n        pair.append(read)\n    # Don't forget to return the last pair!\n    yield pair\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_strand_transcript","title":"<code>extract_strand_transcript(gtf_file)</code>","text":"<p>Extract strand and transcript ID data from <code>.gtf</code> file.</p> <p>Parameters:</p> Name Type Description Default <code>gtf_file</code> <code>Path | str</code> <p>Path to a 'gtf' file.</p> required <p>Returns:</p> Type Description <code>tuple[dict[str, tuple[str]], dict[str, tuple[str]]]</code> <p>Two dictionaries are returned, one of the <code>strand</code> the other of the <code>transcript_id</code> both using the <code>gene_id</code> as the key.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_strand_transcript(gtf_file: str | Path) -&gt; tuple[defaultdict[Any, Any], defaultdict[Any, list[Any]]]:\n    \"\"\"\n    Extract strand and transcript ID data from ``.gtf`` file.\n\n    Parameters\n    ----------\n    gtf_file : Path | str\n        Path to a 'gtf' file.\n\n    Returns\n    -------\n    tuple[dict[str, tuple[str]], dict[str, tuple[str]]]\n        Two dictionaries are returned, one of the ``strand`` the other of the ``transcript_id`` both using the\n        ``gene_id`` as the key.\n    \"\"\"\n    strand = defaultdict(str)\n    transcript = defaultdict(list)\n    for entry in io.load_file(gtf_file):\n        if not entry.feature == \"transcript\":\n            continue\n        strand[entry.gene_id] = entry.strand\n        transcript[entry.gene_id].append(entry.transcript_id)\n    logger.info(f\"Extracted features from : {gtf_file}\")\n    return (strand, transcript)\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_transcripts","title":"<code>extract_transcripts(bed_file)</code>","text":"<p>Extract features from <code>.bed</code> file and return as a dictionary indexed by <code>transcript_id</code>.</p> <p>Parameters:</p> Name Type Description Default <code>bed_file</code> <code>str | Path</code> <p>Path, as string or pathlib Path, to a <code>.bed</code> file.</p> required <p>Returns:</p> Type Description <code>dict[Any, list[tuple[Any, int, int, Any, Any]]]</code> <p>Dictionary of <code>chromosome</code>, <code>start</code>, <code>end</code>, <code>transcript_id</code> and <code>bedstrand</code> indexed by <code>transcript_id</code>.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_transcripts(bed_file: str | Path) -&gt; dict[Any, list[tuple[Any, int, int, Any, Any]]]:\n    \"\"\"\n    Extract features from ``.bed`` file and return as a dictionary indexed by ``transcript_id``.\n\n    Parameters\n    ----------\n    bed_file : str | Path\n        Path, as string or pathlib Path, to a ``.bed`` file.\n\n    Returns\n    -------\n    dict[Any, list[tuple[Any, int, int, Any, Any]]]\n        Dictionary of ``chromosome``, ``start``, ``end``, ``transcript_id`` and ``bedstrand`` indexed by\n        ``transcript_id``.\n    \"\"\"\n    coordinates = defaultdict(list)\n    for line in io.load_file(bed_file):\n        contents = line.strip().split(\"\\t\")\n        transcript_id = contents[3].replace(\"_intron\", \"\")\n        coordinates[transcript_id].append(\n            (\n                contents[0],\n                int(contents[1]),\n                int(contents[2]),\n                transcript_id,\n                contents[5],\n            )\n        )\n    logger.info(f\"Extracted features from : {bed_file}\")\n    return coordinates\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.extract_utron","title":"<code>extract_utron(features, gene_transcript, coordinates)</code>","text":"<p>Extract and sum the utrons based on tag.</p> <p>ACTION : This function needs better documentation, my guess is that its extracting the transcripts to genes and then getting some related information (what I'm not sure) from the .bed file and adding these up.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>str</code> <p>A tag from an assigned read.</p> required <code>gene_transcript</code> <code>TextIO</code> <p>Transcript to gene from a <code>.gtf</code> file.</p> required <code>coordinates</code> <code>Any</code> <p>Untranslated region coordinates from a <code>.bed</code> file.</p> required <p>Returns:</p> Type Description <code>list | None</code> <p>List of the length of assigned regions.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def extract_utron(features: dict[str, Any], gene_transcript: Any, coordinates: Any) -&gt; list[tuple[int | str]] | None:\n    \"\"\"\n    Extract and sum the utrons based on tag.\n\n    ACTION : This function needs better documentation, my guess is that its extracting the transcripts to genes and\n    then getting some related information (what I'm not sure) from the .bed file and adding these up.\n\n    Parameters\n    ----------\n    features : str\n        A tag from an assigned read.\n    gene_transcript : TextIO\n        Transcript to gene from a ``.gtf`` file.\n    coordinates : Any\n        Untranslated region coordinates from a ``.bed`` file.\n\n    Returns\n    -------\n    list | None\n        List of the length of assigned regions.\n    \"\"\"\n    if features[\"status\"] == \"Assigned\":\n        untranslated_regions = [coordinates[transcript] for transcript in gene_transcript[features[\"transcript\"]]]\n        return sum(untranslated_regions, [])\n    return []\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.filter_spliced_utrons","title":"<code>filter_spliced_utrons(pair_features, blocks, read='read1')</code>","text":"<p>Filter utrons where start is in the block ends or end is in the block start.</p> <p>Parameters:</p> Name Type Description Default <code>pair_features</code> <code>dict[str, dict]</code> <p>Dictionary of extracted features and utron in both read directions.</p> required <code>blocks</code> <code>dic[str:(dict[str, set])]</code> <p>Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.</p> required <code>read</code> <code>str</code> <p>Direction of read to filter on, default is ''read1'' but can also use ''read2''.</p> <code>'read1'</code> <p>Returns:</p> Type Description <code>dict[str, tuple(Any)]</code> <p>Dictionary of the chromosome, start, end and strand of transcripts that are within introns.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def filter_spliced_utrons(\n    pair_features: dict[str, dict[str, Any]],\n    blocks: dict[str, dict[str, tuple[Any, ...]]],\n    read: str = \"read1\",\n) -&gt; dict[str, list[Any]]:\n    \"\"\"\n    Filter utrons where start is in the block ends or end is in the block start.\n\n    Parameters\n    ----------\n    pair_features : dict[str, dict]\n        Dictionary of extracted features and utron in both read directions.\n    blocks : dic[str: dict[str, set]]\n        Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.\n    read : str\n        Direction of read to filter on, default is ''read1'' but can also use ''read2''.\n\n    Returns\n    -------\n    dict[str, tuple(Any)]\n        Dictionary of the chromosome, start, end and strand of transcripts that are within introns.\n    \"\"\"\n    spliced_3ui: dict[str, list[Any]] = {}\n    for chromosome, start, end, transcript_id, strand in pair_features[read][\"utron\"]:\n        if start in blocks[read][\"ends\"] and end in blocks[read][\"starts\"]:\n            # Why add an empty list and append a tuple?\n            if transcript_id not in spliced_3ui:\n                spliced_3ui[transcript_id] = []\n            spliced_3ui[transcript_id].append((start, end, chromosome, strand))\n    return spliced_3ui\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.filter_within_introns","title":"<code>filter_within_introns(pair_features, blocks, read='read1')</code>","text":"<p>Filter utrons that are within introns.</p> <p>Parameters:</p> Name Type Description Default <code>pair_features</code> <code>dict[str, dict]</code> <p>Dictionary of extracted features and utron in both read directions.</p> required <code>blocks</code> <code>dic[str:(dict[str, set])]</code> <p>Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.</p> required <code>read</code> <code>str</code> <p>Direction of read to filter on, default is ''read1'' but can also use ''read2''.</p> <code>'read1'</code> <p>Returns:</p> Type Description <code>dict[str, tuple(Any)]</code> <p>Dictionary of the chromosome, start, end and strand of transcripts that are within introns.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def filter_within_introns(\n    pair_features: dict[str, dict[str, Any]],\n    blocks: dict[str, dict[str, tuple[Any, ...]]],\n    read: str = \"read1\",\n) -&gt; dict[str, tuple[Any]]:\n    \"\"\"\n    Filter utrons that are within introns.\n\n    Parameters\n    ----------\n    pair_features : dict[str, dict]\n        Dictionary of extracted features and utron in both read directions.\n    blocks : dic[str: dict[str, set]]\n        Nested dictionary of start and ends for each read. Top level is read, with a dictionary of start and end.\n    read : str\n        Direction of read to filter on, default is ''read1'' but can also use ''read2''.\n\n    Returns\n    -------\n    dict[str, tuple(Any)]\n        Dictionary of the chromosome, start, end and strand of transcripts that are within introns.\n    \"\"\"\n    within_intron: dict[str, Any] = {}\n    for chromosome, start, end, transcript_id, strand in pair_features[read][\"utron\"]:\n        start_end_within_intron = (\n            start &lt;= pair_features[read][\"start\"] &lt;= end or start &lt;= pair_features[read][\"end\"] &lt;= end\n        )\n        spans_intron = (\n            pair_features[read][\"start\"] &lt; start\n            and pair_features[read][\"end\"] &gt; end\n            and (end - start) &lt; pair_features[read][\"length\"]\n        )\n        if (  # pylint: disable=too-many-boolean-expressions\n            (start_end_within_intron or spans_intron)\n            # Start should not be in ends and ends should not be in start, can we combine the start and end block\n            # sets I wonder?\n            and start not in blocks[\"read1\"][\"ends\"]\n            and end not in blocks[\"read1\"][\"starts\"]\n            and start not in blocks[\"read2\"][\"ends\"]\n            and end not in blocks[\"read2\"][\"starts\"]\n        ):\n            # Why add an empty list and append a tuple?\n            if transcript_id not in within_intron:\n                within_intron[transcript_id] = []\n            within_intron[transcript_id].append((start, end, chromosome, strand))\n    return within_intron\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.remove_common_reads","title":"<code>remove_common_reads(retained, spliced)</code>","text":"<p>Remove reads that are common to both retained and spliced sets.</p> <p>Parameters:</p> Name Type Description Default <code>retained</code> <code>set[list[Any]]</code> <p>Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'', ''chromosome'' and ''strand''.</p> required <code>spliced</code> <code>set[list[Any]]</code> <p>Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'', ''chromosome'' and ''strand''.</p> required <p>Returns:</p> Type Description <code>tuple[set[list[Any]], set[list[Any]]]</code> <p>A tuple of the ''retained'' (first) and ''spliced'' reads with common items removed.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def remove_common_reads(\n    retained: frozenset[list[Any]], spliced: frozenset[list[Any]]\n) -&gt; tuple[frozenset[list[Any]], frozenset[list[Any]]]:\n    \"\"\"\n    Remove reads that are common to both retained and spliced sets.\n\n    Parameters\n    ----------\n    retained : set[list[Any]]\n        Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'',\n        ''chromosome'' and ''strand''.\n    spliced : set[list[Any]]\n        Set of retained reads. Each item is a tuple with ''transcript_id'' and a list of ''start'', ''end'',\n        ''chromosome'' and ''strand''.\n\n    Returns\n    -------\n    tuple[set[list[Any]], set[list[Any]]]\n        A tuple of the ''retained'' (first) and ''spliced'' reads with common items removed.\n    \"\"\"\n    common = retained &amp; spliced\n    retained -= common\n    spliced -= common\n    # logger.debug(\"Removed common elements from retained and spliced.\")\n    return (retained, spliced)\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.unique_conversions","title":"<code>unique_conversions(reads1, reads2)</code>","text":"<p>Create a unique set of conversions that are to be retained.</p> <p>Parameters:</p> Name Type Description Default <code>reads1</code> <code>dict[str, list[tuple[Any]]]</code> <p>A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'', ''chromsome'' and ''strand'' recorded.</p> required <code>reads2</code> <code>dict[str, list[tuple[Any]]]</code> <p>A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'', ''chromsome'' and ''strand'' recorded.</p> required <p>Returns:</p> Type Description <code>set[list[Any]]</code> <p>Combines the two sets of observations and de-duplicates them, returning only the unique assigned conversions.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def unique_conversions(\n    reads1: dict[str, Any],\n    reads2: dict[str, Any],\n) -&gt; frozenset[list[Any]]:\n    \"\"\"\n    Create a unique set of conversions that are to be retained.\n\n    Parameters\n    ----------\n    reads1 : dict[str, list[tuple[Any]]]\n        A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'',\n        ''chromsome'' and ''strand'' recorded.\n    reads2 : dict[str, list[tuple[Any]]]\n        A dictionary of reads mapped to transcripts (key) which overlap introns. Each read has the ''start'',  ''end'',\n        ''chromsome'' and ''strand'' recorded.\n\n    Returns\n    -------\n    set[list[Any]]\n        Combines the two sets of observations and de-duplicates them, returning only the unique assigned conversions.\n    \"\"\"\n    flat1 = [(key, nested_list) for key, values in reads1.items() for nested_list in values]\n    flat2 = [(key, nested_list) for key, values in reads2.items() for nested_list in values]\n    # logger.debug(\"Extracted unique conversions in both reads, combining to unique set.\")\n    return frozenset(flat1 + flat2)  # type: ignore[arg-type]\n</code></pre>"},{"location":"api/isoslam/#isoslam.isoslam.zip_blocks","title":"<code>zip_blocks(read)</code>","text":"<p>Zip the block starts and ends into two lists.</p> <p>Parameters:</p> Name Type Description Default <code>read</code> <code>AlignedSegment</code> <p>An individual aligned segment read from a ''.bam'' file.</p> required <p>Returns:</p> Type Description <code>tuple[list[int], list[int]]</code> <p>Tuple of two lists of integers the first is start location, the second is the end location.</p> Source code in <code>isoslam/isoslam.py</code> <pre><code>def zip_blocks(read: AlignedSegment) -&gt; Iterator[tuple[Any, ...]]:\n    \"\"\"\n    Zip the block starts and ends into two lists.\n\n    Parameters\n    ----------\n    read : AlignedSegment\n        An individual aligned segment read from a ''.bam'' file.\n\n    Returns\n    -------\n    tuple[list[int], list[int]]\n        Tuple of two lists of integers the first is start location, the second is the end location.\n    \"\"\"\n    return zip(*read.get_blocks())\n</code></pre>"},{"location":"api/logging/","title":"Logging","text":"<p>Setup and configure logging.</p>"},{"location":"api/logging/#isoslam.logging.setup","title":"<code>setup(level='INFO')</code>","text":"<p>Loguru setup with the required logging level and format.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>str</code> <p>Log level, default is \"INFO\", other options \"WARNING\", \"DEBUG\" etc.</p> <code>'INFO'</code> Source code in <code>isoslam/logging.py</code> <pre><code>def setup(level: str = \"INFO\") -&gt; None:\n    \"\"\"\n    Loguru setup with the required logging level and format.\n\n    Parameters\n    ----------\n    level : str\n        Log level, default is \"INFO\", other options \"WARNING\", \"DEBUG\" etc.\n    \"\"\"\n    logger.remove()\n    logger.add(sys.stderr)\n    logger.add(\n        sys.stderr,\n        colorize=True,\n        level=level.upper(),\n        format=\"&lt;green&gt;{time:HH:mm:ss}&lt;/green&gt; \"\n        \"| &lt;level&gt;{level}&lt;/level&gt; | \"\n        \"&lt;magenta&gt;{file}&lt;/magenta&gt;:&lt;magenta&gt;{module}&lt;/magenta&gt;:&lt;magenta&gt;{function}&lt;/magenta&gt;:&lt;magenta&gt;{line}&lt;/magenta&gt;\"\n        \" | &lt;level&gt;{message}&lt;/level&gt;\",\n    )\n</code></pre>"},{"location":"api/plotting/","title":"Plotting","text":"<p>Functions for plotting output.</p>"},{"location":"api/processing/","title":"Processing","text":"<p>Entry point, sub-parsers and arguments and processing functions.</p>"},{"location":"api/processing/#isoslam.processing.create_parser","title":"<code>create_parser()</code>","text":"<p>Create a parser for reading options.</p> <p>Parser is created with multiple sub-parsers for eading options to run <code>isoslam</code>.</p> <p>Returns:</p> Type Description <code>ArgumentParser</code> <p>Argument parser.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def create_parser() -&gt; arg.ArgumentParser:\n    \"\"\"\n    Create a parser for reading options.\n\n    Parser is created with multiple sub-parsers for eading options to run ``isoslam``.\n\n    Returns\n    -------\n    arg.ArgumentParser\n        Argument parser.\n    \"\"\"\n    parser = arg.ArgumentParser(\n        description=\"Run various programs related to IsoSLAM. Add the name of the program you wish to run.\"\n    )\n    parser.add_argument(\n        \"-v\",\n        \"--version\",\n        action=\"version\",\n        version=f\"Installed version of IsoSlam : {__version__}\",\n        help=\"Report the installed version of IsoSLAM.\",\n    )\n    parser.add_argument(\n        \"-c\",\n        \"--config-file\",\n        dest=\"config_file\",\n        type=Path,\n        required=False,\n        help=\"Path to a YAML configuration file.\",\n    )\n    parser.add_argument(\n        \"-b\",\n        \"--base-dir\",\n        dest=\"base_dir\",\n        type=Path,\n        required=False,\n        help=\"Base directory to run isoslam on.\",\n    )\n    parser.add_argument(\n        \"-o\",\n        \"--output-dir\",\n        dest=\"output_dir\",\n        type=Path,\n        required=False,\n        help=\"Output directory to write results to.\",\n    )\n    parser.add_argument(\n        \"-l\",\n        \"--log-level\",\n        dest=\"log_level\",\n        type=str,\n        required=False,\n        help=\"Logging level to use, default is 'info' for verbose output use 'debug'.\",\n    )\n\n    subparsers = parser.add_subparsers(title=\"program\", description=\"Available programs listed below:\", dest=\"program\")\n\n    # Create sub-parsers for different stages\n\n    # Process processes all files\n    process_parser = subparsers.add_parser(\n        \"process\",\n        description=\"Process all files and run all summary plotting and statistics.\",\n        help=\"Process all files and run all summary plotting and statistics.\",\n    )\n    process_parser.add_argument(\n        \"-b\",\n        \"--bam-file\",\n        dest=\"bam_file\",\n        type=Path,\n        required=False,\n        help=\"Path to '.bam' file that has undergone read assignment with 'featureCount'.\",\n    )\n    process_parser.add_argument(\n        \"-g\", \"--gtf-file\", dest=\"gtf_file\", type=Path, required=False, help=\"Path to '.gtf' transcript assembly file.\"\n    )\n    process_parser.add_argument(\n        \"-d\",\n        \"--bed-file\",\n        dest=\"bed_file\",\n        type=Path,\n        required=False,\n        help=\"Path to '.bed' utron file. Must be bed6 format.\",\n    )\n    process_parser.add_argument(\n        \"-v\", \"--vcf-file\", dest=\"vcf_file\", type=Path, required=False, help=\"Path to '.vcf.gz' file.\"\n    )\n    process_parser.add_argument(\n        \"-u\",\n        \"--upper-pairs-limit\",\n        dest=\"upper_pairs_limit\",\n        type=int,\n        required=False,\n        help=\"Upper limit of pairs to be processed.\",\n    )\n    process_parser.add_argument(\n        \"-f\",\n        \"--first-matched-limit\",\n        dest=\"first_matched_limit\",\n        type=int,\n        required=False,\n        help=\"Limit of matches.\",\n    )\n    process_parser.add_argument(\n        \"--delim\",\n        dest=\"delim\",\n        type=str,\n        required=False,\n        help=\"Delimiter to use in output.\",\n    )\n    process_parser.add_argument(\n        \"--output-file\",\n        dest=\"output_file\",\n        type=str,\n        required=False,\n        help=\"File to write results to.\",\n    )\n    process_parser.set_defaults(func=process)\n\n    # Create configuration sub-parser\n    create_config_parser = subparsers.add_parser(\n        \"create-config\",\n        description=\"Create a configuration file using the defaults.\",\n        help=\"Create a configuration file using the defaults.\",\n    )\n    create_config_parser.add_argument(\n        \"-f\",\n        \"--filename\",\n        dest=\"filename\",\n        type=Path,\n        required=False,\n        default=\"config.yaml\",\n        help=\"Name of YAML file to save configuration to (default 'config.yaml').\",\n    )\n    create_config_parser.add_argument(\n        \"-o\",\n        \"--output-dir\",\n        dest=\"output_dir\",\n        type=Path,\n        required=False,\n        default=\"./\",\n        help=\"Path to where the YAML file should be saved (default './' the current directory).\",\n    )\n    create_config_parser.set_defaults(func=io.create_config)\n\n    # Summarise counts sub-parser\n    summary_counts_parser = subparsers.add_parser(\n        \"summary-counts\",\n        description=\"Summarise the counts.\",\n        help=\"Summarise the counts.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--file-ext\",\n        dest=\"file_ext\",\n        type=str,\n        required=False,\n        default=\".tsv\",\n        help=\"File extension of summarized files to process.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--directory\",\n        dest=\"directory\",\n        type=Path,\n        required=False,\n        help=\"Directory to search for input files.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--conversions-var\",\n        dest=\"conversions_var\",\n        type=str,\n        required=False,\n        help=\"Name of column that holds details of conversions.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--conversions-threshold\",\n        dest=\"conversions_threshold\",\n        type=int,\n        required=False,\n        help=\"Minimum number of conversions.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--test-file\",\n        dest=\"test_file\",\n        type=str,\n        required=False,\n        help=\"Pattern used in test file names.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--filename-var\",\n        dest=\"filename_var\",\n        type=str,\n        required=False,\n        help=\"Name of column that holds file names.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--regex\",\n        dest=\"regex\",\n        type=str,\n        required=False,\n        help=\"Regular expression for extracting day/hour/replication from filenames.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--outfile\",\n        dest=\"outfile\",\n        type=Path,\n        required=False,\n        default=\"summary_counts.tsv\",\n        help=\"Output filename to save results to, will be nested under 'output_dir'.\",\n    )\n    summary_counts_parser.add_argument(\n        \"--separator\",\n        dest=\"sep\",\n        type=str,\n        required=False,\n        default=\"\\t\",\n        help=\"Field separator to use in output file, default is '\\t' but other values (e.g. ',' are allowed).\",\n    )\n    summary_counts_parser.set_defaults(func=summarise_counts)\n\n    # Additional parsers for future functionality\n    # summarize_counts_parser = subparsers.add_parser(\n    #     \"summarize\",\n    #     description=\"Summarize counts.\",\n    #     help=\"Summarize counts.\",\n    # )\n    # summarize_counts_parser.set_defaults(func=summarize_counts)\n    # plot_conversions_parser = subparsers.add_parser(\n    #     \"plot_conversions\",\n    #     description=\"Plot conversions.\",\n    #     help=\"Plot conversions.\",\n    # )\n    # plot_conversions_parser.set_defaults(func=plot_conversions)\n    return parser\n</code></pre>"},{"location":"api/processing/#isoslam.processing.entry_point","title":"<code>entry_point(manually_provided_args=None, testing=False)</code>","text":"<p>Entry point for all IsoSLAM programs.</p> <p>Main entry point for running 'isoslam' which allows the different processing, plotting and testing modules to be run.</p> <p>Parameters:</p> Name Type Description Default <code>manually_provided_args</code> <code>None</code> <p>Manually provided arguments.</p> <code>None</code> <code>testing</code> <code>bool</code> <p>Whether testing is being carried out.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Function does not return anything.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def entry_point(manually_provided_args: list[Any] | None = None, testing: bool = False) -&gt; None | arg.Namespace:\n    \"\"\"\n    Entry point for all IsoSLAM programs.\n\n    Main entry point for running 'isoslam' which allows the different processing, plotting and testing modules to be\n    run.\n\n    Parameters\n    ----------\n    manually_provided_args : None\n        Manually provided arguments.\n    testing : bool\n        Whether testing is being carried out.\n\n    Returns\n    -------\n    None\n        Function does not return anything.\n    \"\"\"\n    parser = create_parser()\n    args = parser.parse_args() if manually_provided_args is None else parser.parse_args(manually_provided_args)\n\n    # If no module has been specified print help and exit\n    if not args.program:\n        parser.print_help()\n        sys.exit()\n\n    if testing:\n        return args\n\n    # Run the specified module(s)\n    args.func(args)\n\n    return None\n</code></pre>"},{"location":"api/processing/#isoslam.processing.process","title":"<code>process(args)</code>","text":"<p>Process a set of files.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace | None</code> <p>Arguments function was invoked with.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars Dataframe of results.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def process(\n    args: arg.Namespace | None,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Process a set of files.\n\n    Parameters\n    ----------\n    args : arg.Namespace | None\n        Arguments function was invoked with.\n\n    Returns\n    -------\n    pl.DataFrame\n        Polars Dataframe of results.\n    \"\"\"\n    config = io.load_and_update_config(args)\n    logger.remove()\n    if \"log_level\" in vars(args) and vars(args)[\"log_level\"] is not None:\n        logging.setup(level=vars(args)[\"log_level\"])\n    else:\n        logging.setup(level=config[\"log_level\"])\n    validation.validate_config(config=config, schema=validation.DEFAULT_CONFIG_SCHEMA, config_type=\"configuration\")\n\n    # Load files...\n    vcffile = io.load_file(config[\"vcf_file\"])\n    utron_coords = isoslam.extract_transcripts(config[\"bed_file\"])\n    strand_dict, tx2gene = isoslam.extract_strand_transcript(config[\"gtf_file\"])\n    # Setup Polars dataframe\n    results = pl.DataFrame(schema=config[\"schema\"])\n    pairs_processed = 0\n    first_matched = 0\n    read_uid = 1\n    # Process the BAM file\n    for pair in isoslam.extract_segment_pairs(config[\"bam_file\"]):\n        # If there are an excessive number of pairs and/or matches per file we break out\n        if pairs_processed &gt;= config[\"upper_pairs_limit\"]:\n            break\n        if first_matched &gt; config[\"first_matched_limit\"]:\n            break\n        # Perform a number of checks as to whether we should proceed with processing\n        # 1. If we don't have a pair skip\n        #    @ns-rse : I tried unpacking and found there were indeed instances where the \"pairs\" could got upto 8 in\n        #    length?\n        if len(pair) != 2:\n            continue\n        # 2. If either reads of the pair are unmapped skip.\n        read1, read2 = pair\n        if read1.is_unmapped or read2.is_unmapped:\n            continue\n        # Use the intersection of sets, this skips if either read1 or read2 aren't assigned, - or +\n        pair_features = isoslam.extract_features_from_pair(pair)\n        if not {\"Assigned\", \"+\", \"-\"} &amp; {pair_features[\"read1\"][\"status\"], pair_features[\"read2\"][\"status\"]}:\n            continue\n        # Check that only Assigned, + and - status are included\n        status_list = [pair_features[\"read1\"][\"status\"], pair_features[\"read2\"][\"status\"]]\n        if all(status in [\"Assigned\", \"+\", \"-\"] for status in status_list):\n            # If strands are equal assign one to strand\n            if strand_dict[pair_features[\"read1\"][\"transcript\"]] == strand_dict[pair_features[\"read2\"][\"transcript\"]]:\n                strand = strand_dict[pair_features[\"read1\"][\"transcript\"]]\n            # ...if not we skip this pair\n            else:\n                continue\n        # If pairs are not both Assigned/+/- we set strand to the transcript from read1, otherwise its read2\n        else:\n            strand = (\n                strand_dict[pair_features[\"read1\"][\"transcript\"]]\n                if pair_features[\"read1\"][\"status\"] == \"Assigned\"\n                else strand_dict[pair_features[\"read2\"][\"transcript\"]]\n            )\n        # Set forward and reverse reads, if they don't match then we skip\n        if read1.is_reverse and not read2.is_reverse:\n            reverse_read = read1\n            forward_read = read2\n        elif read2.is_reverse and not read1.is_reverse:\n            reverse_read = read2\n            forward_read = read1\n        else:\n            # Not proper pair\n            continue\n\n        # We _haven't_ skipped any pairs so increment the pair counter\n        pairs_processed += 1\n        # Processing now begins...\n        # Extract utron for the gene\n        pair_features[\"read1\"][\"utron\"] = isoslam.extract_utron(\n            features=pair_features[\"read1\"], gene_transcript=tx2gene, coordinates=utron_coords\n        )\n        pair_features[\"read2\"][\"utron\"] = isoslam.extract_utron(\n            features=pair_features[\"read2\"], gene_transcript=tx2gene, coordinates=utron_coords\n        )\n        # Get blocks\n        block_starts1, block_ends1 = isoslam.zip_blocks(read1)\n        block_starts2, block_ends2 = isoslam.zip_blocks(read2)\n        blocks = {\n            \"read1\": {\"starts\": block_starts1, \"ends\": block_ends1},\n            \"read2\": {\"starts\": block_starts2, \"ends\": block_ends2},\n        }\n        # Retain within introns\n        read1_within_intron = isoslam.filter_within_introns(pair_features, blocks, read=\"read1\")\n        read2_within_intron = isoslam.filter_within_introns(pair_features, blocks, read=\"read2\")\n        # Retain spliced\n        read1_spliced_3UI = isoslam.filter_spliced_utrons(pair_features, blocks, read=\"read1\")\n        read2_spliced_3UI = isoslam.filter_spliced_utrons(pair_features, blocks, read=\"read2\")\n        # List of all dictionaries\n        retained_reads = [read1_within_intron, read2_within_intron, read1_spliced_3UI, read2_spliced_3UI]\n        # Check that there are some retained regions (all dictionaries would be empty if there are none retained and\n        # not {} evaluates to True, hence wrapping in all())\n        if all(not contents for contents in retained_reads):\n            continue\n        # We have got a match so increment counts\n        first_matched += 1\n\n        # Unique conversions within introns to be retained\n        assign_conversions_to_retained = isoslam.unique_conversions(read1_within_intron, read2_within_intron)\n        # Unique conversions within 3UI to be retained\n        assign_conversions_to_spliced = isoslam.unique_conversions(read1_spliced_3UI, read2_spliced_3UI)\n        ## If there are any events in both we want to remove them - this should be rare\n        assign_conversions_to_retained, assign_conversions_to_spliced = isoslam.remove_common_reads(\n            assign_conversions_to_retained, assign_conversions_to_spliced\n        )\n        # If we are mapped to a +ve stranded transcript, then count T&gt;C in the forward read and A&gt;G in the reverse read.\n        if strand == \"+\":\n            coverage_counts = isoslam.count_conversions_across_pairs(\n                forward_read=forward_read,\n                reverse_read=reverse_read,\n                vcf_file=vcffile,\n                forward_conversion=config[\"forward_reads\"],\n                reverse_conversion=config[\"reverse_reads\"],\n            )\n        elif strand == \"-\":\n            # If we are mapped to a -ve stranded transcript, count T&gt;C in the reverse read and A&gt;G in the forward read.\n            # NB - can either flip the reads or the conversion that are passed in, here we flip the read\n            coverage_counts = isoslam.count_conversions_across_pairs(\n                forward_read=reverse_read,\n                reverse_read=forward_read,\n                vcf_file=vcffile,\n                forward_conversion=config[\"forward_reads\"],\n                reverse_conversion=config[\"reverse_reads\"],\n            )\n        else:\n            # should not be possible - but just in case\n            pass\n\n        results = isoslam.append_data(\n            assigned_conversions=assign_conversions_to_retained,\n            coverage_counts=coverage_counts,  # pylint: disable=possibly-used-before-assignment\n            read_uid=read_uid,\n            assignment=\"Ret\",\n            results=results,\n            schema=config[\"schema\"],\n        )\n        results = isoslam.append_data(\n            assigned_conversions=assign_conversions_to_spliced,\n            coverage_counts=coverage_counts,  # pylint: disable=possibly-used-before-assignment\n            read_uid=read_uid,\n            assignment=\"Spl\",\n            results=results,\n            schema=config[\"schema\"],\n        )\n        read_uid += 1\n\n    results = results.sort(by=[\"read_uid\", \"transcript_id\", \"chr\", \"start\", \"end\"])\n    io.data_frame_to_file(data=results, output_dir=config[\"output_dir\"], outfile=config[\"output_file\"])\n\n    return results\n</code></pre>"},{"location":"api/processing/#isoslam.processing.summarise_counts","title":"<code>summarise_counts(args)</code>","text":"<p>Take a set of output files and summarise the number of conversions.</p> <p>Counts are made within file, chromosome, transcript, start, end, assignment and whether there is one or more conversion observed.</p> <p>Parameters:</p> Name Type Description Default <code>args</code> <code>Namespace | None</code> <p>Arguments function was invoked with.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Function does not return anything.</p> Source code in <code>isoslam/processing.py</code> <pre><code>def summarise_counts(args: arg.Namespace | None) -&gt; None:\n    \"\"\"\n    Take a set of output files and summarise the number of conversions.\n\n    Counts are made within file, chromosome, transcript, start, end, assignment and whether there is one or more\n    conversion observed.\n\n    Parameters\n    ----------\n    args : arg.Namespace | None\n        Arguments function was invoked with.\n\n    Returns\n    -------\n    None\n        Function does not return anything.\n    \"\"\"\n    # Load the configuration file (default or user) and update with supplied flags\n    config = io.load_and_update_config(args)\n    logger.remove()\n    if vars(args)[\"log_level\"] is not None:\n        logging.setup(level=vars(args)[\"log_level\"])\n    else:\n        logging.setup(level=config[\"log_level\"])\n    summary_counts_config = config[\"summary_counts\"]\n    output_config = summary_counts_config.pop(\"output\")\n    output_config[\"output_dir\"] = config[\"output_dir\"]\n    summary_counts = summary.summary_counts(**summary_counts_config)\n    summary_counts = summary_counts.sort(by=[\"Chr\", \"Transcript_id\", \"Start\"])\n    io.data_frame_to_file(summary_counts, **output_config)\n    logger.info(f\"Summary counts file written to : {output_config['output_dir']}/{output_config['outfile']}\")\n</code></pre>"},{"location":"api/summary/","title":"Summary","text":"<p>Functions for summarising output.</p>"},{"location":"api/summary/#isoslam.summary.Statistics","title":"<code>Statistics</code>  <code>dataclass</code>","text":"<p>Staistical summary of results.</p> Source code in <code>isoslam/summary.py</code> <pre><code>@dataclass()\nclass Statistics:  # pylint: disable=too-many-instance-attributes\n    \"\"\"Staistical summary of results.\"\"\"\n\n    # Initialised attributes\n    file_ext: str\n    directory: str | Path\n    groupby: list[str] | None\n    conversions_var: str | None\n    conversions_threshold: int\n    test_file: str | None\n    regex: str | None\n\n    # Generated atrtibutes\n    data: pl.DataFrame = field(init=False)\n    averages: pl.DataFrame = field(init=False)\n    baseline: pl.DataFrame = field(init=False)\n    normliased: pl.DataFrame = field(init=False)\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"After initialisation the files are loaded and prepared for analysis.\"\"\"\n        self.data = summary_counts(\n            file_ext=self._file_ext,\n            directory=self._directory,\n            regex=self._regex,\n            groupby=self._groupby,\n            conversions_var=self._conversions_var,\n            conversions_threshold=self._conversions_threshold,\n            test_file=self._test_file,\n        )\n        _df = aggregate_conversions(self.data, self.groupby, self._conversions_var)\n        _df = filter_no_conversions(_df, self.groupby, self._conversions_var, test=False)\n        _df = get_one_or_more_conversion(_df, self.groupby, self._conversions_var)\n        self.averages = percent_conversions_across_replicates(_df, self.groupby)\n        self.baseline = select_base_levels(self.averages)\n        self.normalised = merge_average_with_baseline(self.averages, self.baseline)\n        # Normalise mean conversion percent change by baseline\n        self.normalised = normalise(\n            self.normalised, to_normalise=\"conversion_percent\", baseline=\"baseline_percent\", normalised=\"normalised\"\n        )\n        # Derive weights within transcript/isoform based on total counts\n        self.normalised = derive_weight_within_isoform(self.normalised, groupby=None, total=\"conversion_total\")\n\n    @property\n    def file_ext(self) -&gt; str:\n        \"\"\"\n        Getter method for ``file_ext``.\n\n        Returns\n        -------\n        str\n            File extension that is loaded.\n        \"\"\"\n        return self._file_ext\n\n    @file_ext.setter\n    def file_ext(self, value: str) -&gt; None:\n        \"\"\"\n        Setter for the file extension.\n\n        Parameters\n        ----------\n        value : str\n            File extension to load data.\n        \"\"\"\n        self._file_ext = value\n\n    @property\n    def directory(self) -&gt; str:\n        \"\"\"\n        Getter method for ``directory``.\n\n        Returns\n        -------\n        str\n            Directory from which output files are loaded.\n        \"\"\"\n        return self._directory\n\n    @directory.setter\n    def directory(self, value: str) -&gt; None:\n        \"\"\"\n        Setter for the file extension.\n\n        Parameters\n        ----------\n        value : str\n            Directory from which files are loaded.\n        \"\"\"\n        self._directory = value\n\n    @property\n    def regex(self) -&gt; str:\n        \"\"\"\n        Getter method for ``regex``.\n\n        Returns\n        -------\n        str\n            Regex for extracting day/hour/replication from filename.\n        \"\"\"\n        return self._regex\n\n    @regex.setter\n    def regex(self, value: str) -&gt; None:\n        \"\"\"\n        Setter for regex used to extract day/hour/replication from filename..\n\n        Parameters\n        ----------\n        value : str\n            Regex to use for extracting day/hour/replication from filename.\n        \"\"\"\n        self._regex = value\n\n    @property\n    def groupby(self) -&gt; list[str]:\n        \"\"\"\n        Getter method for ``groupby``.\n\n        Returns\n        -------\n        list[str]\n            List of variables to groupby.\n        \"\"\"\n        return self._groupby\n\n    @groupby.setter\n    def groupby(self, value: list[str]) -&gt; None:\n        \"\"\"\n        Setter for the ``groupby`` property.\n\n        Parameters\n        ----------\n        value : list[str]\n            Variables to group data by.\n        \"\"\"\n        self._groupby = value\n\n    @property\n    def conversions_var(self) -&gt; str:\n        \"\"\"\n        Getter method for ``conversions_var``.\n\n        Returns\n        -------\n        str\n            The conversions variable.\n        \"\"\"\n        return self._conversions_var\n\n    @conversions_var.setter\n    def conversions_var(self, value: str) -&gt; None:\n        \"\"\"\n        Setter for the ``conversions_var`` property.\n\n        Parameters\n        ----------\n        value : list[str]\n            Variables to group data by.\n        \"\"\"\n        self._conversions_var = value\n\n    @property\n    def conversions_threshold(self) -&gt; int:\n        \"\"\"\n        Getter method for ``conversions_threshold``.\n\n        Returns\n        -------\n        int\n            The conversion threshold for counting.\n        \"\"\"\n        return self._conversions_threshold\n\n    @conversions_threshold.setter\n    def conversions_threshold(self, value: int) -&gt; None:\n        \"\"\"\n        Setter for the ``conversions_threshold``.\n\n        Parameters\n        ----------\n        value : int\n            Threshold value for counting conversions.\n        \"\"\"\n        self._conversions_threshold = value\n\n    @property\n    def test_file(self) -&gt; str:\n        \"\"\"\n        Getter method for ``test_file``.\n\n        Returns\n        -------\n        str\n            String pattern of test filename for excluding test file data.\n        \"\"\"\n        return self._test_file\n\n    @test_file.setter\n    def test_file(self, value: str) -&gt; None:\n        \"\"\"\n        Setter for the ``test_file`` value.\n\n        Parameters\n        ----------\n        value : str\n            Value of ``test_file``.\n        \"\"\"\n        self._test_file = value\n\n    @property\n    def shape(self) -&gt; tuple[int, int]:\n        \"\"\"\n        Getter for the shape of the dataframe.\n\n        Returns\n        -------\n        tuple[int, int]\n            Shape of the Polars dataframe.\n        \"\"\"\n        return self.data.shape  # type: ignore[no-any-return]\n\n    @property\n    def unique(self) -&gt; int:\n        \"\"\"\n        Getter for the number of unique files loaded.\n\n        Returns\n        -------\n        int\n            Number of unique rows.\n        \"\"\"\n        return self.unique_rows()\n\n    def unique_rows(self, columns: list[str] | None = None) -&gt; int:\n        \"\"\"\n        Identify unique rows in the data for a given set of columns.\n\n        Parameters\n        ----------\n        columns : list[str]\n            Columns to use for identifying unique observations. If ``None`` defaults to ``filename`` which returns the\n            number of unique files loaded from the ``directory`` with ``file_ext``.\n\n        Returns\n        -------\n        int\n            Number of unique rows for the given set of variables.\n        \"\"\"\n        columns = [\"filename\"] if columns is None else columns\n        return len(self.data.unique(subset=columns))\n</code></pre>"},{"location":"api/summary/#isoslam.summary.Statistics.conversions_threshold","title":"<code>conversions_threshold</code>  <code>property</code> <code>writable</code>","text":"<p>Getter method for <code>conversions_threshold</code>.</p> <p>Returns:</p> Type Description <code>int</code> <p>The conversion threshold for counting.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.conversions_var","title":"<code>conversions_var</code>  <code>property</code> <code>writable</code>","text":"<p>Getter method for <code>conversions_var</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>The conversions variable.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.directory","title":"<code>directory</code>  <code>property</code> <code>writable</code>","text":"<p>Getter method for <code>directory</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>Directory from which output files are loaded.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.file_ext","title":"<code>file_ext</code>  <code>property</code> <code>writable</code>","text":"<p>Getter method for <code>file_ext</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>File extension that is loaded.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.groupby","title":"<code>groupby</code>  <code>property</code> <code>writable</code>","text":"<p>Getter method for <code>groupby</code>.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>List of variables to groupby.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.regex","title":"<code>regex</code>  <code>property</code> <code>writable</code>","text":"<p>Getter method for <code>regex</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>Regex for extracting day/hour/replication from filename.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.shape","title":"<code>shape</code>  <code>property</code>","text":"<p>Getter for the shape of the dataframe.</p> <p>Returns:</p> Type Description <code>tuple[int, int]</code> <p>Shape of the Polars dataframe.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.test_file","title":"<code>test_file</code>  <code>property</code> <code>writable</code>","text":"<p>Getter method for <code>test_file</code>.</p> <p>Returns:</p> Type Description <code>str</code> <p>String pattern of test filename for excluding test file data.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.unique","title":"<code>unique</code>  <code>property</code>","text":"<p>Getter for the number of unique files loaded.</p> <p>Returns:</p> Type Description <code>int</code> <p>Number of unique rows.</p>"},{"location":"api/summary/#isoslam.summary.Statistics.__post_init__","title":"<code>__post_init__()</code>","text":"<p>After initialisation the files are loaded and prepared for analysis.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"After initialisation the files are loaded and prepared for analysis.\"\"\"\n    self.data = summary_counts(\n        file_ext=self._file_ext,\n        directory=self._directory,\n        regex=self._regex,\n        groupby=self._groupby,\n        conversions_var=self._conversions_var,\n        conversions_threshold=self._conversions_threshold,\n        test_file=self._test_file,\n    )\n    _df = aggregate_conversions(self.data, self.groupby, self._conversions_var)\n    _df = filter_no_conversions(_df, self.groupby, self._conversions_var, test=False)\n    _df = get_one_or_more_conversion(_df, self.groupby, self._conversions_var)\n    self.averages = percent_conversions_across_replicates(_df, self.groupby)\n    self.baseline = select_base_levels(self.averages)\n    self.normalised = merge_average_with_baseline(self.averages, self.baseline)\n    # Normalise mean conversion percent change by baseline\n    self.normalised = normalise(\n        self.normalised, to_normalise=\"conversion_percent\", baseline=\"baseline_percent\", normalised=\"normalised\"\n    )\n    # Derive weights within transcript/isoform based on total counts\n    self.normalised = derive_weight_within_isoform(self.normalised, groupby=None, total=\"conversion_total\")\n</code></pre>"},{"location":"api/summary/#isoslam.summary.Statistics.unique_rows","title":"<code>unique_rows(columns=None)</code>","text":"<p>Identify unique rows in the data for a given set of columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list[str]</code> <p>Columns to use for identifying unique observations. If <code>None</code> defaults to <code>filename</code> which returns the number of unique files loaded from the <code>directory</code> with <code>file_ext</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>Number of unique rows for the given set of variables.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def unique_rows(self, columns: list[str] | None = None) -&gt; int:\n    \"\"\"\n    Identify unique rows in the data for a given set of columns.\n\n    Parameters\n    ----------\n    columns : list[str]\n        Columns to use for identifying unique observations. If ``None`` defaults to ``filename`` which returns the\n        number of unique files loaded from the ``directory`` with ``file_ext``.\n\n    Returns\n    -------\n    int\n        Number of unique rows for the given set of variables.\n    \"\"\"\n    columns = [\"filename\"] if columns is None else columns\n    return len(self.data.unique(subset=columns))\n</code></pre>"},{"location":"api/summary/#isoslam.summary.aggregate_conversions","title":"<code>aggregate_conversions(df, groupby='replicate', converted='one_or_more_conversion')</code>","text":"<p>Subset data where there have not been one or more conversions.</p> <p>NB : This needs a better description, I've failed to capture the essence of what is being done here.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Summary dataframe aggregated to give counts of one or more conversion.</p> required <code>groupby</code> <code>str | list[str]</code> <p>Variables to group the data by.</p> <code>'replicate'</code> <code>converted</code> <code>str</code> <p>Variable that contains whether conversions have been observed or not.</p> <code>'one_or_more_conversion'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Aggregated dataframe.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def aggregate_conversions(\n    df: pl.DataFrame, groupby: str | list[str] | None = \"replicate\", converted: str | None = \"one_or_more_conversion\"\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Subset data where there have not been one or more conversions.\n\n    NB : This needs a better description, I've failed to capture the essence of what is being done here.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Summary dataframe aggregated to give counts of one or more conversion.\n    groupby : str | list[str], optional\n        Variables to group the data by.\n    converted : str\n        Variable that contains whether conversions have been observed or not.\n\n    Returns\n    -------\n    pl.DataFrame\n        Aggregated dataframe.\n    \"\"\"\n    # if groupby is None:\n    #     groupby = GROUPBY_DAY_HR_REP\n    groupby = get_groupby(groupby)\n    # Its important to ensure that the data is not just groupby but that within that it is then sorted by the converted\n    # variable. This _should_ be the case if being passed data from summary_count() but to make sure we explicitly sort\n    # the data so that pl.first(converted) will _always_ get 'False' first if pl.len() == 2\n    # Making sure this was correct caused @ns-rse quite a few headaches as initially it appeared that the sorting was\n    # retained from earlier steps but that True &lt; False!\n    sortby = groupby.copy()\n    sortby.append(converted)  # type: ignore[arg-type]\n    df = df.sort(sortby)\n    q = df.lazy().group_by(groupby, maintain_order=True).agg(pl.len(), pl.first(converted))\n    non_captured = q.collect()\n    return non_captured.sort(groupby)\n</code></pre>"},{"location":"api/summary/#isoslam.summary.append_files","title":"<code>append_files(file_ext='.tsv', directory=None)</code>","text":"<p>Append a set of files into a Polars DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>file_ext</code> <code>str</code> <p>File extension to search for results to summarise.</p> <code>'.tsv'</code> <code>directory</code> <code>str | Path | None</code> <p>Path on which to search for files with <code>file_ext</code>, if <code>None</code> then current working directory is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrames of each file found.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def append_files(file_ext: str = \".tsv\", directory: str | Path | None = None) -&gt; pl.DataFrame:\n    \"\"\"\n    Append a set of files into a Polars DataFrame.\n\n    Parameters\n    ----------\n    file_ext : str\n        File extension to search for results to summarise.\n    directory : str | Path | None\n        Path on which to search for files with ``file_ext``, if ``None`` then current working directory is used.\n\n    Returns\n    -------\n    pl.DataFrame\n        A Polars DataFrames of each file found.\n    \"\"\"\n    _data = io.load_output_files(file_ext, directory)\n    all_data = [data.with_columns(filename=pl.lit(key)) for key, data in _data.items()]\n    return pl.concat(all_data)\n</code></pre>"},{"location":"api/summary/#isoslam.summary.derive_weight_within_isoform","title":"<code>derive_weight_within_isoform(df, groupby='assignment', total='conversion_total')</code>","text":"<p>Calculate weighting used for normalised percentages within each isoform across all time points.</p> <p>Where the number of total reads (across replications) is higher then we are more confident in the percentage of conversions observed and so we weight the percentages at each time point by the proportion of total counts which were calculated previously when deriving the percentage of conversions across replicates (with the <code>_percent_conversions_across_replicates()</code> function).</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe for which weights are to be derived.</p> required <code>groupby</code> <code>list[str]</code> <p>Grouping for summation of total counts, defaults to <code>[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\"]</code>.</p> <code>'assignment'</code> <code>total</code> <code>str</code> <p>Variable that nolds the total number of conversions (across all replicates), default is <code>conversion_total</code> and shouldn't need changing.</p> <code>'conversion_total'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with two new columns, the sum of total conversions across replicates and time points (<code>conversion_total_all_time_points</code>) and the weight of conversions at each time point (<code>conversion_weight</code>).</p> Source code in <code>isoslam/summary.py</code> <pre><code>def derive_weight_within_isoform(\n    df: pl.DataFrame,\n    groupby: str | list[str] | None = \"assignment\",\n    total: str = \"conversion_total\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Calculate weighting used for normalised percentages within each isoform across all time points.\n\n    Where the number of total reads (across replications) is higher then we are more confident in the percentage of\n    conversions observed and so we weight the percentages at each time point by the proportion of total counts which\n    were calculated previously when deriving the percentage of conversions across replicates (with the\n    ``_percent_conversions_across_replicates()`` function).\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Dataframe for which weights are to be derived.\n    groupby : list[str]\n        Grouping for summation of total counts, defaults to ``[\"Transcript_id\", \"Strand\", \"Start\", \"End\",\n        \"Assignment\"]``.\n    total : str\n        Variable that nolds the total number of conversions (across all replicates), default is ``conversion_total`` and\n        shouldn't need changing.\n\n    Returns\n    -------\n    pl.DataFrame\n        DataFrame with two new columns, the sum of total conversions across replicates and time points\n        (``conversion_total_all_time_points``) and the weight of conversions at each time point (``conversion_weight``).\n    \"\"\"\n    groupby = get_groupby(groupby)\n    counts_across_isoform = df.group_by(groupby).agg([pl.col(total).sum().alias(\"conversion_total_all_time_points\")])\n    df = df.join(counts_across_isoform, on=groupby, how=\"inner\")\n    return df.with_columns((pl.col(total) / pl.col(\"conversion_total_all_time_points\")).alias(\"conversion_weight\"))\n</code></pre>"},{"location":"api/summary/#isoslam.summary.extract_day_hour_and_replicate","title":"<code>extract_day_hour_and_replicate(df, column='filename', regex='^d(\\\\w+)_(\\\\w+)hr(\\\\w+)_')</code>","text":"<p>Extract the hour and replicate from the filename stored in a dataframes column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars DataFrame.</p> required <code>column</code> <code>str</code> <p>The name of the column that holds the filename, default <code>filename</code>.</p> <code>'filename'</code> <code>regex</code> <code>str</code> <p>Regular expression pattern to extract the hour and replicate from, default <code>r\"^d(\\w+)_(\\w+)hr(\\w+)_\"</code>.</p> <code>'^d(\\\\w+)_(\\\\w+)hr(\\\\w+)_'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame augmented with the hour and replicate extracted from the filename.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def extract_day_hour_and_replicate(\n    df: pl.DataFrame, column: str = \"filename\", regex: str = r\"^d(\\w+)_(\\w+)hr(\\w+)_\"\n) -&gt; pl.DataFrame:\n    r\"\"\"\n    Extract the hour and replicate from the filename stored in a dataframes column.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Polars DataFrame.\n    column : str\n        The name of the column that holds the filename, default ``filename``.\n    regex : str\n        Regular expression pattern to extract the hour and replicate from, default ``r\"^d(\\w+)_(\\w+)hr(\\w+)_\"``.\n\n    Returns\n    -------\n    pl.DataFrame\n        Polars DataFrame augmented with the hour and replicate extracted from the filename.\n    \"\"\"\n    return df.with_columns(\n        (pl.col(column).str.extract(regex, group_index=1).str.to_integer(strict=False).alias(\"day\")),\n        (pl.col(column).str.extract(regex, group_index=2).str.to_integer(strict=False).alias(\"hour\")),\n        (pl.col(column).str.extract(regex, group_index=3).str.to_integer(strict=False).alias(\"replicate\")),\n    )\n</code></pre>"},{"location":"api/summary/#isoslam.summary.filter_no_conversions","title":"<code>filter_no_conversions(df, groupby='replicate', converted='one_or_more_conversion', test=False)</code>","text":"<p>Filter dataframe for instances where only no conversions have been observed.</p> <p>NB : This needs a better description, I've failed to capture the essence of what is being done here.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Summary dataframe aggregated to give counts of one or more conversion.</p> required <code>groupby</code> <code>str | list[str]</code> <p>Variables to group the data by.</p> <code>'replicate'</code> <code>converted</code> <code>str</code> <p>Variable that contains whether conversions have been observed or not.</p> <code>'one_or_more_conversion'</code> <code>test</code> <code>bool</code> <p>Whether the function is being tested or not. This will prevent a call to <code>_aggregate_conversions()</code> to aggregate the input and simply filter the data.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Aggregated dataframe.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def filter_no_conversions(\n    df: pl.DataFrame,\n    groupby: str | list[str] | None = \"replicate\",\n    converted: str | None = \"one_or_more_conversion\",\n    test: bool = False,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Filter dataframe for instances where only no conversions have been observed.\n\n    NB : This needs a better description, I've failed to capture the essence of what is being done here.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Summary dataframe aggregated to give counts of one or more conversion.\n    groupby : str | list[str], optional\n        Variables to group the data by.\n    converted : str\n        Variable that contains whether conversions have been observed or not.\n    test : bool\n        Whether the function is being tested or not. This will prevent a call to ``_aggregate_conversions()`` to\n        aggregate the input and simply filter the data.\n\n    Returns\n    -------\n    pl.DataFrame\n        Aggregated dataframe.\n    \"\"\"\n    if not test:\n        df = aggregate_conversions(df, groupby, converted)\n    # pylint: disable=singleton-comparison\n    return df.filter((pl.col(\"len\") == 1) &amp; (pl.col(converted) == False)).drop(\"len\")  # noqa: E712\n</code></pre>"},{"location":"api/summary/#isoslam.summary.find_read_pairs","title":"<code>find_read_pairs(df, index_columns=None, assignment='Assignment')</code>","text":"<p>Find instances where there are conversions for both <code>Return</code> and <code>Splice</code> assignments.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars DataFrame.</p> required <code>index_columns</code> <code>list[str]</code> <p>List of index columns to select from the dataframe. Should include the unique identifiers, typically (<code>Transcript_id</code>, <code>Strand</code>, <code>Start</code> and <code>End</code> which are the defaults) but does not need to include the ''assignment'' column.</p> <code>None</code> <code>assignment</code> <code>str</code> <p>Column the defines assignment of events to <code>Ret</code> (<code>Return</code>) or <code>Spl</code> (<code>Splice</code>).</p> <code>'Assignment'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame of the <code>index_columns</code> where both a <code>Ret</code> and <code>Spl</code> event have been observed.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def find_read_pairs(\n    df: pl.DataFrame, index_columns: list[str] | None = None, assignment: str | None = \"Assignment\"\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Find instances where there are conversions for both ``Return`` and ``Splice`` assignments.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Polars DataFrame.\n    index_columns : list[str]\n        List of index columns to select from the dataframe. Should include the unique identifiers, typically\n        (``Transcript_id``, ``Strand``, ``Start`` and ``End`` which are the defaults) but does not need to include the\n        ''assignment'' column.\n    assignment : str\n        Column the defines assignment of events to ``Ret`` (``Return``) or ``Spl`` (``Splice``).\n\n    Returns\n    -------\n    pl.DataFrame\n        Polars DataFrame of the ``index_columns`` where both a ``Ret`` and ``Spl`` event have been observed.\n    \"\"\"\n    if assignment is None:\n        assignment = \"Assignment\"\n    if index_columns is None:\n        index_columns = get_groupby(groupby=\"base\")\n    index_columns.append(assignment)\n    df_return = df.select(index_columns).filter(pl.col(assignment) == \"Ret\")\n    df_splice = df.select(index_columns).filter(pl.col(assignment) == \"Spl\")\n    index_columns.remove(assignment)\n    # We use sorted(index_columns, reverse=True) so that the order is consistent for testing, the reverse option roughly\n    # gets things close to the expected order of columns used in the data.\n    return (\n        df_return.join(df_splice, on=index_columns, how=\"inner\")\n        .select(sorted(index_columns, reverse=True))\n        .unique()\n        .sort(by=sorted(index_columns, reverse=True))\n    )\n</code></pre>"},{"location":"api/summary/#isoslam.summary.get_groupby","title":"<code>get_groupby(groupby)</code>","text":"<p>Get grouping variables.</p> <p>.. csv-table:: Possible groupings    :header: 'Value','Grouping'</p> <p>'base',','[\"Transcript_id\", \"Strand\", \"Start\", \"End\"]' 'assignment',','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\"]' 'filename','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"filename\"]' 'time','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"day\", \"hour\"]' 'replicate','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"day\", \"hour\", \"replicate\"]' 'None','Value of <code>groupby</code>.'</p> <p>This is typically <code>[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\"]</code> when <code>groupby</code> is <code>None</code> but returns <code>groupby</code> otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>groupby</code> <code>list[str] | None</code> <p>Variables to groupby.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>List of variables to group data by.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If invalid value string is passed.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def get_groupby(groupby: str | list[str] | None) -&gt; list[str]:  # pylint: disable=too-many-return-statements\n    \"\"\"\n    Get grouping variables.\n\n    .. csv-table:: Possible groupings\n       :header: 'Value','Grouping'\n\n    'base',','[\"Transcript_id\", \"Strand\", \"Start\", \"End\"]'\n    'assignment',','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\"]'\n    'filename','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"filename\"]'\n    'time','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"day\", \"hour\"]'\n    'replicate','[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"day\", \"hour\", \"replicate\"]'\n    'None','Value of ``groupby``.'\n\n    This is typically ``[\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\"]`` when ``groupby`` is ``None`` but\n    returns ``groupby`` otherwise.\n\n    Parameters\n    ----------\n    groupby : list[str] | None\n        Variables to groupby.\n\n    Returns\n    -------\n    list[str]\n        List of variables to group data by.\n\n    Raises\n    ------\n    ValueError\n        If invalid value string is passed.\n    \"\"\"\n    if groupby is not None and not isinstance(groupby, list):\n        if groupby not in {\"assignment\", \"base\", \"filename\", \"time\", \"replicate\"}:\n            raise ValueError(\"You must specify a valid grouping or pass a list to groupby.\")\n        if groupby == \"base\":\n            return [\"Transcript_id\", \"Strand\", \"Start\", \"End\"]\n        if groupby == \"assignment\":\n            return [\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\"]\n        if groupby == \"filename\":\n            return [\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"filename\"]\n        if groupby == \"time\":\n            return [\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"day\", \"hour\"]\n        if groupby == \"replicate\":\n            return [\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\", \"day\", \"hour\", \"replicate\"]\n    if groupby is None:\n        return [\"Transcript_id\", \"Strand\", \"Start\", \"End\", \"Assignment\"]\n    return groupby  # type: ignore[return-value]\n</code></pre>"},{"location":"api/summary/#isoslam.summary.get_one_or_more_conversion","title":"<code>get_one_or_more_conversion(df, groupby='replicate', converted='one_or_more_conversion')</code>","text":"<p>Extract instances where one or more conversion has occurred.</p> <p>There are some cases where this isn't the case and for a given subset the <code>converted</code> variable, which indicates if one or more conversion has occurred will only be <code>False</code> For such instances dummy entries are created based on the <code>groupby</code> variable and appended to the subset of instances where this one or more conversions have been observed.</p> <p>This function takes as input the results of <code>summary_count()</code> it will not work with intermediate files.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Summary dataframe aggregated to give counts of one or more conversion.</p> required <code>groupby</code> <code>str | list[str]</code> <p>Variables to group the data by.</p> <code>'replicate'</code> <code>converted</code> <code>str</code> <p>Variable that contains whether conversions have been observed or not.</p> <code>'one_or_more_conversion'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Aggregated dataframe.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def get_one_or_more_conversion(\n    df: pl.DataFrame, groupby: str | list[str] | None = \"replicate\", converted: str = \"one_or_more_conversion\"\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Extract instances where one or more conversion has occurred.\n\n    There are some cases where this isn't the case and for a given subset the ``converted`` variable, which indicates if\n    one or more conversion has occurred will only be ``False`` For such instances dummy entries are created based on the\n    ``groupby`` variable and appended to the subset of instances where this one or more conversions have been observed.\n\n    This function takes as input the results of ``summary_count()`` it will not work with intermediate files.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Summary dataframe aggregated to give counts of one or more conversion.\n    groupby : str | list[str], optional\n        Variables to group the data by.\n    converted : str\n        Variable that contains whether conversions have been observed or not.\n\n    Returns\n    -------\n    pl.DataFrame\n        Aggregated dataframe.\n    \"\"\"\n    groupby = get_groupby(groupby)\n    no_conversions = filter_no_conversions(df, groupby, converted)\n    groupby.append(converted)\n    no_conversions = df.join(no_conversions, on=groupby, how=\"inner\", maintain_order=\"left\")\n    no_conversions = no_conversions.with_columns(\n        conversion_count=0,\n        one_or_more_conversion=True,\n        conversion_percent=0.0,\n    )\n    no_conversions = no_conversions.with_columns(pl.col(\"conversion_count\").cast(pl.UInt32))\n    df = pl.concat([df, no_conversions.select(df.columns)])\n    keep = groupby + [\"conversion_count\", \"conversion_total\", \"conversion_percent\"]\n    # pylint: disable=singleton-comparison\n    return df.filter(pl.col(converted) == True).select(keep).sort(groupby)  # noqa: E712\n</code></pre>"},{"location":"api/summary/#isoslam.summary.merge_average_with_baseline","title":"<code>merge_average_with_baseline(df_average, df_baseline, join_on='assignment', zero_baseline_remove=True)</code>","text":"<p>Merge a data frame with the baseline measurements.</p> <p>Typically for this workflow this involves merging the average data frame (across replicates at each of the transcripts/start/end/strand/assignments) with the average at the baseline to allow normalising the data.</p> <p>Parameters:</p> Name Type Description Default <code>df_average</code> <code>DataFrame</code> <p>Polars Dataframe of averaged data.</p> required <code>df_baseline</code> <code>DataFrame</code> <p>Polars Dataframe of averaged baseline data.</p> required <code>join_on</code> <code>list[str] | None</code> <p>Variables to join the data frames on, if <code>None</code> (default) it is set to <code>Transcript_id, Start, End, Assignment, Strand</code>.</p> <code>'assignment'</code> <code>zero_baseline_remove</code> <code>bool</code> <p>Remove instances where the baseline percentage conversion is zero.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Averaged and baseline data frame merged on <code>join_on</code>.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def merge_average_with_baseline(\n    df_average: pl.DataFrame,\n    df_baseline: pl.DataFrame,\n    join_on: str | list[str] | None = \"assignment\",\n    zero_baseline_remove: bool = True,\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Merge a data frame with the baseline measurements.\n\n    Typically for this workflow this involves merging the average data frame (across replicates at each of the\n    transcripts/start/end/strand/assignments) with the average at the baseline to allow normalising the data.\n\n    Parameters\n    ----------\n    df_average : pl.DataFrame\n        Polars Dataframe of averaged data.\n    df_baseline : pl.DataFrame\n        Polars Dataframe of averaged baseline data.\n    join_on : list[str] | None\n        Variables to join the data frames on, if ``None`` (default) it is set to ``Transcript_id, Start, End,\n        Assignment, Strand``.\n    zero_baseline_remove : bool\n        Remove instances where the baseline percentage conversion is zero.\n\n    Returns\n    -------\n    pl.DataFrame\n        Averaged and baseline data frame merged on ``join_on``.\n    \"\"\"\n    join_on = get_groupby(groupby=join_on)\n    if zero_baseline_remove:\n        df_baseline = df_baseline.filter(pl.col(\"baseline_percent\") != 0.0)\n    return df_average.join(df_baseline, on=join_on)\n</code></pre>"},{"location":"api/summary/#isoslam.summary.normalise","title":"<code>normalise(df, to_normalise='conversion_percent', baseline='baseline_percent', normalised='normalised_percent')</code>","text":"<p>Normalise variables based on the baseline measurement.</p> <p>Assumes that you have merged the averaged dataset with the averaged baseline variables so that the parameter of interest as its related baseline measurement paired with it. Values are normalised by dividing by the baseline value such that baseline will always start at <code>1</code> and subsequent values (time-points) are relative to this and show increases or decreases. Typically these will be relative changes in the (averaged) percentage of conversions.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe from <code>_merge_average_with_baseline</code>.</p> required <code>to_normalise</code> <code>str</code> <p>Variable to be normalised, default is <code>conversion_percent</code>.</p> <code>'conversion_percent'</code> <code>baseline</code> <code>str</code> <p>Variable to use for normalising, default is <code>baseline_percent</code>.</p> <code>'baseline_percent'</code> <code>normalised</code> <code>str</code> <p>Variable name for normalised value, default is <code>normalised_percent</code>.</p> <code>'normalised_percent'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars dataframe with normalised values.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def normalise(\n    df: pl.DataFrame,\n    to_normalise: str = \"conversion_percent\",\n    baseline: str = \"baseline_percent\",\n    normalised: str = \"normalised_percent\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Normalise variables based on the baseline measurement.\n\n    Assumes that you have merged the averaged dataset with the averaged baseline variables so that the parameter of\n    interest as its related baseline measurement paired with it. Values are normalised by dividing by the baseline value\n    such that baseline will always start at ``1`` and subsequent values (time-points) are relative to this and show\n    increases or decreases. Typically these will be relative changes in the (averaged) percentage of conversions.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Dataframe from ``_merge_average_with_baseline``.\n    to_normalise : str\n        Variable to be normalised, default is ``conversion_percent``.\n    baseline : str\n        Variable to use for normalising, default is ``baseline_percent``.\n    normalised : str\n        Variable name for normalised value, default is ``normalised_percent``.\n\n    Returns\n    -------\n    pl.DataFrame\n        Polars dataframe with normalised values.\n    \"\"\"\n    return df.with_columns([(pl.col(to_normalise) / pl.col(baseline)).alias(normalised)])\n</code></pre>"},{"location":"api/summary/#isoslam.summary.percent_conversions_across_replicates","title":"<code>percent_conversions_across_replicates(df, groupby='time', count='conversion_count', total='conversion_total')</code>","text":"<p>Percentage of conversions across replicates for each time point.</p> <p>The raw counts and total conversions for each replicate are available. These are summed and the percentage of conversions across replicates calculated. This is mathematically the same as taking the weighted mean of the percentage of conversions within each replicate.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars Dataframe of conversions.</p> required <code>groupby</code> <code>str | list[str]</code> <p>Variables to <code>group_by</code> the data, default is <code>transcript_id, start, end, assignment, day, hour</code>.</p> <code>'time'</code> <code>count</code> <code>str</code> <p>Variable/column name holding the counts, default is <code>conversion_count</code>.</p> <code>'conversion_count'</code> <code>total</code> <code>str</code> <p>Variable/column name holding the total number of conversions, default is <code>conversion_total</code>.</p> <code>'conversion_total'</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Weighted mean of the percentage of conversions (weighted by total conversions) across replicates for the given transcript/assignment/strand/day/hour (as specified by <code>groupby</code>).</p> Source code in <code>isoslam/summary.py</code> <pre><code>def percent_conversions_across_replicates(\n    df: pl.DataFrame,\n    groupby: str | list[str] | None = \"time\",\n    count: str = \"conversion_count\",\n    total: str = \"conversion_total\",\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Percentage of conversions across replicates for each time point.\n\n    The raw counts and total conversions for each replicate are available. These are summed and the percentage of\n    conversions across replicates calculated. This is mathematically the same as taking the weighted mean of the\n    percentage of conversions within each replicate.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Polars Dataframe of conversions.\n    groupby : str | list[str], optional\n        Variables to ``group_by`` the data, default is ``transcript_id, start, end, assignment, day, hour``.\n    count : str\n        Variable/column name holding the counts, default is ``conversion_count``.\n    total : str\n        Variable/column name holding the total number of conversions, default is ``conversion_total``.\n\n    Returns\n    -------\n    pl.DataFrame\n        Weighted mean of the percentage of conversions (weighted by total conversions) across replicates for the given\n        transcript/assignment/strand/day/hour (as specified by ``groupby``).\n    \"\"\"\n    groupby = get_groupby(groupby)\n    _keep = groupby + [count, total]\n    return (\n        df.select(_keep)\n        .group_by(groupby, maintain_order=True)\n        .agg([pl.col(count).sum(), pl.col(total).sum()])\n        .with_columns(((pl.col(count) / pl.col(total)) * 100).alias(\"conversion_percent\"))\n    )\n</code></pre>"},{"location":"api/summary/#isoslam.summary.remove_zero_baseline","title":"<code>remove_zero_baseline(df, groupby='base', percent_col=None)</code>","text":"<p>Remove data where the percentage change at baseline is zero.</p> <p>Removes all observations for a transcript/strand/start/end/assignment where the percentage change at baseline is zero. Such instances need removing because the data is normalised by the baseline measurement and division by zero leads to <code>NaN</code>/<code>Inf</code> data points which can not analysed in any meaningful way.</p> <p>Typically this should be run on the data after averaging across replicates since the percentage change is calculated across all replicates and any observation with zero percentage changes could still contribute to the total number of events. There is however nothing preventing the function from being used on data prior to averaging but that would be atypical usage.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars DataFrame with percentage changes at each time point for transcript/strand/start/end/assignment.</p> required <code>groupby</code> <code>str | list[str]</code> <p>Grouping of variables to look within for baseline of zero percent change. Default is <code>base</code> which groups by transcript_id/strand/start/end/assignment.</p> <code>'base'</code> <code>percent_col</code> <code>str</code> <p>Column name that holds the percentage, defaults to 'conversion_percent' if not specified.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Polars DataFrame with groups where the percent change at baseline is zero removed.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def remove_zero_baseline(\n    df: pl.DataFrame, groupby: str | list[str] | None = \"base\", percent_col: str | None = None\n) -&gt; pl.DataFrame:\n    \"\"\"\n    Remove data where the percentage change at baseline is zero.\n\n    Removes all observations for a transcript/strand/start/end/assignment where the percentage change at baseline is\n    zero. Such instances need removing because the data is normalised by the baseline measurement and division by zero\n    leads to ``NaN``/``Inf`` data points which can not analysed in any meaningful way.\n\n    Typically this should be run on the data _after_ averaging across replicates since the percentage change is\n    calculated across all replicates and any observation with zero percentage changes could still contribute to the\n    total number of events. There is however nothing preventing the function from being used on data prior to averaging\n    but that would be atypical usage.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Polars DataFrame with percentage changes at each time point for transcript/strand/start/end/assignment.\n    groupby : str | list[str]\n        Grouping of variables to look within for baseline of zero percent change. Default is ``base`` which groups by\n        transcript_id/strand/start/end/assignment.\n    percent_col : str\n        Column name that holds the percentage, defaults to 'conversion_percent' if not specified.\n\n    Returns\n    -------\n    pl.DataFrame\n        Polars DataFrame with groups where the percent change at baseline is zero removed.\n    \"\"\"\n    groupby = get_groupby(groupby)\n    percent_col = \"conversion_percent\" if percent_col is None else percent_col\n    df_zero_baseline = df.filter(pl.col(percent_col) == 0.0)\n    # Use an \"anti\" join which returns rows from the left (df) which do not have a match on the right (df_zero_basleine)\n    return df.join(df_zero_baseline, on=groupby, how=\"anti\")\n</code></pre>"},{"location":"api/summary/#isoslam.summary.select_base_levels","title":"<code>select_base_levels(df, base_day=0, base_hour=0)</code>","text":"<p>Select the base level reference across all data.</p> <p>This allows selecting the base level of totals and percents which are used for normalising values. Will drop the column <code>replicate</code> from the data frame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Polars Dataframe of conversions.</p> required <code>base_day</code> <code>int</code> <p>Day to be used for reference, default is <code>0</code> and is unlikely to need changing.</p> <code>0</code> <code>base_hour</code> <code>int</code> <p>Hour to be used for reference, default is <code>0</code> and is unlikely to need changing.</p> <code>0</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Subset of data with values at baseline (default <code>day == 0 &amp; hour == 0</code>).</p> Source code in <code>isoslam/summary.py</code> <pre><code>def select_base_levels(df: pl.DataFrame, base_day: int = 0, base_hour: int = 0) -&gt; pl.DataFrame:\n    \"\"\"\n    Select the base level reference across all data.\n\n    This allows selecting the base level of totals and percents which are used for normalising values. Will drop the\n    column ``replicate`` from the data frame.\n\n    Parameters\n    ----------\n    df : pl.DataFrame\n        Polars Dataframe of conversions.\n    base_day : int\n        Day to be used for reference, default is ``0`` and is unlikely to need changing.\n    base_hour : int\n        Hour to be used for reference, default is ``0`` and is unlikely to need changing.\n\n    Returns\n    -------\n    pl.DataFrame\n        Subset of data with values at baseline (default ``day == 0 &amp; hour == 0``).\n    \"\"\"\n    return (\n        df.select(pl.all().name.map(lambda col_name: col_name.replace(\"conversion\", \"baseline\")))\n        .filter((pl.col(\"day\") == base_day) &amp; (pl.col(\"hour\") == base_hour))\n        .drop([\"day\", \"hour\"])\n    )\n</code></pre>"},{"location":"api/summary/#isoslam.summary.summary_counts","title":"<code>summary_counts(file_ext='.tsv', directory=None, groupby=None, conversions_var='Conversions', conversions_threshold=1, test_file='no4sU', filename_var=None, regex=None)</code>","text":"<p>Group the data and count by various factors.</p> <p>Typically though we want to know whether conversions have happened or not and this is based on the <code>Conversions  &gt;= 1</code>, but this is configurable via the <code>conversions_var</code> and <code>conversions_threshold</code> parameters.</p> <p>Parameters:</p> Name Type Description Default <code>file_ext</code> <code>str</code> <p>File extension to search for results to summarise.</p> <code>'.tsv'</code> <code>directory</code> <code>str | Path | None</code> <p>Path on which to search for files with <code>file_ext</code>, if <code>None</code> then current working directory is used.</p> <code>None</code> <code>groupby</code> <code>list[str]</code> <p>List of variables to group the counts by, if <code>None</code> then groups the data by <code>Transcript_id</code>, <code>Strand</code>, <code>Start</code>, <code>End</code>, <code>Assignment</code>, and   <code>filename</code>.</p> <code>None</code> <code>conversions_var</code> <code>str</code> <p>The column name that holds conversions, default <code>Conversions</code>.</p> <code>'Conversions'</code> <code>conversions_threshold</code> <code>int</code> <p>Threshold for counting conversions, default <code>1</code>.</p> <code>1</code> <code>test_file</code> <code>str | None</code> <p>Unique identifier for test file, files with this string in their names are removed.</p> <code>'no4sU'</code> <code>filename_var</code> <code>str | NOne</code> <p>Column that holds filename.</p> <code>None</code> <code>regex</code> <code>str</code> <p>Regular expression pattern to extract the hour and replicate from, default <code>r\"^d(\\w+)_(\\w+)hr(\\w+)_\"</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A Polars DataFrame counting the total conversions, number by whether conversions happened and the percentage.</p> Source code in <code>isoslam/summary.py</code> <pre><code>def summary_counts(\n    file_ext: str = \".tsv\",\n    directory: str | Path | None = None,\n    groupby: list[str] | None = None,\n    conversions_var: str = \"Conversions\",\n    conversions_threshold: int = 1,\n    test_file: str | None = \"no4sU\",\n    filename_var: str | None = None,\n    regex: str | None = None,\n) -&gt; pl.DataFrame:\n    r\"\"\"\n    Group the data and count by various factors.\n\n    Typically though we want to know whether conversions have happened or not and this is based on the ``Conversions  &gt;=\n    1``, but this is configurable via the ``conversions_var`` and ``conversions_threshold`` parameters.\n\n    Parameters\n    ----------\n    file_ext : str\n        File extension to search for results to summarise.\n    directory : str | Path | None\n        Path on which to search for files with ``file_ext``, if ``None`` then current working directory is used.\n    groupby : list[str]\n        List of variables to group the counts by, if ``None`` then groups the data by ``Transcript_id``,\n        ``Strand``, ``Start``, ``End``, ``Assignment``, and   ``filename``.\n    conversions_var : str\n        The column name that holds conversions, default ``Conversions``.\n    conversions_threshold : int\n        Threshold for counting conversions, default ``1``.\n    test_file : str | None\n        Unique identifier for test file, files with this string in their names are removed.\n    filename_var : str | NOne\n        Column that holds filename.\n    regex : str\n        Regular expression pattern to extract the hour and replicate from, default ``r\"^d(\\w+)_(\\w+)hr(\\w+)_\"``.\n\n    Returns\n    -------\n    pl.DataFrame\n        A Polars DataFrame counting the total conversions, number by whether conversions happened and the percentage.\n    \"\"\"\n    if groupby is None:\n        groupby = GROUPBY_FILENAME\n    if filename_var is None:\n        filename_var = \"filename\"\n    if regex is None:\n        regex = r\"^d(\\w+)_(\\w+)hr(\\w+)_\"\n    df = append_files(file_ext, directory)\n    if test_file is not None:\n        df = df.filter(pl.col(filename_var) != test_file)\n    df = df.with_columns([(pl.col(conversions_var) &gt;= conversions_threshold).alias(\"one_or_more_conversion\")])\n    # Get counts by variables, including one_or_more_conversion\n    groupby.append(\"one_or_more_conversion\")\n    df_count_conversions = df.group_by(groupby).len(name=\"conversion_count\")\n    # Aggregate again ignoring one_or_more_conversion to give total counts at site\n    groupby.remove(\"one_or_more_conversion\")\n    df_count_total = df.group_by(groupby).len(name=\"conversion_total\")\n    # Combine counts and totals and calculate percent\n    df_count_conversions = df_count_conversions.join(df_count_total, on=groupby)\n    df_count_conversions = df_count_conversions.with_columns(\n        (pl.col(\"conversion_count\") / pl.col(\"conversion_total\")).alias(\"conversion_percent\")\n    )\n    df_count_conversions = extract_day_hour_and_replicate(df_count_conversions, filename_var, regex)\n    # Sort the data and remove tests (where day is null)\n    sort = groupby + [\"day\", \"hour\", \"replicate\", \"one_or_more_conversion\"]\n    df_count_conversions = df_count_conversions.sort(sort, maintain_order=True)\n    return df_count_conversions.filter(~pl.col(\"day\").is_null())\n</code></pre>"},{"location":"api/utils/","title":"Utils","text":"<p>Utilities and helper functionsfor IsoSLAM.</p>"},{"location":"api/utils/#isoslam.utils.update_config","title":"<code>update_config(config, args)</code>","text":"<p>Update the configuration with any arguments.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>dict</code> <p>Dictionary of configuration (typically read from YAML file specified with '-c/--config '). required <code>args</code> <code>Namespace</code> <p>Command line arguments.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary updated with command arguments.</p> Source code in <code>isoslam/utils.py</code> <pre><code>def update_config(config: dict[str, Any], args: dict[str, Any]) -&gt; dict[str, Any]:\n    \"\"\"\n    Update the configuration with any arguments.\n\n    Parameters\n    ----------\n    config : dict\n        Dictionary of configuration (typically read from YAML file specified with '-c/--config &lt;filename&gt;').\n    args : Namespace\n        Command line arguments.\n\n    Returns\n    -------\n    dict\n        Dictionary updated with command arguments.\n    \"\"\"\n    args = vars(args) if isinstance(args, Namespace) else args\n    args_keys = args.keys()\n    for config_key, config_value in config.items():\n        if isinstance(config_value, dict):\n            update_config(config_value, args)\n        else:\n            if config_key in args_keys and args[config_key] is not None and config_value is not args[config_key]:\n                original_value = config[config_key]\n                config[config_key] = args[config_key]\n                logger.info(f\"Updated config config[{config_key}] : {original_value} &gt; {args[config_key]} \")\n    if \"base_dir\" in config.keys():\n        config[\"base_dir\"] = io._str_to_path(config[\"base_dir\"])  # pylint: disable=protected-access\n    if \"output_dir\" in config.keys():\n        config[\"output_dir\"] = io._str_to_path(config[\"output_dir\"])  # pylint: disable=protected-access\n    return config\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>This document describes how to contribute to the development of this software.</p>"},{"location":"contributing/#bug-reports","title":"Bug Reports","text":"<p>If you find a bug we need to know about it so we can fix it. Please report your bugs on our GitHub Issues page.</p>"},{"location":"contributing/#feature-requests","title":"Feature Requests","text":"<p>If you find IsoSLAM useful but think it can be improved you can make a feature request.</p>"},{"location":"contributing/#code-contributions","title":"Code Contributions","text":"<p>If you would like to fix a bug or add a new feature that is great, Pull Requests are very welcome.</p> <p>However, we have adopted a number of good software development practises that ensure the code and documentation is linted and that unit and regression tests pass both locally and on Continuous Integration. The rest of this page helps explain how to set yourself up with these various tools.</p>"},{"location":"contributing/#virtual-environments","title":"Virtual Environments","text":"<p>Use of virtual environments, particularly during development of Python packages, is encouraged. There are lots of options out there for you to choose from including...</p> <ul> <li>Miniconda</li> <li>venv</li> <li>virtualenvwrapper</li> </ul> <p>Which you choose is up to you, although you should be wary of using the Miniconda distribution from Anaconda if any of your work is carried out for or in conjunction with a commercial entity.</p>"},{"location":"contributing/#uv","title":"uv","text":"<p>Developers are using the uv package manager to setup and control environments to which end a <code>uv.lock</code> file is included in the repository. uv supports managing virtual environments so you may wish to install and use this tool at the system level to manage your virtual environments for this package.</p>"},{"location":"contributing/#cloning-the-repository","title":"Cloning the Repository","text":"<p>Once you have setup your virtual environment you should clone the repository from GitHub</p> <pre><code>cd ~/path/you/want/to/clone/to\ngit clone https://github.com/sudlab/IsoSLAM\n</code></pre>"},{"location":"contributing/#install-development","title":"Install development","text":"<p>Once you have clone the IsoSLAM repository you should install all the package along with all development and documentation dependencies in \"editable\" mode. This means you can test the changes you make in real time.</p> <pre><code>cd IsoSLAM\npip install --no-cache-dir -e .[docs,dev]\n</code></pre>"},{"location":"contributing/#git","title":"Git","text":"<p>Git is used to version control development of the package. The <code>main</code> branch on GitHub and the pre-commit hooks have protections in place that prevent committing/pushing directly to the <code>main</code> branch. This means you should create a branch to undertake development or fix bugs.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Ideally an issue should have been created detailing the feature request. If it is a large amount of work this should be captured in an issue labelled \"Epic\" and the steps taken to achieve all work broken down into smaller issues.</p>"},{"location":"contributing/#branch-nomenclature","title":"Branch nomenclature","text":"<p>When undertaking work on a particular issue it is useful to use informative branch names. These convey information about what the branch is for beyond simply \"<code>adding-feature-x</code>\". You should create the branch using your GitHub username, followed by the issue number and a short description of the work being undertaken. For example <code>ns-rse/31-merge-slam-3uis</code> as this allows others to know who has been undertaking the work, what issue the work relates to and has an informative name as to the nature of that work.</p>"},{"location":"contributing/#ignoring-revisions","title":"Ignoring Revisions","text":"<p>If you make small stylistic changes to code, e.g. during linting, and you wish the git blame to attribute the code to the original author you can ignore your commit by adding it to the repositories <code>.git-blame-ignore-revs</code> file and enabling the use of it in your repositories configuration with the following command.</p> <pre><code>git config blame.ignoreRevsFile .git-blame-ignore-revs\n</code></pre>"},{"location":"contributing/#linting","title":"Linting","text":"<p>Linting is the practice of following a consistent coding style. For Python that style is defined in PEP8. By following a consistent style across a code base it is easier to read and understand the code written by others (including your past self!). We use the following linters implemented as pre-commit hooks</p> <ul> <li>Python</li> <li>Black</li> <li>Blacken-docs</li> <li>flake8</li> <li>Numpydoc</li> <li>Ruff</li> <li>Other</li> <li>Codespell (Spelling across all filesyy)</li> <li>markdownlint-cli2 (Markdown)</li> <li>prettier (Markdown, YAML)</li> </ul> <p>NB It is important that Python files lint with <code>black</code> otherwise the API section of this website will not automatically build the web-pages for that section.</p>"},{"location":"contributing/#pre-commit","title":"Pre-commit","text":"<p>Style checks are made using the pre-commit framework which is one of the development dependencies and should have been installed in the previous step. You can check if its is installed in your virtual environment with <code>pip show pre-commit</code>. If you have pre-commit installed install the hook using...</p> <pre><code>pre-commit install\n</code></pre> <p>This adds a file to <code>.git/hooks/pre-commit</code> that will run all of the hooks specified in <code>.pre-commit-config.yaml</code>. The first time these are run it will take a little while as a number of virtual environments are downloaded for the first time. It might be a good time to run these manually on the code base you have just cloned which should pass all checks.</p> <pre><code>pre-commit run --all-files\n</code></pre> <p>These should all pass. Now whenever you try to make a <code>git commit</code> these checks will run before the commit is made and if any fail you will be advised of what has failed. Some of the linters such as Black and Ruff will automatically correct any errors that they find and you will have to stage the files that have changed again. Not all errors can be automatically corrected (e.g. Numpydoc validation and Pylint) and you will have to manually correct these.</p>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>It is sensible and easiest to write informative docstrings when first defining your modules, classes and methods/functions. Doing so is a useful adie-memoire not only for others but your future self and with modern Language Servers that will, on configuration, show you the docstrings when using the functions it helps save time.</p> <p>You will find your commits fail the numpydoc-validation pre-commit hook if you do not write docstrings and will be prompted to add one.</p>"},{"location":"contributing/#testing","title":"Testing","text":"<p>We use the pytest framework with various plugins in our testing suite. When correcting bugs and adding features at a bare minimum the existing tests should not fail. Where possible we would be grateful of contributions to the test suite. This means if an edge case has been identified and a solution derived a test is added that checks the edge case is correctly handled. For new features would ideally mean writing unit-tests to ensure each function or method works as intended and for larger classes that behaviour is as expected. Sometimes tests will need updating in light of bug fixes and features which is to be expected, but remember to commit updates to tests as well as to code to ensure the Continuous Integration tests pass.</p>"},{"location":"contributing/#pytest-testmon","title":"Pytest-testmon","text":"<p>To shorten the feedback loop during development the pytest-testmon plugin is used as a pre-commit hook so that only the tests affected by the changes that are being committed are run. This requires that on first installing the package you create a local database of the state of the tests by running the following...</p> <pre><code>pytest --test-mon\n</code></pre> <p>This creates the files <code>.testmondata</code> which stores the current state of tests. Once created commits will only run affected tests. However if your environment has changed, such as adding new packages or updating installed packages you will have to recreate the database.</p>"},{"location":"contributing/#pull-requests","title":"Pull Requests","text":"<p>Once you have made your changes and committed them you will at some point wish to make a Pull Request to merge them into the <code>main</code> branch.</p> <p>In order to keep Git history clean and easier to understand you can perform an interactive <code>git rebase -i</code> on your feature branch to squash related commits and tidy up your commit history.</p> <p>When your branch is ready for merging with <code>main</code> open a Pull Request. You can use the GitHub keywords of <code>close[s|d]</code>/<code>fix[es|ed]</code> / <code>resolve[s|d]</code> followed by the issue number in the body of your commit message which will change the status of the issue to \"Closed\" when the Pull Request is merged.</p> <p>Pull Requests will be reviewed in a timely and hopefully constructive manner.</p>"},{"location":"contributing/adding_modules/","title":"Adding modules","text":"<p>This document describes how to add additional post-processing modules to summarise, model or plot the results. As the processing pipeline uses Python these steps are undertaken using Pandas, Polars, plotnine and statsmodels but other frameworks can be used to extend functionality such as Scikit-learn (or any other Python module!).</p> <p>You should set yourself up with a development environment as described in the contributing section so that linting and pre-commit hooks will run locally, shortening the development feedback loop.</p>"},{"location":"contributing/adding_modules/#organising-modules","title":"Organising Modules","text":"<p>Typically the functionality that is likely to be added will be some aspect that summarises or plots the post-processing results. To which end there is the <code>isoslam.summary</code> and <code>isoslam.plotting</code> modules to which functionality should be added. There is no harm in adding a new module, particularly if there are a large number of functions that are required but you may wish to consider adding the entry point function to one of these modules.</p>"},{"location":"contributing/adding_modules/#parameters","title":"Parameters","text":"<p>The parameters should be clearly defined with typehints as the pre-commit hooks will fail (if not locally then in Continuous Integration which blocks merging). The nomenclature used for parameters should be the basis of configuration options used (see next section). This makes it possible to leverage <code>**kwargs</code> to pass options loaded from the configuration dictionary and updated from the command line, through to the functions.</p>"},{"location":"contributing/adding_modules/#configuration","title":"Configuration","text":"<p>Configuration options should be added to <code>isoslam/default_config.yaml</code>. A section should be defined for the module you are adding, in this worked example we are creating the <code>plot_conversions</code> and so we would add a section that corresponds to the arguments required for plotting. The function name is the top-level and options for this module are nested within.</p> <pre><code>plot_conversions:\n  group_by: \"read\"\n  theme: \"classic\"\n</code></pre>"},{"location":"contributing/adding_modules/#validation","title":"Validation","text":"<p>There is a validation module in place <code>isoslam.validation</code> which checks that the parameters in the <code>default_config.yaml</code>, a user supplied configuration or command line options are of the expected type. You need to add the options you have added to <code>isoslam/default_config.yaml</code> to the <code>DEFAULT_CONFIG_SCHEMA</code> that is defined in the <code>isoslam.validation</code> module. The examples there should be informative for writing/adding new dictionary entries. The keys are the fields expected in the configuration, the values are the expected types or the <code>schema.Or()</code> function which states the type(s)/values that are permitted and lists an <code>error=\"&lt;error message&gt;\"</code> that is displayed if the condition is not met. For the above additional configuration you would add the following to the <code>DEFAULT_CONFIG_SCHEMA</code>, nesting the options as reflected in the configuration structure.</p> <pre><code>DEFAULT_CONFIG_SCHEMA = Schema(\n    {\n        \"plot_conversions\": {\n            \"group_by\": Or(\n                \"read\",\n                \"pair\",\n                error=\"Invalid value in config for plot.conversions.group_by, valid values are 'read' or 'pair'\",\n            ),\n            \"theme\": Or(\n                \"classic\",\n                \"bw\",\n                error=\"Invalid value in config for plot.theme, valid values are 'classic' or 'bw'\",\n            ),\n        }\n    }\n)\n</code></pre>"},{"location":"contributing/adding_modules/#entry-points","title":"Entry Points","text":"<p>To make the module available at the command line, and in turn possible to integrate into the CGAT pipeline with you need what is known in Python packaging as an entry point. This is a method of providing a simple command line interface to access your program and sub-modules so that they do not need prefixing with <code>python -m</code>. The module where this is setup is <code>isoslam.processing</code> where you will see there is a <code>create_config()</code> function which creates an argument parser along with sub-parsers. The new function you are adding will be added as a sub-parser, in the example below we add a sub-parser for plotting the number of conversions per read.</p> <p>There should be one argument for every configuration option defined in <code>default_config.yaml</code>, which in turn mirrors the options used in the functions that you call, and each <code>dest</code> should match these names. The updating of the configuration based on command line options is contingent on these aligning.</p> <pre><code>def create_parser() -&gt; arg.ArgumentParser:\n    \"\"\"\n    Create a parser for reading options.\n\n    Parser is created with multiple sub-parsers for eading options to run ``isoslam``.\n\n    Returns\n    -------\n    arg.ArgumentParser\n        Argument parser.\n    \"\"\"\n    ...\n\n    # Plot conversions per read\n    plot_conversions = subparsers.add_parser(\n        \"plot-conversions-per-read\",\n        description=\"Plot the conversions per read.\",\n        help=\"Plot the conversions per read.\",\n    )\n    plot_conversions_parser.add_argument(\n        \"--file-pattern\",\n        dest=\"file_pattern\",\n        type=str,\n        required=False,\n        default=\"*_summarized.tsv\",\n        help=\"Regular expression for summarized files to process.\",\n    )\n    plot_conversions_parser.add_argument(\n        \"--outfile\",\n        dest=\"outfile\",\n        type=Path,\n        required=False,\n        default=\"conversions.png\",\n        help=\"Output filename to save results to, will be nested under 'output_dir'.\",\n    )\n    plot_conversions_parser.add_argument(\n        \"--separator\",\n        dest=\"sep\",\n        type=str,\n        required=False,\n        default=\"\\t\",\n        help=\"Field separator to use in output file, default is '\\t' but other values (e.g. ',' are allowed).\",\n    )\n    plot_conversions.set_defaults(func=plot_conversions)\n</code></pre> <p>This sets up the subparser <code>plot_conversions_parser</code> which has three optional arguments to specify the <code>file_pattern</code> to be searched for and subsequently loaded, the <code>outfile</code> name which will be nested under the <code>output_dir</code> (which is an argument to the <code>isoslam</code> entry point) and the <code>separator</code> that is used in the files. Finally a default function is set, in this case <code>plot_conversions</code>.</p> <p>The <code>plot_conversions()</code> function, which we will define within the processing module does the work of calling the module you have added. The only argument it needs is <code>args</code> which will be the <code>arguments.Namespace</code> that is created by the argument parser. These are used to update the default options which read from the <code>isoslam/default_config.yaml</code> with values the user enters which is the validated with a call to <code>validation.validate_config()</code>.</p> <p>The task of plotting conversions requires that we first load a series of files, combine them and summarise them which are the first set of steps taken, including some subsetting of configuration options. This data is then summarized and plotted using the functions defined and imported from the <code>isoslam.summary</code> module and the <code>isoslam.plotting</code> modules.</p> <pre><code>from isoslam import plotting as plot\nfrom isoslam import summary, validation\n\n\ndef plot_conversions(args: arg.Namespace | None) -&gt; None:\n    \"\"\"\n    Take a set of output files and summarise the number of conversions.\n\n    Counts are made within file, chromosome, transcript, start, end, assignment and whether there is one or more\n    conversion observed.\n\n    Parameters\n    ----------\n    args : arg.Namespace | None\n        Arguments function was invoked with.\n\n    Returns\n    -------\n    None\n        Function does not return anything.\n    \"\"\"\n    config = io.load_and_update_config(args)\n    logger.remove()\n    if vars(args)[\"log_level\"] is not None:\n        logging.setup(level=vars(args)[\"log_level\"])\n    else:\n        logging.setup(level=config[\"log_level\"])\n    # Validate the configuration\n    validation.validate_config(\n        config=config,\n        schema=validation.DEFAULT_CONFIG_SCHEMA,\n        config_type=\"configuration\",\n    )\n    # Load and summarise the data\n    plot_conversions_config = config[\"plot_conversions\"]\n    output_config = summary_counts_config.pop(\"output\")\n    output_config[\"output_dir\"] = config[\"output_dir\"]\n    plot.conversions(**plot_conversions_config)\n    logger.info(\n        f\"Conversions per read plotted to : {output_config['output_dir]}/{output_config['outfile]}\"\n    )\n</code></pre>"},{"location":"contributing/profiling/","title":"Profiling","text":"<p>The speed of execution of <code>isoslam process</code> can be investigated using Profiling which analyses which functions and methods take up the most processing time. In order to undertake profiling the <code>cProfile</code> standard library can be used. Visualisation of the results can be aided using the SnakeViz package.</p>"},{"location":"contributing/profiling/#performing-profiling","title":"Performing Profiling","text":"<p>You need a set of sample files to process in order to undertake profiling. Here we use the sample files that are included as part of the test suite used in development that can be found under <code>tests/resources</code>. If you do not yet have these locally you should <code>git clone</code> the repository and install it in a clean virtual environment with the development dependencies.</p> <pre><code>git clone git@github.com:sudlab/IsoSLAM.git\ncd IsoSLAM\npip install -e .[dev]\n</code></pre> <p>We now make a <code>tmp/test-YYYYMMDD</code> directory and copy the necessary files here.</p> <pre><code>mkdir -p tmp/test-$(date +%Y%m%d)                            # Uses the current date\ncp -r tests/resources/{bam,gtf,bed,vcf} tmp/test-20250221    # Modify to reflect the current date\n</code></pre> <p>We can run profiling on these samples using the following which writes the profiling to <code>isoslam-YYYYMMDD.prof</code>.</p> <pre><code>cd tmp/test-20250221\npython -m cProfile -o isoslam-$(date +%Y%m%d).prof $(isoslam process \\\n   --bam-file bam/sorted_assigned/d0_no4sU_filtered_remapped_sorted.sorted.assigned.bam \\\n   --gtf-file gtf/test_wash1.gtf \\\n   --bed-file bed/test_coding_introns.bed \\\n   --vcf vcf/d0.vcf.gz)\n</code></pre> <p>You can verify output has been produced using <code>parquet-tools</code> which is part of the <code>dev</code> dependencies.</p> <pre><code>parquet-tools show output/results.parquet\n</code></pre> <p>The profiling data should have been written to <code>isoslam.prof</code>.</p> <pre><code>head isoslam.prof\n</code></pre>"},{"location":"contributing/profiling/#visualisation-of-profiling","title":"Visualisation of Profiling","text":"<p>To visualise the results of profiling you can invoke <code>snakeviz</code> with the <code>.prof</code> file that has been generated.</p> <pre><code>snakeviz isoslam-&lt;YYYYMMDD&gt;.prof\n</code></pre> <p>This should launch a new browser tab with the icicle where the amount of time spent within a function is proportional to the size of the bar.</p>"}]}